#!/usr/bin/env python3
"""
–ü–æ–ª–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è RTX 5090 —Å –æ–±—Ö–æ–¥–æ–º –≤—Å–µ—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
"""

import os

# –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –í–°–ï –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –î–û –∏–º–ø–æ—Ä—Ç–∞ torch
# –û–±—Ö–æ–¥ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤–µ—Ä—Å–∏–π –¥—Ä–∞–π–≤–µ—Ä–∞
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["NVIDIA_VISIBLE_DEVICES"] = "0"

# –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤–µ—Ä—Å–∏–π NVML
os.environ["CUDA_MODULE_LOADING"] = "LAZY"
os.environ["PYTORCH_NVML_BASED_CUDA_CHECK"] = "0"

# –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è RTX 5090
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
os.environ["TORCH_CUDA_ARCH_LIST"] = "5.0;6.0;6.1;7.0;7.5;8.0;8.6;8.9;9.0+PTX;12.0"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"
os.environ["TORCH_COMPILE_DISABLE"] = "1"
os.environ["PYTORCH_JIT"] = "0"
os.environ["CUDA_FORCE_PTX_JIT"] = "1"

# –§–æ—Ä—Å–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CUDA –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–æ–∫
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "0"
os.environ["PYTORCH_CUDA_FORCE_CUDA_ENABLED"] = "1"

# –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –æ–±—Ö–æ–¥ –¥–ª—è –Ω–æ–≤—ã—Ö GPU
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["PYTORCH_CUDA_FORCE_DEVICE_CAPABILITY"] = "12.0"

import torch

# –•–∞–∫ –¥–ª—è –æ–±—Ö–æ–¥–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
original_cuda_is_available = torch.cuda.is_available


def patched_cuda_is_available():
    """–ü–∞—Ç—á–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è is_available –∫–æ—Ç–æ—Ä–∞—è –æ–±—Ö–æ–¥–∏—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏"""
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ–∑–¥–∞—Ç—å —Ç–µ–Ω–∑–æ—Ä –Ω–∞ GPU –Ω–∞–ø—Ä—è–º—É—é
        device = torch.device("cuda:0")
        test = torch.zeros(1).to(device)
        return True
    except:
        return False


# –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—á
torch.cuda.is_available = patched_cuda_is_available


def test_gpu_with_workarounds():
    """–¢–µ—Å—Ç GPU —Å –æ–±—Ö–æ–¥–Ω—ã–º–∏ –ø—É—Ç—è–º–∏ –¥–ª—è RTX 5090"""
    print("üîß –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—Ö–æ–¥–Ω—ã—Ö –ø—É—Ç–µ–π –¥–ª—è RTX 5090...")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    print(f"PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__}")
    print(f"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞ (—Å –ø–∞—Ç—á–µ–º): {torch.cuda.is_available()}")

    if torch.cuda.is_available():
        print("‚úÖ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞ —á–µ—Ä–µ–∑ –æ–±—Ö–æ–¥–Ω–æ–π –ø—É—Ç—å!")

        try:
            # –ü—Ä—è–º–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
            device = torch.device("cuda:0")
            print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —Å–æ–∑–¥–∞–Ω–æ: {device}")

            # –¢–µ—Å—Ç –æ–ø–µ—Ä–∞—Ü–∏–π
            print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–π –Ω–∞ GPU...")
            x = torch.randn(1000, 1000, device=device)
            y = torch.randn(1000, 1000, device=device)
            z = torch.matmul(x, y)

            print("‚úÖ –ú–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ!")
            print(f"   –†–∞–∑–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {z.shape}")
            print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {z.device}")

            # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU
            try:
                print("\nüìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU:")
                print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU: {torch.cuda.device_count()}")
                for i in range(torch.cuda.device_count()):
                    props = torch.cuda.get_device_properties(i)
                    print(f"   GPU {i}: {props.name}")
                    print(f"   Compute Capability: {props.major}.{props.minor}")
                    print(f"   –ü–∞–º—è—Ç—å: {props.total_memory / 1024**3:.1f} GB")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–≤–æ–π—Å—Ç–≤–∞ GPU: {e}")
                print("   –ù–æ GPU —Ä–∞–±–æ—Ç–∞–µ—Ç!")

            # –¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            import time

            print("\n‚ö° –¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")

            # –ü—Ä–æ–≥—Ä–µ–≤
            for _ in range(10):
                _ = torch.matmul(x, y)

            # –ó–∞–º–µ—Ä
            torch.cuda.synchronize()
            start = time.time()

            for _ in range(100):
                _ = torch.matmul(x, y)

            torch.cuda.synchronize()
            elapsed = time.time() - start

            print(f"‚úÖ 100 –æ–ø–µ—Ä–∞—Ü–∏–π –∑–∞ {elapsed:.3f} —Å–µ–∫—É–Ω–¥")
            print(f"   –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {100 / elapsed:.1f} –æ–ø–µ—Ä–∞—Ü–∏–π/—Å–µ–∫")

            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å GPU: {e}")
            import traceback

            traceback.print_exc()
            return False
    else:
        print("‚ùå CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–∞–∂–µ —Å –æ–±—Ö–æ–¥–Ω—ã–º–∏ –ø—É—Ç—è–º–∏")
        return False


def create_gpu_init_module():
    """–°–æ–∑–¥–∞–µ—Ç –º–æ–¥—É–ª—å –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ GPU –≤ –¥—Ä—É–≥–∏—Ö —Ñ–∞–π–ª–∞—Ö"""

    init_code = '''#!/usr/bin/env python3
"""
–ú–æ–¥—É–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ GPU –¥–ª—è RTX 5090
–ò–º–ø–æ—Ä—Ç–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –º–æ–¥—É–ª—å –ü–ï–†–ï–î torch –≤ –≤–∞—à–∏—Ö —Å–∫—Ä–∏–ø—Ç–∞—Ö
"""

import os

# –û–±—Ö–æ–¥ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤–µ—Ä—Å–∏–π –¥—Ä–∞–π–≤–µ—Ä–∞
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['PYTORCH_NVML_BASED_CUDA_CHECK'] = '0'
os.environ['CUDA_MODULE_LOADING'] = 'LAZY'

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è RTX 5090
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ['TORCH_CUDA_ARCH_LIST'] = '5.0;6.0;6.1;7.0;7.5;8.0;8.6;8.9;9.0+PTX;12.0'
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
os.environ['TORCH_COMPILE_DISABLE'] = '1'
os.environ['PYTORCH_JIT'] = '0'
os.environ['CUDA_FORCE_PTX_JIT'] = '1'
os.environ['PYTORCH_CUDA_FORCE_CUDA_ENABLED'] = '1'
os.environ['PYTORCH_CUDA_FORCE_DEVICE_CAPABILITY'] = '12.0'

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º torch –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—á
import torch

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
_original_is_available = torch.cuda.is_available

def _patched_is_available():
    """–ü–∞—Ç—á–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è is_available"""
    try:
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–ø—Ä—è–º—É—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU
        device = torch.device('cuda:0')
        test = torch.zeros(1).to(device)
        del test
        return True
    except:
        # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
        try:
            return _original_is_available()
        except:
            return False

# –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—á
torch.cuda.is_available = _patched_is_available

# –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

print(f"üéÆ GPU –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è RTX 5090: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ = {device}")
'''

    with open("gpu_init_rtx5090.py", "w") as f:
        f.write(init_code)

    print("\nüìù –°–æ–∑–¥–∞–Ω –º–æ–¥—É–ª—å gpu_init_rtx5090.py")
    print("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –µ–≥–æ —Ç–∞–∫:")
    print("   import gpu_init_rtx5090  # –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ü–ï–†–ï–î torch!")
    print("   import torch")
    print("   # –¢–µ–ø–µ—Ä—å GPU –¥–æ–ª–∂–µ–Ω —Ä–∞–±–æ—Ç–∞—Ç—å")


if __name__ == "__main__":
    success = test_gpu_with_workarounds()

    if success:
        create_gpu_init_module()
        print("\n‚úÖ GPU —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –æ–±—Ö–æ–¥–Ω—ã–º–∏ –ø—É—Ç—è–º–∏!")
        print("–†–µ—à–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ –ø—Ä–æ–µ–∫—Ç–µ.")
    else:
        print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞—Å—Ç–∞–≤–∏—Ç—å GPU —Ä–∞–±–æ—Ç–∞—Ç—å")
        print("–í–æ–∑–º–æ–∂–Ω–æ, —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥—Ä–∞–π–≤–µ—Ä–æ–≤")
