#!/usr/bin/env python3
"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö ML v2 - —Ä–∞–±–æ—Ç–∞ —Å JSONB —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
"""

import asyncio
import json

import numpy as np

from core.logger import setup_logger
from database.connections.postgres import AsyncPGPool

logger = setup_logger("ml_data_quality")


async def check_processed_market_data():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–∞–±–ª–∏—Ü–µ processed_market_data —Å JSONB –ø–æ–ª—è–º–∏"""

    logger.info("\n" + "=" * 60)
    logger.info("üìä –ü–†–û–í–ï–†–ö–ê –¢–ê–ë–õ–ò–¶–´ processed_market_data")
    logger.info("=" * 60)

    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ
    query = """
        SELECT
            id, symbol, timestamp, datetime,
            open, high, low, close, volume,
            technical_indicators,
            ml_features,
            direction_15m, direction_1h,
            future_return_15m, future_return_1h
        FROM processed_market_data
        WHERE timestamp > EXTRACT(EPOCH FROM NOW() - INTERVAL '1 hour') * 1000
        ORDER BY timestamp DESC
        LIMIT 100
    """

    rows = await AsyncPGPool.fetch(query)

    if not rows:
        logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å")
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –≤–æ–æ–±—â–µ –¥–∞–Ω–Ω—ã–µ
        count_query = "SELECT COUNT(*) as cnt FROM processed_market_data"
        count_result = await AsyncPGPool.fetch(count_query)
        total_count = count_result[0]["cnt"]
        logger.info(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ —Ç–∞–±–ª–∏—Ü–µ: {total_count}")

        if total_count > 0:
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞ –≤—Ä–µ–º–µ–Ω–∏
            query = """
                SELECT
                    id, symbol, timestamp, datetime,
                    open, high, low, close, volume,
                    technical_indicators,
                    ml_features
                FROM processed_market_data
                ORDER BY timestamp DESC
                LIMIT 10
            """
            rows = await AsyncPGPool.fetch(query)
        else:
            return False

    logger.info(f"üìà –ù–∞–π–¥–µ–Ω–æ {len(rows)} –∑–∞–ø–∏—Å–µ–π")

    issues = []

    for row in rows[:5]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 5 –∑–∞–ø–∏—Å–µ–π –¥–µ—Ç–∞–ª—å–Ω–æ
        symbol = row["symbol"]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –ø–æ–ª—è
        for field in ["open", "high", "low", "close", "volume"]:
            value = float(row[field]) if row[field] else 0
            if value <= 0:
                issues.append(f"‚ùå {symbol}: {field} = {value}")
            elif field != "volume" and value > 1000000:
                issues.append(f"‚ö†Ô∏è {symbol}: –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ü–µ–Ω–∞ {field} = {value}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º technical_indicators JSONB
        if row["technical_indicators"]:
            try:
                indicators = (
                    json.loads(row["technical_indicators"])
                    if isinstance(row["technical_indicators"], str)
                    else row["technical_indicators"]
                )

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
                for key in ["rsi", "macd", "bb_upper", "bb_lower", "ema_short", "ema_long"]:
                    if key in indicators:
                        val = indicators[key]
                        if val is None or (isinstance(val, (int, float)) and np.isnan(val)):
                            issues.append(f"‚ùå {symbol}: {key} = NaN")
                        elif key == "rsi" and (val < 0 or val > 100):
                            issues.append(f"‚ùå {symbol}: RSI –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ = {val}")

            except Exception as e:
                issues.append(f"‚ùå {symbol}: –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ technical_indicators: {e}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º ml_features JSONB
        if row["ml_features"]:
            try:
                features = (
                    json.loads(row["ml_features"])
                    if isinstance(row["ml_features"], str)
                    else row["ml_features"]
                )

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN –∑–Ω–∞—á–µ–Ω–∏—è
                nan_features = []
                for key, val in features.items():
                    if val is None or (isinstance(val, (int, float)) and np.isnan(val)):
                        nan_features.append(key)

                if nan_features:
                    issues.append(f"‚ùå {symbol}: NaN –≤ ML features: {', '.join(nan_features[:5])}")

            except Exception as e:
                issues.append(f"‚ùå {symbol}: –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ ml_features: {e}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
    symbols = set(row["symbol"] for row in rows)
    logger.info("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    logger.info(f"  ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤: {len(symbols)}")
    logger.info(f"  ‚Ä¢ –°–∏–º–≤–æ–ª—ã: {', '.join(list(symbols)[:10])}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
    if len(rows) > 1:
        timestamps = sorted([row["timestamp"] for row in rows])
        time_diffs = np.diff(timestamps)
        avg_diff = np.mean(time_diffs) / 1000 / 60  # –≤ –º–∏–Ω—É—Ç–∞—Ö
        logger.info(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª: {avg_diff:.2f} –º–∏–Ω—É—Ç")

    # –í—ã–≤–æ–¥–∏–º –ø—Ä–æ–±–ª–µ–º—ã
    if issues:
        logger.warning(f"\n‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ {len(issues)} –ø—Ä–æ–±–ª–µ–º:")
        for issue in issues[:10]:
            logger.warning(f"  {issue}")
        if len(issues) > 10:
            logger.warning(f"  ... –∏ –µ—â–µ {len(issues) - 10} –ø—Ä–æ–±–ª–µ–º")
        return False
    else:
        logger.info("\n‚úÖ –î–∞–Ω–Ω—ã–µ –≤ processed_market_data –≤ –Ω–æ—Ä–º–µ!")
        return True


async def check_ml_predictions():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –≤ signals –∏ processed_market_data"""

    logger.info("\n" + "=" * 60)
    logger.info("ü§ñ –ü–†–û–í–ï–†–ö–ê ML –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô")
    logger.info("=" * 60)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏–≥–Ω–∞–ª—ã
    query = """
        SELECT *
        FROM signals
        WHERE created_at > NOW() - INTERVAL '24 hours'
        ORDER BY created_at DESC
        LIMIT 50
    """

    rows = await AsyncPGPool.fetch(query)

    if not rows:
        logger.warning("‚ö†Ô∏è –ù–µ—Ç —Å–∏–≥–Ω–∞–ª–æ–≤ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞")
    else:
        logger.info(f"üìà –ù–∞–π–¥–µ–Ω–æ {len(rows)} —Å–∏–≥–Ω–∞–ª–æ–≤")

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∏–≥–Ω–∞–ª—ã
        ml_signals = [
            r for r in rows if r.get("strategy_name") and "ml" in r["strategy_name"].lower()
        ]
        logger.info(f"  ‚Ä¢ ML —Å–∏–≥–Ω–∞–ª–æ–≤: {len(ml_signals)}")

        if ml_signals:
            symbols = set(r["symbol"] for r in ml_signals)
            logger.info(f"  ‚Ä¢ –°–∏–º–≤–æ–ª—ã: {', '.join(symbols)}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ processed_market_data
    query = """
        SELECT
            symbol,
            COUNT(*) as cnt,
            AVG(future_return_15m) as avg_return_15m,
            AVG(future_return_1h) as avg_return_1h,
            SUM(CASE WHEN direction_15m = 1 THEN 1 ELSE 0 END) as buy_signals,
            SUM(CASE WHEN direction_15m = -1 THEN 1 ELSE 0 END) as sell_signals
        FROM processed_market_data
        WHERE timestamp > EXTRACT(EPOCH FROM NOW() - INTERVAL '1 hour') * 1000
        GROUP BY symbol
    """

    pred_rows = await AsyncPGPool.fetch(query)

    if pred_rows:
        logger.info("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
        for row in pred_rows:
            if row["cnt"] > 0:
                buy_pct = (row["buy_signals"] or 0) / row["cnt"] * 100
                sell_pct = (row["sell_signals"] or 0) / row["cnt"] * 100
                logger.info(f"  ‚Ä¢ {row['symbol']}: BUY {buy_pct:.1f}%, SELL {sell_pct:.1f}%")

                if row["avg_return_15m"]:
                    logger.info(
                        f"    –°—Ä–µ–¥–Ω–∏–π return: 15m={row['avg_return_15m']:.4f}, 1h={row['avg_return_1h']:.4f}"
                    )

    return True


async def check_raw_market_data():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—ã—Ä—ã—Ö —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""

    logger.info("\n" + "=" * 60)
    logger.info("üìà –ü–†–û–í–ï–†–ö–ê RAW_MARKET_DATA")
    logger.info("=" * 60)

    query = """
        SELECT
            symbol,
            COUNT(*) as cnt,
            MIN(timestamp) as min_ts,
            MAX(timestamp) as max_ts,
            AVG(close) as avg_close
        FROM raw_market_data
        WHERE timestamp > EXTRACT(EPOCH FROM NOW() - INTERVAL '1 hour') * 1000
        GROUP BY symbol
    """

    rows = await AsyncPGPool.fetch(query)

    if not rows:
        logger.warning("‚ö†Ô∏è –ù–µ—Ç —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        count_query = "SELECT COUNT(*) as cnt FROM raw_market_data"
        count_result = await AsyncPGPool.fetch(count_query)
        total = count_result[0]["cnt"]
        logger.info(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {total}")
        return False

    logger.info("üìä –î–∞–Ω–Ω—ã–µ –ø–æ —Å–∏–º–≤–æ–ª–∞–º:")
    for row in rows:
        time_range = (row["max_ts"] - row["min_ts"]) / 1000 / 60
        logger.info(
            f"  ‚Ä¢ {row['symbol']}: {row['cnt']} –∑–∞–ø–∏—Å–µ–π –∑–∞ {time_range:.1f} –º–∏–Ω, avg=${row['avg_close']:.2f}"
        )

    return True


async def check_database_integrity():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏"""

    logger.info("\n" + "=" * 60)
    logger.info("üîó –ü–†–û–í–ï–†–ö–ê –¶–ï–õ–û–°–¢–ù–û–°–¢–ò –î–ê–ù–ù–´–•")
    logger.info("=" * 60)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–≤—è–∑—å raw_market_data -> processed_market_data
    query = """
        SELECT
            COUNT(DISTINCT r.id) as raw_count,
            COUNT(DISTINCT p.raw_data_id) as processed_count
        FROM raw_market_data r
        LEFT JOIN processed_market_data p ON r.id = p.raw_data_id
        WHERE r.timestamp > EXTRACT(EPOCH FROM NOW() - INTERVAL '1 hour') * 1000
    """

    result = await AsyncPGPool.fetch(query)
    if result:
        row = result[0]
        if row["raw_count"] and row["processed_count"]:
            coverage = row["processed_count"] / row["raw_count"] * 100
            logger.info(f"üìä –ü–æ–∫—Ä—ã—Ç–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π: {coverage:.1f}%")
            logger.info(f"  ‚Ä¢ –°—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {row['raw_count']}")
            logger.info(f"  ‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö: {row['processed_count']}")
        else:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–≤—è–∑–µ–π")

    return True


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏"""

    logger.info("\n" + "=" * 80)
    logger.info("üîç –ü–†–û–í–ï–†–ö–ê –ö–ê–ß–ï–°–¢–í–ê ML –î–ê–ù–ù–´–• V2")
    logger.info("=" * 80)

    results = {}

    # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
    results["raw_data"] = await check_raw_market_data()

    # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    results["processed_data"] = await check_processed_market_data()

    # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    results["ml_predictions"] = await check_ml_predictions()

    # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å
    results["integrity"] = await check_database_integrity()

    # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    logger.info("\n" + "=" * 80)
    logger.info("üìä –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
    logger.info("=" * 80)

    passed = sum(1 for v in results.values() if v)
    total = len(results)

    for check, status in results.items():
        status_str = "‚úÖ PASSED" if status else "‚ùå FAILED"
        logger.info(f"  ‚Ä¢ {check}: {status_str}")

    logger.info(f"\nüìà –†–µ–∑—É–ª—å—Ç–∞—Ç: {passed}/{total} –ø—Ä–æ–≤–µ—Ä–æ–∫ –ø—Ä–æ–π–¥–µ–Ω–æ")

    if passed == total:
        logger.info("üéâ –í—Å–µ –¥–∞–Ω–Ω—ã–µ –≤ –æ—Ç–ª–∏—á–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏!")
    elif passed >= total * 0.5:
        logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–∞")
    else:
        logger.warning("‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å–≤–µ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö")

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    if not results["raw_data"]:
        logger.info("\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –∑–∞–ø—É—Å—Ç–∏—Ç–µ –∑–∞–≥—Ä—É–∑–∫—É –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö:")
        logger.info("   python scripts/load_historical_data_quick.py")

    if not results["processed_data"]:
        logger.info("\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–∞–±–æ—Ç—É ML pipeline:")
        logger.info("   python test_ml_uniqueness.py")


if __name__ == "__main__":
    asyncio.run(main())
