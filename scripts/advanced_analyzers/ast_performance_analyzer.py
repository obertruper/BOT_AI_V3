#!/usr/bin/env python3
"""
–í—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π AST –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è BOT_AI_V3
–¶–µ–ª—å: –∞–Ω–∞–ª–∏–∑ 1959 —Ñ—É–Ω–∫—Ü–∏–π –∑–∞ < 30 —Å–µ–∫—É–Ω–¥
"""
import ast
import asyncio
import hashlib
import json
import pickle
import sys
import time
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor
from functools import lru_cache
from multiprocessing import cpu_count
from pathlib import Path
from typing import Any

PROJECT_ROOT = Path(__file__).parent.parent.parent


class HighPerformanceASTAnalyzer:
    """
    –í—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä AST —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å—é
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
    1. –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Ö–µ—à—É —Ñ–∞–π–ª–∞
    2. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π
    3. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π AST walker
    4. –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    """

    def __init__(self, max_workers: int = None, cache_file: str | None = None):
        self.max_workers = max_workers or min(32, (cpu_count() or 1) + 4)
        self.cache = {}
        self.cache_file = cache_file or str(PROJECT_ROOT / "data" / "ast_cache.pickle")
        self.total_files = 0
        self.processed_files = 0

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–µ—à –µ—Å–ª–∏ –µ—Å—Ç—å
        self._load_cache()

    def _load_cache(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–µ—à –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            cache_path = Path(self.cache_file)
            if cache_path.exists():
                with open(cache_path, "rb") as f:
                    self.cache = pickle.load(f)
                print(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω –∫–µ—à: {len(self.cache)} —Ñ–∞–π–ª–æ–≤")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–µ—à–∞: {e}")
            self.cache = {}

    def _save_cache(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–µ—à –≤ —Ñ–∞–π–ª"""
        try:
            cache_path = Path(self.cache_file)
            cache_path.parent.mkdir(parents=True, exist_ok=True)
            with open(cache_path, "wb") as f:
                pickle.dump(self.cache, f)
            print(f"üíæ –ö–µ—à —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {len(self.cache)} —Ñ–∞–π–ª–æ–≤")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–µ—à–∞: {e}")

    async def analyze_codebase_fast(self, project_root: Path) -> dict[str, Any]:
        """–ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ–π –∫–æ–¥–æ–≤–æ–π –±–∞–∑—ã"""
        start_time = time.time()

        print("üöÄ –ó–∞–ø—É—Å–∫ –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ AST")
        print(f"   –í–æ—Ä–∫–µ—Ä–æ–≤: {self.max_workers}")
        print(f"   –ü—Ä–æ–µ–∫—Ç: {project_root}")

        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ Python —Ñ–∞–π–ª—ã
        python_files = [
            f
            for f in project_root.rglob("*.py")
            if not any(
                exclude in str(f)
                for exclude in [
                    "__pycache__",
                    ".venv",
                    "venv",
                    ".git",
                    "node_modules",
                    "htmlcov",
                    ".pytest_cache",
                    "analysis_results",
                    "BOT_AI_V2",
                    "BOT_Trading",
                    "__MACOSX",  # –ò—Å–∫–ª—é—á–∞–µ–º —Å—Ç–∞—Ä—É—é –≤–µ—Ä—Å–∏—é
                ]
            )
        ]

        self.total_files = len(python_files)
        print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ Python —Ñ–∞–π–ª–æ–≤: {self.total_files}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ —Ñ–∞–π–ª—ã –Ω—É–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å
        files_to_process = []
        cache_hits = 0

        for file_path in python_files:
            file_hash = self._get_file_hash(str(file_path))
            cache_key = f"{file_path}#{file_hash}"

            if cache_key not in self.cache:
                files_to_process.append(file_path)
            else:
                cache_hits += 1

        print(f"üéØ Cache hits: {cache_hits}/{self.total_files}")
        print(f"üîÑ –§–∞–π–ª–æ–≤ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {len(files_to_process)}")

        if not files_to_process:
            print("‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã –≤ –∫–µ—à–µ, —Å–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...")
            return self._merge_cached_results(python_files)

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        batch_size = max(1, len(files_to_process) // self.max_workers)
        batches = [
            files_to_process[i : i + batch_size]
            for i in range(0, len(files_to_process), batch_size)
        ]

        print(f"üì¶ –°–æ–∑–¥–∞–Ω–æ –±–∞—Ç—á–µ–π: {len(batches)} (—Ä–∞–∑–º–µ—Ä: {batch_size})")

        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π
        loop = asyncio.get_event_loop()

        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            print("‚ö° –ó–∞–ø—É—Å–∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏...")

            tasks = [
                loop.run_in_executor(
                    executor, self._analyze_batch_static, batch, i + 1, len(batches)
                )
                for i, batch in enumerate(batches)
            ]

            batch_results = await asyncio.gather(*tasks)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
        final_results = self._merge_results(batch_results, python_files)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–µ—à
        self._save_cache()

        elapsed = time.time() - start_time
        print(f"üéâ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω –∑–∞ {elapsed:.2f}—Å")
        print(f"   –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {self.total_files/elapsed:.1f} —Ñ–∞–π–ª–æ–≤/—Å–µ–∫")

        return final_results

    @staticmethod
    def _analyze_batch_static(files_batch: list[Path], batch_num: int, total_batches: int) -> dict:
        """–°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞ (–¥–ª—è multiprocessing)"""
        analyzer = HighPerformanceASTAnalyzer()
        return analyzer._analyze_batch(files_batch, batch_num, total_batches)

    def _analyze_batch(self, files_batch: list[Path], batch_num: int, total_batches: int) -> dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∞—Ç—á —Ñ–∞–π–ª–æ–≤"""
        print(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {batch_num}/{total_batches} ({len(files_batch)} —Ñ–∞–π–ª–æ–≤)")

        batch_results = {
            "functions": {},
            "classes": {},
            "imports": [],  # –≠—Ç–æ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–æ–∫, –∞ –Ω–µ —Å–ª–æ–≤–∞—Ä—å
            "call_graph": defaultdict(set),
            "processed_files": [],
        }

        for i, file_path in enumerate(files_batch):
            try:
                file_hash = self._get_file_hash(str(file_path))
                cache_key = f"{file_path}#{file_hash}"

                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª
                file_result = self._analyze_single_file_optimized(file_path)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
                self.cache[cache_key] = file_result

                # –î–æ–±–∞–≤–ª—è–µ–º –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –±–∞—Ç—á–∞
                self._merge_file_results(batch_results, file_result)
                batch_results["processed_files"].append(str(file_path))

                # –ü—Ä–æ–≥—Ä–µ—Å—Å
                if (i + 1) % 10 == 0:
                    print(f"   üìÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1}/{len(files_batch)} —Ñ–∞–π–ª–æ–≤ –≤ –±–∞—Ç—á–µ {batch_num}")

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ñ–∞–π–ª–µ {file_path}: {e}")
                continue

        print(f"‚úÖ –ë–∞—Ç—á {batch_num} –∑–∞–≤–µ—Ä—à—ë–Ω")
        return batch_results

    @lru_cache(maxsize=10000)
    def _get_file_hash(self, file_path: str) -> str:
        """–•–µ—à —Ñ–∞–π–ª–∞ –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        try:
            with open(file_path, "rb") as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception:
            return "error"

    def _analyze_single_file_optimized(self, file_path: Path) -> dict:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        try:
            with open(file_path, encoding="utf-8") as f:
                content = f.read()

            # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª
            if not content.strip():
                return self._empty_file_result()

            tree = ast.parse(content, filename=str(file_path))

            # –ë—ã—Å—Ç—Ä—ã–π AST walker
            visitor = FastASTVisitor(str(file_path))
            visitor.visit(tree)

            return visitor.get_results()

        except SyntaxError as e:
            return {"error": f"Syntax error: {e}", "functions": {}, "classes": {}, "imports": []}
        except Exception as e:
            return {"error": f"Analysis error: {e}", "functions": {}, "classes": {}, "imports": []}

    def _empty_file_result(self) -> dict:
        """–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –ø—É—Å—Ç–æ–≥–æ —Ñ–∞–π–ª–∞"""
        return {"functions": {}, "classes": {}, "imports": [], "call_graph": {}}

    def _merge_file_results(self, batch_results: dict, file_result: dict):
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ñ–∞–π–ª–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –±–∞—Ç—á–∞"""
        batch_results["functions"].update(file_result.get("functions", {}))
        batch_results["classes"].update(file_result.get("classes", {}))

        # –ü—Ä–∞–≤–∏–ª—å–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏–º–ø–æ—Ä—Ç—ã (—ç—Ç–æ —Å–ø–∏—Å–æ–∫, –∞ –Ω–µ —Å–ª–æ–≤–∞—Ä—å)
        file_imports = file_result.get("imports", [])
        if isinstance(file_imports, list):
            batch_results["imports"].extend(file_imports)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º call graph
        for caller, callees in file_result.get("call_graph", {}).items():
            if isinstance(callees, (list, set)):
                batch_results["call_graph"][caller].update(callees)

    def _merge_results(self, batch_results: list[dict], all_files: list[Path]) -> dict:
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –±–∞—Ç—á–µ–π"""
        print("üîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–∞—Ç—á–µ–π...")

        final_result = {
            "functions": {},
            "classes": {},
            "imports": [],  # –°–ø–∏—Å–æ–∫ –∏–º–ø–æ—Ä—Ç–æ–≤
            "call_graph": defaultdict(set),
            "statistics": {},
            "processed_files": [],
        }

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–∞—Ç—á–µ–π
        for batch_result in batch_results:
            final_result["functions"].update(batch_result.get("functions", {}))
            final_result["classes"].update(batch_result.get("classes", {}))
            final_result["imports"].extend(batch_result.get("imports", []))  # –†–∞—Å—à–∏—Ä—è–µ–º —Å–ø–∏—Å–æ–∫
            final_result["processed_files"].extend(batch_result.get("processed_files", []))

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º call graph
            for caller, callees in batch_result.get("call_graph", {}).items():
                if isinstance(callees, (list, set)):
                    final_result["call_graph"][caller].update(callees)

        # –î–æ–±–∞–≤–ª—è–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        cached_results = self._get_cached_results_for_files(all_files)
        final_result["functions"].update(cached_results.get("functions", {}))
        final_result["classes"].update(cached_results.get("classes", {}))
        final_result["imports"].extend(cached_results.get("imports", []))  # –†–∞—Å—à–∏—Ä—è–µ–º —Å–ø–∏—Å–æ–∫

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º set –≤ list –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        final_result["call_graph"] = {
            caller: list(callees) for caller, callees in final_result["call_graph"].items()
        }

        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        final_result["statistics"] = {
            "total_files": len(all_files),
            "total_functions": len(final_result["functions"]),
            "total_classes": len(final_result["classes"]),
            "total_imports": len(final_result["imports"]),
            "processed_files": len(final_result["processed_files"]),
        }

        print("üìä –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   –§–∞–π–ª–æ–≤: {final_result['statistics']['total_files']}")
        print(f"   –§—É–Ω–∫—Ü–∏–π: {final_result['statistics']['total_functions']}")
        print(f"   –ö–ª–∞—Å—Å–æ–≤: {final_result['statistics']['total_classes']}")
        print(f"   –ò–º–ø–æ—Ä—Ç–æ–≤: {final_result['statistics']['total_imports']}")

        return final_result

    def _merge_cached_results(self, all_files: list[Path]) -> dict:
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ –∫–µ—à–∞ –¥–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤"""
        print("üìÅ –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ –∫–µ—à–∞...")

        final_result = {
            "functions": {},
            "classes": {},
            "imports": [],  # –°–ø–∏—Å–æ–∫ –∏–º–ø–æ—Ä—Ç–æ–≤
            "call_graph": defaultdict(set),
            "statistics": {},
        }

        for file_path in all_files:
            file_hash = self._get_file_hash(str(file_path))
            cache_key = f"{file_path}#{file_hash}"

            if cache_key in self.cache:
                file_result = self.cache[cache_key]
                self._merge_file_results(final_result, file_result)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º set –≤ list
        final_result["call_graph"] = {
            caller: list(callees) for caller, callees in final_result["call_graph"].items()
        }

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        final_result["statistics"] = {
            "total_files": len(all_files),
            "total_functions": len(final_result["functions"]),
            "total_classes": len(final_result["classes"]),
            "total_imports": len(final_result["imports"]),
            "cache_source": True,
        }

        return final_result

    def _get_cached_results_for_files(self, files: list[Path]) -> dict:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Ñ–∞–π–ª–æ–≤"""
        cached_result = {
            "functions": {},
            "classes": {},
            "imports": [],  # –°–ø–∏—Å–æ–∫ –∏–º–ø–æ—Ä—Ç–æ–≤
            "call_graph": defaultdict(set),
        }

        for file_path in files:
            file_hash = self._get_file_hash(str(file_path))
            cache_key = f"{file_path}#{file_hash}"

            if cache_key in self.cache:
                file_result = self.cache[cache_key]
                self._merge_file_results(cached_result, file_result)

        return cached_result


class FastASTVisitor(ast.NodeVisitor):
    """
    –ë—ã—Å—Ç—Ä—ã–π visitor –¥–ª—è AST —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞–∫–ª–∞–¥–Ω—ã–º–∏ —Ä–∞—Å—Ö–æ–¥–∞–º–∏
    """

    def __init__(self, file_path: str):
        self.file_path = file_path
        self.functions = {}
        self.classes = {}
        self.imports = []
        self.call_graph = defaultdict(set)
        self.current_class = None
        self.current_function = None

    def visit_FunctionDef(self, node: ast.FunctionDef):
        """–ê–Ω–∞–ª–∏–∑ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏"""
        func_name = f"{self.file_path}:{node.name}"

        self.functions[func_name] = {
            "name": node.name,
            "line": node.lineno,
            "file": self.file_path,
            "args": [arg.arg for arg in node.args.args],
            "decorators": [self._get_decorator_name(d) for d in node.decorator_list],
            "is_async": isinstance(node, ast.AsyncFunctionDef),
            "is_method": self.current_class is not None,
            "class_name": self.current_class,
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        prev_function = self.current_function
        self.current_function = func_name

        # –û–±—Ö–æ–¥–∏–º —Ç–µ–ª–æ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—ã–∑–æ–≤–æ–≤
        self.generic_visit(node)

        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        self.current_function = prev_function

    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef):
        """–ê–Ω–∞–ª–∏–∑ async —Ñ—É–Ω–∫—Ü–∏–π"""
        self.visit_FunctionDef(node)

    def visit_ClassDef(self, node: ast.ClassDef):
        """–ê–Ω–∞–ª–∏–∑ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–∞"""
        class_name = f"{self.file_path}:{node.name}"

        self.classes[class_name] = {
            "name": node.name,
            "line": node.lineno,
            "file": self.file_path,
            "base_classes": [self._get_base_name(base) for base in node.bases],
            "decorators": [self._get_decorator_name(d) for d in node.decorator_list],
            "methods": [],
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        prev_class = self.current_class
        self.current_class = node.name

        # –û–±—Ö–æ–¥–∏–º —Ç–µ–ª–æ –∫–ª–∞—Å—Å–∞
        self.generic_visit(node)

        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        self.current_class = prev_class

    def visit_Call(self, node: ast.Call):
        """–ê–Ω–∞–ª–∏–∑ –≤—ã–∑–æ–≤–æ–≤ —Ñ—É–Ω–∫—Ü–∏–π"""
        if self.current_function:
            called_func = self._get_call_name(node)
            if called_func:
                self.call_graph[self.current_function].add(called_func)

        self.generic_visit(node)

    def visit_Import(self, node: ast.Import):
        """–ê–Ω–∞–ª–∏–∑ –∏–º–ø–æ—Ä—Ç–æ–≤"""
        for alias in node.names:
            self.imports.append(
                {"type": "import", "module": alias.name, "alias": alias.asname, "line": node.lineno}
            )

    def visit_ImportFrom(self, node: ast.ImportFrom):
        """–ê–Ω–∞–ª–∏–∑ from-–∏–º–ø–æ—Ä—Ç–æ–≤"""
        for alias in node.names:
            self.imports.append(
                {
                    "type": "from",
                    "module": node.module,
                    "name": alias.name,
                    "alias": alias.asname,
                    "line": node.lineno,
                }
            )

    def _get_decorator_name(self, decorator: ast.expr) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–º—è –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–∞"""
        if isinstance(decorator, ast.Name):
            return decorator.id
        elif isinstance(decorator, ast.Attribute):
            return f"{self._get_attribute_name(decorator.value)}.{decorator.attr}"
        else:
            return str(decorator)

    def _get_base_name(self, base: ast.expr) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–º—è –±–∞–∑–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞"""
        if isinstance(base, ast.Name):
            return base.id
        elif isinstance(base, ast.Attribute):
            return f"{self._get_attribute_name(base.value)}.{base.attr}"
        else:
            return str(base)

    def _get_call_name(self, call: ast.Call) -> str | None:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–º—è –≤—ã–∑—ã–≤–∞–µ–º–æ–π —Ñ—É–Ω–∫—Ü–∏–∏"""
        if isinstance(call.func, ast.Name):
            return call.func.id
        elif isinstance(call.func, ast.Attribute):
            return f"{self._get_attribute_name(call.func.value)}.{call.func.attr}"
        else:
            return None

    def _get_attribute_name(self, attr: ast.expr) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–º—è –∞—Ç—Ä–∏–±—É—Ç–∞"""
        if isinstance(attr, ast.Name):
            return attr.id
        elif isinstance(attr, ast.Attribute):
            return f"{self._get_attribute_name(attr.value)}.{attr.attr}"
        else:
            return "unknown"

    def get_results(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞"""
        return {
            "functions": self.functions,
            "classes": self.classes,
            "imports": self.imports,
            "call_graph": {k: list(v) for k, v in self.call_graph.items()},
        }


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    if len(sys.argv) > 1:
        project_path = Path(sys.argv[1])
    else:
        project_path = PROJECT_ROOT

    analyzer = HighPerformanceASTAnalyzer()

    print(f"üéØ –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞: {project_path}")
    results = await analyzer.analyze_codebase_fast(project_path)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output_file = PROJECT_ROOT / "analysis_results" / "fast_ast_analysis.json"
    output_file.parent.mkdir(parents=True, exist_ok=True)

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_file}")

    return results


if __name__ == "__main__":
    asyncio.run(main())
