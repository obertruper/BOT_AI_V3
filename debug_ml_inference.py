#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ ML inference –ø—Ä–æ–±–ª–µ–º—ã
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤—Å–µ —ç—Ç–∞–ø—ã ML pipeline –æ—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –¥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
"""

import asyncio
import logging
import pickle
import sys
from pathlib import Path

import numpy as np
import pandas as pd
import torch

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.DEBUG, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent))

from ml.logic.feature_engineering import FeatureEngineer
from ml.logic.patchtst_model import create_unified_model


async def debug_ml_inference():
    """–ü–æ–ª–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ ML inference pipeline"""

    print("üîç === –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê ML INFERENCE PIPELINE ===")

    try:
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        model_config = {
            "model": {
                "input_size": 240,
                "output_size": 20,
                "context_window": 96,
                "patch_len": 16,
                "stride": 8,
                "d_model": 256,
                "n_heads": 4,
                "e_layers": 3,
                "d_ff": 512,
                "dropout": 0.1,
                "temperature_scaling": True,
                "temperature": 2.0,
            }
        }

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"üñ•Ô∏è  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

        # === 1. –ü–†–û–í–ï–†–ö–ê –ó–ê–ì–†–£–ó–ö–ò –ú–û–î–ï–õ–ò ===
        print("\n1Ô∏è‚É£ === –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò ===")

        model_path = Path("models/saved/best_model_20250728_215703.pth")
        if not model_path.exists():
            model_path = Path("ml/models/saved/best_model.pth")

        print(f"üìÅ –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏: {model_path}")
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {model_path.exists()}")

        if model_path.exists():
            # –ó–∞–≥—Ä—É–∂–∞–µ–º checkpoint –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            checkpoint = torch.load(model_path, map_location=device)
            print(f"üìä –ö–ª—é—á–∏ –≤ checkpoint: {list(checkpoint.keys())}")

            if "model_state_dict" in checkpoint:
                state_dict = checkpoint["model_state_dict"]
                print(f"üî¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –º–æ–¥–µ–ª–∏: {len(state_dict)}")

                # –ü–µ—á–∞—Ç–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–π –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                print("üóùÔ∏è –ü–µ—Ä–≤—ã–µ 10 –∫–ª—é—á–µ–π –º–æ–¥–µ–ª–∏:")
                for i, key in enumerate(list(state_dict.keys())[:10]):
                    print(f"   {i + 1}. {key}: {state_dict[key].shape}")

        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        print("\nüèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        model = create_unified_model(model_config)
        model.to(device)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
        if model_path.exists():
            checkpoint = torch.load(model_path, map_location=device)
            model.load_state_dict(checkpoint["model_state_dict"])
            print("‚úÖ –í–µ—Å–∞ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

        model.eval()
        print(f"üéØ –ú–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º–µ eval: {not model.training}")

        # === 2. –ü–†–û–í–ï–†–ö–ê SCALER ===
        print("\n2Ô∏è‚É£ === –ó–ê–ì–†–£–ó–ö–ê SCALER ===")

        scaler_path = Path("models/saved/data_scaler.pkl")
        if not scaler_path.exists():
            scaler_path = Path("ml/models/saved/data_scaler.pkl")

        print(f"üìÅ –ü—É—Ç—å –∫ scaler: {scaler_path}")
        print(f"‚úÖ Scaler —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {scaler_path.exists()}")

        scaler = None
        if scaler_path.exists():
            with open(scaler_path, "rb") as f:
                scaler = pickle.load(f)
            print(f"üìä –¢–∏–ø scaler: {type(scaler)}")
            if hasattr(scaler, "n_features_in_"):
                print(f"üî¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ scaler: {scaler.n_features_in_}")

        # === 3. –ì–ï–ù–ï–†–ê–¶–ò–Ø –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–• ===
        print("\n3Ô∏è‚É£ === –ì–ï–ù–ï–†–ê–¶–ò–Ø –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–• ===")

        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ OHLCV –¥–∞–Ω–Ω—ã–µ (–∫–∞–∫ –±—É–¥—Ç–æ –ø–æ–ª—É—á–µ–Ω—ã —Å –±–∏—Ä–∂–∏)
        dates = pd.date_range(start="2024-01-01", periods=200, freq="15min")
        base_price = 50000.0

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ OHLCV –¥–∞–Ω–Ω—ã–µ
        np.random.seed(42)  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        price_changes = np.random.normal(0, 0.01, len(dates))  # 1% –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å

        prices = [base_price]
        for change in price_changes[1:]:
            new_price = prices[-1] * (1 + change)
            prices.append(new_price)

        # –°–æ–∑–¥–∞–µ–º OHLCV —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π –≤–∞—Ä–∏–∞—Ü–∏–µ–π
        test_data = []
        for i, (date, close) in enumerate(zip(dates, prices)):
            high = close * (1 + abs(np.random.normal(0, 0.005)))  # –í—ã—Å–æ—Ç–∞ —Å–≤–µ—á–∏
            low = close * (1 - abs(np.random.normal(0, 0.005)))  # –ù–∏–∑ —Å–≤–µ—á–∏
            open_price = (
                prices[i - 1] if i > 0 else close
            )  # –û—Ç–∫—Ä—ã—Ç–∏–µ = –ø—Ä–µ–¥—ã–¥—É—â–µ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ
            volume = np.random.uniform(100, 1000)  # –°–ª—É—á–∞–π–Ω—ã–π –æ–±—ä–µ–º

            test_data.append(
                {
                    "datetime": date,
                    "open": open_price,
                    "high": max(high, close, open_price),
                    "low": min(low, close, open_price),
                    "close": close,
                    "volume": volume,
                    "symbol": "BTCUSDT",
                }
            )

        test_df = pd.DataFrame(test_data)
        print(f"üìä –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(test_df)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π OHLCV")
        print(
            f"üìà –î–∏–∞–ø–∞–∑–æ–Ω —Ü–µ–Ω: {test_df['close'].min():.2f} - {test_df['close'].max():.2f}"
        )
        print("üìã –ü–µ—Ä–≤—ã–µ 3 –∑–∞–ø–∏—Å–∏:")
        print(test_df.head(3)[["datetime", "open", "high", "low", "close", "volume"]])

        # === 4. FEATURE ENGINEERING ===
        print("\n4Ô∏è‚É£ === FEATURE ENGINEERING ===")

        feature_engineer = FeatureEngineer()
        print("üîß FeatureEngineer —Å–æ–∑–¥–∞–Ω")

        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 120 –∑–∞–ø–∏—Å–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–±–æ–ª—å—à–µ —á–µ–º context_window=96)
        recent_data = test_df.tail(120).copy()
        print(
            f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ {len(recent_data)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è feature engineering"
        )

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        try:
            features_array = feature_engineer.create_features(recent_data)
            print("‚úÖ Feature engineering —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω")
            print(f"üî¢ –†–∞–∑–º–µ—Ä –º–∞—Å—Å–∏–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {features_array.shape}")
            print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
            print(f"   Min: {features_array.min():.6f}")
            print(f"   Max: {features_array.max():.6f}")
            print(f"   Mean: {features_array.mean():.6f}")
            print(f"   Std: {features_array.std():.6f}")
            print(f"   NaN count: {np.isnan(features_array).sum()}")
            print(f"   Inf count: {np.isinf(features_array).sum()}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            if np.isnan(features_array).any():
                print("‚ö†Ô∏è  –û–ë–ù–ê–†–£–ñ–ï–ù–´ NaN –∑–Ω–∞—á–µ–Ω–∏—è!")
            if np.isinf(features_array).any():
                print("‚ö†Ô∏è  –û–ë–ù–ê–†–£–ñ–ï–ù–´ Inf –∑–Ω–∞—á–µ–Ω–∏—è!")

            # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ context_window —Å—Ç—Ä–æ–∫ –¥–ª—è –º–æ–¥–µ–ª–∏
            context_window = 96
            if len(features_array) >= context_window:
                model_input = features_array[-context_window:]
                print(f"üéØ –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏: {model_input.shape}")
            else:
                print(
                    f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö! –ù—É–∂–Ω–æ {context_window}, –µ—Å—Ç—å {len(features_array)}"
                )
                return

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ feature engineering: {e}")
            import traceback

            traceback.print_exc()
            return

        # === 5. –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –î–ê–ù–ù–´–• ===
        print("\n5Ô∏è‚É£ === –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –î–ê–ù–ù–´–• ===")

        if scaler:
            print("üîÑ –ü—Ä–∏–º–µ–Ω—è–µ–º scaler...")
            try:
                features_scaled = scaler.transform(model_input)
                print("‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞")
                print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:")
                print(f"   Min: {features_scaled.min():.6f}")
                print(f"   Max: {features_scaled.max():.6f}")
                print(f"   Mean: {features_scaled.mean():.6f}")
                print(f"   Std: {features_scaled.std():.6f}")

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
                if abs(features_scaled.mean()) > 1.0:
                    print(
                        "‚ö†Ô∏è  –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏!"
                    )
                if features_scaled.std() > 5.0:
                    print(
                        "‚ö†Ô∏è  –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–∞—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏!"
                    )

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
                features_scaled = model_input
        else:
            print("‚ö†Ô∏è  Scaler –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—ã—Ä—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")
            features_scaled = model_input

        # === 6. INFERENCE –ú–û–î–ï–õ–ò ===
        print("\n6Ô∏è‚É£ === INFERENCE –ú–û–î–ï–õ–ò ===")

        print("üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–Ω–∑–æ—Ä–∞ –¥–ª—è –º–æ–¥–µ–ª–∏...")
        x_tensor = torch.FloatTensor(features_scaled).unsqueeze(0).to(device)
        print(f"üìä –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞: {x_tensor.shape}")
        print(f"üñ•Ô∏è  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —Ç–µ–Ω–∑–æ—Ä–∞: {x_tensor.device}")
        print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞:")
        print(f"   Min: {x_tensor.min().item():.6f}")
        print(f"   Max: {x_tensor.max().item():.6f}")
        print(f"   Mean: {x_tensor.mean().item():.6f}")
        print(f"   Std: {x_tensor.std().item():.6f}")

        print("üß† –ó–∞–ø—É—Å–∫ inference...")
        try:
            with torch.no_grad():
                outputs = model(x_tensor)

            print("‚úÖ Inference –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            print(f"üìä –†–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞: {outputs.shape}")

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            outputs_np = outputs.cpu().numpy()[0]
            print("üìä –í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (20 –∑–Ω–∞—á–µ–Ω–∏–π):")
            for i, value in enumerate(outputs_np):
                print(f"   –í—ã—Ö–æ–¥ {i:2d}: {value:8.6f}")

            print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã—Ö–æ–¥–æ–≤:")
            print(f"   Min: {outputs_np.min():.6f}")
            print(f"   Max: {outputs_np.max():.6f}")
            print(f"   Mean: {outputs_np.mean():.6f}")
            print(f"   Std: {outputs_np.std():.6f}")
            print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {len(np.unique(np.round(outputs_np, 6)))}")

            # === –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ü–†–û–ë–õ–ï–ú–´ ===
            print("\nüîç === –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ü–†–û–ë–õ–ï–ú–´ ===")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≤—ã—Ö–æ–¥–æ–≤
            unique_outputs = len(np.unique(np.round(outputs_np, 3)))
            print(f"üéØ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (—Å —Ç–æ—á–Ω–æ—Å—Ç—å—é –¥–æ 3 –∑–Ω–∞–∫–æ–≤): {unique_outputs}")

            if unique_outputs < 5:
                print(
                    "üö® –ü–†–û–ë–õ–ï–ú–ê: –°–ª–∏—à–∫–æ–º –º–∞–ª–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤—ã—Ö–æ–¥–æ–≤ - –º–æ–¥–µ–ª—å –¥–∞–µ—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è!"
                )

                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
                print("\nüîç –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:")

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏
                total_params = sum(p.numel() for p in model.parameters())
                print(f"üìä –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏: {total_params:,}")

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (–Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤ eval —Ä–µ–∂–∏–º–µ)
                requires_grad = sum(p.requires_grad for p in model.parameters())
                print(f"üéØ –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ç—Ä–µ–±—É—é—â–∏—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤: {requires_grad}")

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–π —Å–ª–æ–π
                first_layer = None
                for name, module in model.named_modules():
                    if isinstance(module, torch.nn.Linear):
                        first_layer = module
                        break

                if first_layer:
                    print(f"üîß –ü–µ—Ä–≤—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π: {first_layer}")
                    print(
                        f"   –í–µ—Å–∞: min={first_layer.weight.min():.6f}, max={first_layer.weight.max():.6f}"
                    )
                    if first_layer.bias is not None:
                        print(
                            f"   –ë–∏–∞—Å—ã: min={first_layer.bias.min():.6f}, max={first_layer.bias.max():.6f}"
                        )

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–∏—è –≤ —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–≥–æ–Ω–∞—Ö
                print("\nüîÑ –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏...")
                outputs_list = []
                for i in range(3):
                    with torch.no_grad():
                        test_output = model(x_tensor)
                    outputs_list.append(test_output.cpu().numpy()[0])
                    print(f"   –ü—Ä–æ–≥–æ–Ω {i + 1}: {test_output.cpu().numpy()[0][:5]}...")

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                all_same = all(
                    np.allclose(outputs_list[0], out, atol=1e-8)
                    for out in outputs_list[1:]
                )
                print(f"üéØ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã: {all_same}")

                if all_same:
                    print("‚úÖ –ú–æ–¥–µ–ª—å –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–Ω–∞ (–Ω–æ—Ä–º–∞–ª—å–Ω–æ)")
                else:
                    print(
                        "‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –¥–∞–µ—Ç —Ä–∞–∑–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–±–ª–µ–º–∞ —Å dropout/batch_norm)"
                    )

            else:
                print("‚úÖ –í—ã—Ö–æ–¥—ã –º–æ–¥–µ–ª–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã - —ç—Ç–æ —Ö–æ—Ä–æ—à–æ")

            # === –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
            print("\n7Ô∏è‚É£ === –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===")

            # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤—ã—Ö–æ–¥–æ–≤ —Å–æ–≥–ª–∞—Å–Ω–æ –º–æ–¥–µ–ª–∏:
            future_returns = outputs_np[0:4]
            future_directions = outputs_np[4:8]
            long_levels = outputs_np[8:12]
            short_levels = outputs_np[12:16]
            risk_metrics = outputs_np[16:20]

            print("üìä –†–∞–∑–±–æ—Ä –ø–æ –≥—Ä—É–ø–ø–∞–º –≤—ã—Ö–æ–¥–æ–≤:")
            print(f"   Future returns (0-3):    {future_returns}")
            print(f"   Future directions (4-7):  {future_directions}")
            print(f"   Long levels (8-11):       {long_levels}")
            print(f"   Short levels (12-15):     {short_levels}")
            print(f"   Risk metrics (16-19):     {risk_metrics}")

            # –ü—Ä–æ–≤–µ—Ä–∏–º –ª–æ–≥–∏–∫—É –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∫–∞–∫ –≤ MLManager
            weights = np.array([0.4, 0.3, 0.2, 0.1])
            weighted_direction = np.sum(future_directions * weights)
            signal_strength = abs(weighted_direction)

            if weighted_direction > 0.05:
                signal_type = "BUY"
            elif weighted_direction < -0.05:
                signal_type = "SELL"
            else:
                signal_type = "NEUTRAL"

            print("\nüéØ –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏:")
            print(f"   –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {weighted_direction:.6f}")
            print(f"   –°–∏–ª–∞ —Å–∏–≥–Ω–∞–ª–∞: {signal_strength:.6f}")
            print(f"   –¢–∏–ø —Å–∏–≥–Ω–∞–ª–∞: {signal_type}")

            # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è - –≤—ã–≤–æ–¥–∏–º –∏—Ö
            if unique_outputs < 5:
                print("\nüö® –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–ë–õ–ï–ú–ê:")
                print(f"   –ú–æ–¥–µ–ª—å –≤—Å–µ–≥–¥–∞ –¥–∞–µ—Ç —Å–∏–≥–Ω–∞–ª: {signal_type}")
                print(f"   –°–∏–ª—É —Å–∏–≥–Ω–∞–ª–∞: {signal_strength:.6f}")
                print("   –≠—Ç–æ –æ–±—ä—è—Å–Ω—è–µ—Ç –ø–æ—á–µ–º—É –≤—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ–¥–∏–Ω–∞–∫–æ–≤—ã!")

                # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é
                print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–°–ü–†–ê–í–õ–ï–ù–ò–Æ:")
                print("   1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏")
                print("   2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
                print("   3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏ –Ω–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å")
                print("   4. –í–æ–∑–º–æ–∂–Ω–æ –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∞ –∏–ª–∏ –Ω–µ –æ–±—É—á–µ–Ω–∞")
                print("   5. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π checkpoint")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ inference: {e}")
            import traceback

            traceback.print_exc()
            return

        print("\nüéâ === –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê ===")

    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(debug_ml_inference())
