"""
ML Prediction Logger - Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ‘Ğ”
"""

import hashlib
import json
import time
from datetime import UTC, datetime
from typing import Any

import numpy as np
import pandas as pd

from core.logger import setup_logger
from database.db_manager import get_db
from database.repositories.ml_prediction_repository import MLPrediction

logger = setup_logger("ml_prediction_logger")


class MLPredictionLogger:
    """
    ĞšĞ»Ğ°ÑÑ Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ML Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ² Ğ‘Ğ”
    """

    def __init__(self):
        """Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ³ĞµÑ€Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹"""
        self.model_version = "unified_patchtst_v1.0"
        self.batch_predictions = []
        self.batch_size = 1  # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ÑÑ€Ğ°Ğ·Ñƒ Ğ¶Ğµ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸
        self._db_manager = None

    async def _get_db_manager(self):
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ Ğ‘Ğ” (Ğ»ĞµĞ½Ğ¸Ğ²Ğ°Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ)"""
        if self._db_manager is None:
            self._db_manager = await get_db()
        return self._db_manager

    async def log_prediction(
        self,
        symbol: str,
        features: np.ndarray,
        model_outputs: np.ndarray,
        predictions: dict[str, Any],
        market_data: pd.DataFrame | None = None,
    ) -> None:
        """
        Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ

        Args:
            symbol: Ğ¢Ğ¾Ñ€Ğ³Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»
            features: ĞœĞ°ÑÑĞ¸Ğ² Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² (240 features)
            model_outputs: Ğ¡Ñ‹Ñ€Ñ‹Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (20 values)
            predictions: Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ
            market_data: Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€Ñ‹Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
        """
        start_time = time.time()

        # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ñ…ÑÑˆ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸
        features_hash = self._compute_features_hash(features)

        # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
        key_features = self._extract_key_features(features, market_data)

        # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²
        feature_stats = self._compute_feature_statistics(features)

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚ MLPrediction
        prediction_record = MLPrediction(
            symbol=symbol,
            timestamp=int(time.time() * 1000),
            predicted_at=datetime.now(UTC),
            # Input features summary
            features_count=len(features),
            features_hash=features_hash,
            lookback_periods=96,  # Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ
            # Key features
            **key_features,
            # Feature statistics  
            **feature_stats,
            # Model outputs - raw predictions
            predicted_return_15m=float(predictions.get("returns_15m", 0)),
            predicted_return_1h=float(predictions.get("returns_1h", 0)),
            predicted_return_4h=float(predictions.get("returns_4h", 0)),
            predicted_return_12h=float(predictions.get("returns_12h", 0)),
            # Direction predictions
            direction_15m=predictions["direction_15m"],
            direction_15m_confidence=float(predictions["confidence_15m"]),
            direction_1h=predictions["direction_1h"],
            direction_1h_confidence=float(predictions["confidence_1h"]),
            direction_4h=predictions["direction_4h"],
            direction_4h_confidence=float(predictions["confidence_4h"]),
            direction_12h=predictions["direction_12h"],
            direction_12h_confidence=float(predictions["confidence_12h"]),
            # Risk metrics
            risk_score=float(predictions.get("risk_score", 0)),
            max_drawdown_predicted=float(predictions.get("max_drawdown", 0)),
            max_rally_predicted=float(predictions.get("max_rally", 0)),
            # Final signal
            signal_type=predictions["signal_type"],
            signal_confidence=float(predictions["signal_confidence"]),
            signal_timeframe=predictions.get("primary_timeframe", "15m"),
            # Model metadata
            model_version=self.model_version,
            inference_time_ms=(time.time() - start_time) * 1000,
            # Full arrays for detailed analysis
            features_array=features.tolist() if features.size < 1000 else None,
            model_outputs_raw=model_outputs.tolist() if model_outputs is not None else None,
        )

        # Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
        self._log_prediction_details(symbol, prediction_record, predictions)

        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² Ğ±Ğ°Ñ‚Ñ‡ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ
        self.batch_predictions.append(prediction_record)

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² Ğ‘Ğ” ĞµÑĞ»Ğ¸ Ğ½Ğ°Ğ±Ñ€Ğ°Ğ»ÑÑ Ğ±Ğ°Ñ‚Ñ‡
        if len(self.batch_predictions) >= self.batch_size:
            await self._save_batch_to_db()

    def _compute_features_hash(self, features: np.ndarray) -> int:
        """Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ñ…ÑÑˆ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸"""
        # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ² bytes Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ñ…ÑÑˆ
        features_bytes = features.astype(np.float32).tobytes()
        hash_obj = hashlib.md5(features_bytes)
        # Ğ‘ĞµÑ€ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 8 Ğ±Ğ°Ğ¹Ñ‚ Ñ…ÑÑˆĞ° Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ¾ int64 range
        # PostgreSQL int64 range: -9223372036854775808 to 9223372036854775807
        hash_int = int(
            hash_obj.hexdigest()[:15], 16
        )  # 15 hex chars = ~56 bits, safely within int64
        return hash_int

    def _extract_key_features(
        self, features: np.ndarray, market_data: pd.DataFrame | None
    ) -> dict[str, float]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°"""
        key_features = {}

        # ĞŸÑ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑÑ‹ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²
        # (Ğ½ÑƒĞ¶Ğ½Ğ¾ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ feature_engineering_v2.py)
        feature_indices = {
            "rsi": 14,  # ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ğ´ĞµĞºÑ
            "macd": 20,
            "bb_position": 30,
            "atr_pct": 40,
            "volume_ratio": 7,
            "trend_strength": 50,
        }

        # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ¿Ğ¾ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ğ¼ ĞµÑĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾
        for name, idx in feature_indices.items():
            if idx < len(features):
                key_features[name] = float(features[idx])
            else:
                key_features[name] = None

        # Ğ•ÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ market_data, Ğ±ĞµÑ€ĞµĞ¼ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ñ†ĞµĞ½Ñƒ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ¼
        if market_data is not None and not market_data.empty:
            last_row = market_data.iloc[-1]
            key_features["close_price"] = float(last_row.get("close", 0))
            key_features["volume"] = float(last_row.get("volume", 0))
            # DEBUG: Ğ»Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¸Ğ· market_data
            logger.debug(
                f"DEBUG market_data last row: close={last_row.get('close')}, volume={last_row.get('volume')}"
            )
        else:
            key_features["close_price"] = 0
            key_features["volume"] = 0
            logger.debug("DEBUG: market_data is None or empty")

        return key_features

    def _compute_feature_statistics(self, features: np.ndarray) -> dict[str, float]:
        """Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼"""
        # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğµ Ñ‡Ğ¸ÑĞ»Ğ°
        valid_features = features[~np.isnan(features)]

        stats = {
            "features_mean": float(np.mean(valid_features)) if len(valid_features) > 0 else 0,
            "features_std": float(np.std(valid_features)) if len(valid_features) > 0 else 0,
            "features_min": float(np.min(valid_features)) if len(valid_features) > 0 else 0,
            "features_max": float(np.max(valid_features)) if len(valid_features) > 0 else 0,
            "zero_variance_count": int(np.sum(np.var(features.reshape(-1, 1), axis=0) < 1e-10)),
            "nan_count": int(np.sum(np.isnan(features))),
        }

        return stats

    def _log_prediction_details(
        self, symbol: str, record: MLPrediction, predictions: dict[str, Any]
    ) -> None:
        """Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ"""

        # DEBUG: ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ°ĞµÑ‚ÑÑ
        logger.debug(
            f"DEBUG predictions content: returns_15m={predictions.get('returns_15m')}, "
            f"returns_1h={predictions.get('returns_1h')}, "
            f"returns_4h={predictions.get('returns_4h')}, "
            f"returns_12h={predictions.get('returns_12h')}"
        )

        # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ²ÑÑ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ Ğ² Ğ¾Ğ´Ğ½Ñƒ ÑÑ‚Ñ€Ğ¾ĞºÑƒ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ½Ğ° Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ°ÑÑŒ Ñ†ĞµĞ»Ğ¸ĞºĞ¾Ğ¼
        table_lines = []
        table_lines.append(
            "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
        )
        table_lines.append(
            f"â•‘                    ML PREDICTION DETAILS - {symbol:^10}              â•‘"
        )
        table_lines.append(
            "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£"
        )
        table_lines.append(
            "â•‘ ğŸ“Š INPUT FEATURES                                                     â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ Feature Count: {record.features_count:<6} â€¢ Hash: {record.features_hash:016x}     â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ NaN Count: {record.nan_count:<6} â€¢ Zero Variance: {record.zero_variance_count:<6}        â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ Mean: {record.features_mean:>8.4f}  â€¢ Std: {record.features_std:>8.4f}            â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ Min:  {record.features_min:>8.4f}  â€¢ Max: {record.features_max:>8.4f}            â•‘"
        )
        table_lines.append(
            "â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢"
        )
        table_lines.append(
            "â•‘ ğŸ¯ KEY INDICATORS                                                     â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ Close: ${getattr(record, 'close_price', 0):>10.2f}  â€¢ Volume: {getattr(record, 'volume', 0):>12.0f}  â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ RSI: {getattr(record, 'rsi', 0):>6.2f}  â€¢ MACD: {getattr(record, 'macd', 0):>8.4f}                  â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ BB Position: {getattr(record, 'bb_position', 0):>6.3f}  â€¢ ATR%: {getattr(record, 'atr_pct', 0):>6.3f}   â•‘"
        )
        table_lines.append(
            "â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢"
        )
        table_lines.append(
            "â•‘ ğŸ“ˆ PREDICTED RETURNS                                                  â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ 15m: {record.predicted_return_15m:>7.4f} ({record.direction_15m:^7}) [{record.direction_15m_confidence:>5.2%}]  â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ 1h:  {record.predicted_return_1h:>7.4f} ({record.direction_1h:^7}) [{record.direction_1h_confidence:>5.2%}]   â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ 4h:  {record.predicted_return_4h:>7.4f} ({record.direction_4h:^7}) [{record.direction_4h_confidence:>5.2%}]   â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ 12h: {record.predicted_return_12h:>7.4f} ({record.direction_12h:^7}) [{record.direction_12h_confidence:>5.2%}] â•‘"
        )
        table_lines.append(
            "â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢"
        )
        table_lines.append(
            "â•‘ âš ï¸  RISK METRICS                                                      â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ Risk Score: {getattr(record, 'risk_score', 0):>6.3f}                                       â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ Max Drawdown: {getattr(record, 'max_drawdown_predicted', 0):>6.2%}                           â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ Max Rally: {getattr(record, 'max_rally_predicted', 0):>6.2%}                              â•‘"
        )
        table_lines.append(
            "â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢"
        )
        table_lines.append(
            "â•‘ ğŸ¯ FINAL SIGNAL                                                       â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ Type: {record.signal_type:^10}  â€¢ Confidence: {record.signal_confidence:>5.2%}       â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ Primary Timeframe: {getattr(record, 'signal_timeframe', 'N/A'):^10}                      â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ Inference Time: {record.inference_time_ms:>6.1f} ms                            â•‘"
        )
        table_lines.append(
            "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        )

        # Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ğ²ÑÑ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ¼ logger.info
        logger.info("\n" + "\n".join(table_lines))

        # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸
        if predictions.get("debug_info"):
            logger.debug(f"Debug info for {symbol}: {predictions['debug_info']}")

    async def _save_batch_to_db(self) -> None:
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ±Ğ°Ñ‚Ñ‡ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ‘Ğ”"""
        if not self.batch_predictions:
            return

        try:
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¼ĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ Ğ‘Ğ” Ğ¸ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹
            db_manager = await self._get_db_manager()
            ml_repo = db_manager.get_ml_prediction_repository()

            # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ bulk_insert Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
            if len(self.batch_predictions) == 1:
                # ĞĞ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ
                await ml_repo.create_prediction(self.batch_predictions[0])
            else:
                # ĞœĞ°ÑÑĞ¾Ğ²Ğ¾Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ
                await ml_repo.bulk_insert(self.batch_predictions)

            logger.info(f"âœ… Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾ {len(self.batch_predictions)} Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ‘Ğ”")

            # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ğ±Ğ°Ñ‚Ñ‡
            self.batch_predictions.clear()

        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ‘Ğ”: {e}")
            # ĞĞµ Ğ¾Ñ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ğ±Ğ°Ñ‚Ñ‡ Ğ¿Ñ€Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞµ, Ğ¿Ğ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ² ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·

    async def save_feature_importance(
        self, feature_names: list[str], importance_scores: np.ndarray
    ) -> None:
        """
        Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ‘Ğ”

        Args:
            feature_names: ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²
            importance_scores: ĞÑ†ĞµĞ½ĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸
        """
        try:
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¼ĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ Ğ‘Ğ” Ğ¸ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹
            db_manager = await self._get_db_manager()
            ml_repo = db_manager.get_ml_prediction_repository()

            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹
            await ml_repo.save_feature_importance(
                feature_names, importance_scores, self.model_version
            )

            logger.info("âœ… Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ° Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾Ğ¿-100 Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ‘Ğ”")

        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²: {e}")

    async def flush(self) -> None:
        """ĞŸÑ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²ÑĞµ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ"""
        if self.batch_predictions:
            await self._save_batch_to_db()


# Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€ Ğ»Ğ¾Ğ³Ğ³ĞµÑ€Ğ°
ml_prediction_logger = MLPredictionLogger()
