"""
ML Prediction Logger - Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ‘Ğ”
"""

import hashlib
import json
import time
from datetime import UTC, datetime
from typing import Any

import numpy as np
import pandas as pd

from core.logger import setup_logger
from database.connections.postgres import AsyncPGPool

logger = setup_logger("ml_prediction_logger")


class MLPredictionLogger:
    """
    ĞšĞ»Ğ°ÑÑ Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ML Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ² Ğ‘Ğ”
    """

    def __init__(self):
        """Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ³ĞµÑ€Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹"""
        self.model_version = "unified_patchtst_v1.0"
        self.batch_predictions = []
        self.batch_size = 1  # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ÑÑ€Ğ°Ğ·Ñƒ Ğ¶Ğµ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸

    async def log_prediction(
        self,
        symbol: str,
        features: np.ndarray,
        model_outputs: np.ndarray,
        predictions: dict[str, Any],
        market_data: pd.DataFrame | None = None,
    ) -> None:
        """
        Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ

        Args:
            symbol: Ğ¢Ğ¾Ñ€Ğ³Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»
            features: ĞœĞ°ÑÑĞ¸Ğ² Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² (240 features)
            model_outputs: Ğ¡Ñ‹Ñ€Ñ‹Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (20 values)
            predictions: Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ
            market_data: Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€Ñ‹Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
        """
        start_time = time.time()

        # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ñ…ÑÑˆ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸
        features_hash = self._compute_features_hash(features)

        # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
        key_features = self._extract_key_features(features, market_data)

        # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²
        feature_stats = self._compute_feature_statistics(features)

        # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ‘Ğ”
        prediction_record = {
            "symbol": symbol,
            "timestamp": int(time.time() * 1000),
            "datetime": datetime.now(UTC),
            # Input features summary
            "features_count": len(features),
            "features_hash": features_hash,
            "lookback_periods": 96,  # Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ
            # Key features
            **key_features,
            # Feature statistics
            **feature_stats,
            # Model outputs - raw predictions
            "predicted_return_15m": float(predictions.get("returns_15m", 0)),
            "predicted_return_1h": float(predictions.get("returns_1h", 0)),
            "predicted_return_4h": float(predictions.get("returns_4h", 0)),
            "predicted_return_12h": float(predictions.get("returns_12h", 0)),
            # Direction predictions
            "direction_15m": predictions["direction_15m"],
            "direction_15m_confidence": float(predictions["confidence_15m"]),
            "direction_1h": predictions["direction_1h"],
            "direction_1h_confidence": float(predictions["confidence_1h"]),
            "direction_4h": predictions["direction_4h"],
            "direction_4h_confidence": float(predictions["confidence_4h"]),
            "direction_12h": predictions["direction_12h"],
            "direction_12h_confidence": float(predictions["confidence_12h"]),
            # Risk metrics
            "risk_score": float(predictions.get("risk_score", 0)),
            "max_drawdown_predicted": float(predictions.get("max_drawdown", 0)),
            "max_rally_predicted": float(predictions.get("max_rally", 0)),
            # Final signal
            "signal_type": predictions["signal_type"],
            "signal_confidence": float(predictions["signal_confidence"]),
            "signal_timeframe": predictions.get("primary_timeframe", "15m"),
            # Model metadata
            "model_version": self.model_version,
            "inference_time_ms": (time.time() - start_time) * 1000,
            # Full arrays for detailed analysis
            "features_array": features.tolist() if features.size < 1000 else None,
            "model_outputs_raw": model_outputs.tolist() if model_outputs is not None else None,
        }

        # Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
        self._log_prediction_details(symbol, prediction_record, predictions)

        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² Ğ±Ğ°Ñ‚Ñ‡ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ
        self.batch_predictions.append(prediction_record)

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² Ğ‘Ğ” ĞµÑĞ»Ğ¸ Ğ½Ğ°Ğ±Ñ€Ğ°Ğ»ÑÑ Ğ±Ğ°Ñ‚Ñ‡
        if len(self.batch_predictions) >= self.batch_size:
            await self._save_batch_to_db()

    def _compute_features_hash(self, features: np.ndarray) -> int:
        """Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ñ…ÑÑˆ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸"""
        # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ² bytes Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ñ…ÑÑˆ
        features_bytes = features.astype(np.float32).tobytes()
        hash_obj = hashlib.md5(features_bytes)
        # Ğ‘ĞµÑ€ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 8 Ğ±Ğ°Ğ¹Ñ‚ Ñ…ÑÑˆĞ° Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ¾ int64 range
        # PostgreSQL int64 range: -9223372036854775808 to 9223372036854775807
        hash_int = int(
            hash_obj.hexdigest()[:15], 16
        )  # 15 hex chars = ~56 bits, safely within int64
        return hash_int

    def _extract_key_features(
        self, features: np.ndarray, market_data: pd.DataFrame | None
    ) -> dict[str, float]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°"""
        key_features = {}

        # ĞŸÑ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑÑ‹ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²
        # (Ğ½ÑƒĞ¶Ğ½Ğ¾ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ feature_engineering_v2.py)
        feature_indices = {
            "rsi": 14,  # ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ğ´ĞµĞºÑ
            "macd": 20,
            "bb_position": 30,
            "atr_pct": 40,
            "volume_ratio": 7,
            "trend_strength": 50,
        }

        # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ¿Ğ¾ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ğ¼ ĞµÑĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾
        for name, idx in feature_indices.items():
            if idx < len(features):
                key_features[name] = float(features[idx])
            else:
                key_features[name] = None

        # Ğ•ÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ market_data, Ğ±ĞµÑ€ĞµĞ¼ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ñ†ĞµĞ½Ñƒ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ¼
        if market_data is not None and not market_data.empty:
            last_row = market_data.iloc[-1]
            key_features["close_price"] = float(last_row.get("close", 0))
            key_features["volume"] = float(last_row.get("volume", 0))
            # DEBUG: Ğ»Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¸Ğ· market_data
            logger.debug(
                f"DEBUG market_data last row: close={last_row.get('close')}, volume={last_row.get('volume')}"
            )
        else:
            key_features["close_price"] = 0
            key_features["volume"] = 0
            logger.debug("DEBUG: market_data is None or empty")

        return key_features

    def _compute_feature_statistics(self, features: np.ndarray) -> dict[str, float]:
        """Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼"""
        # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğµ Ñ‡Ğ¸ÑĞ»Ğ°
        valid_features = features[~np.isnan(features)]

        stats = {
            "features_mean": float(np.mean(valid_features)) if len(valid_features) > 0 else 0,
            "features_std": float(np.std(valid_features)) if len(valid_features) > 0 else 0,
            "features_min": float(np.min(valid_features)) if len(valid_features) > 0 else 0,
            "features_max": float(np.max(valid_features)) if len(valid_features) > 0 else 0,
            "zero_variance_count": int(np.sum(np.var(features.reshape(-1, 1), axis=0) < 1e-10)),
            "nan_count": int(np.sum(np.isnan(features))),
        }

        return stats

    def _log_prediction_details(
        self, symbol: str, record: dict[str, Any], predictions: dict[str, Any]
    ) -> None:
        """Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ"""

        # DEBUG: ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ°ĞµÑ‚ÑÑ
        logger.debug(
            f"DEBUG predictions content: returns_15m={predictions.get('returns_15m')}, "
            f"returns_1h={predictions.get('returns_1h')}, "
            f"returns_4h={predictions.get('returns_4h')}, "
            f"returns_12h={predictions.get('returns_12h')}"
        )

        # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ²ÑÑ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ Ğ² Ğ¾Ğ´Ğ½Ñƒ ÑÑ‚Ñ€Ğ¾ĞºÑƒ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ½Ğ° Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ°ÑÑŒ Ñ†ĞµĞ»Ğ¸ĞºĞ¾Ğ¼
        table_lines = []
        table_lines.append(
            "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
        )
        table_lines.append(
            f"â•‘                    ML PREDICTION DETAILS - {symbol:^10}              â•‘"
        )
        table_lines.append(
            "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£"
        )
        table_lines.append(
            "â•‘ ğŸ“Š INPUT FEATURES                                                     â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ Feature Count: {record['features_count']:<6} â€¢ Hash: {record['features_hash']:016x}     â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ NaN Count: {record['nan_count']:<6} â€¢ Zero Variance: {record['zero_variance_count']:<6}        â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ Mean: {record['features_mean']:>8.4f}  â€¢ Std: {record['features_std']:>8.4f}            â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ Min:  {record['features_min']:>8.4f}  â€¢ Max: {record['features_max']:>8.4f}            â•‘"
        )
        table_lines.append(
            "â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢"
        )
        table_lines.append(
            "â•‘ ğŸ¯ KEY INDICATORS                                                     â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ Close: ${record.get('close_price', 0):>10.2f}  â€¢ Volume: {record.get('volume', 0):>12.0f}  â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ RSI: {record.get('rsi', 0):>6.2f}  â€¢ MACD: {record.get('macd', 0):>8.4f}                  â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ BB Position: {record.get('bb_position', 0):>6.3f}  â€¢ ATR%: {record.get('atr_pct', 0):>6.3f}   â•‘"
        )
        table_lines.append(
            "â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢"
        )
        table_lines.append(
            "â•‘ ğŸ“ˆ PREDICTED RETURNS                                                  â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ 15m: {record['predicted_return_15m']:>7.4f} ({record['direction_15m']:^7}) [{record['direction_15m_confidence']:>5.2%}]  â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ 1h:  {record['predicted_return_1h']:>7.4f} ({record['direction_1h']:^7}) [{record['direction_1h_confidence']:>5.2%}]   â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ 4h:  {record['predicted_return_4h']:>7.4f} ({record['direction_4h']:^7}) [{record['direction_4h_confidence']:>5.2%}]   â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ 12h: {record['predicted_return_12h']:>7.4f} ({record['direction_12h']:^7}) [{record['direction_12h_confidence']:>5.2%}] â•‘"
        )
        table_lines.append(
            "â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢"
        )
        table_lines.append(
            "â•‘ âš ï¸  RISK METRICS                                                      â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ Risk Score: {record.get('risk_score', 0):>6.3f}                                       â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ Max Drawdown: {record.get('max_drawdown_predicted', 0):>6.2%}                           â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ Max Rally: {record.get('max_rally_predicted', 0):>6.2%}                              â•‘"
        )
        table_lines.append(
            "â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢"
        )
        table_lines.append(
            "â•‘ ğŸ¯ FINAL SIGNAL                                                       â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ Type: {record['signal_type']:^10}  â€¢ Confidence: {record['signal_confidence']:>5.2%}       â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ Primary Timeframe: {record.get('signal_timeframe', 'N/A'):^10}                      â•‘"
        )
        table_lines.append(
            f"â•‘   â€¢ Inference Time: {record['inference_time_ms']:>6.1f} ms                            â•‘"
        )
        table_lines.append(
            "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        )

        # Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ğ²ÑÑ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ¼ logger.info
        logger.info("\n" + "\n".join(table_lines))

        # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸
        if predictions.get("debug_info"):
            logger.debug(f"Debug info for {symbol}: {predictions['debug_info']}")

    async def _save_batch_to_db(self) -> None:
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ±Ğ°Ñ‚Ñ‡ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ‘Ğ”"""
        if not self.batch_predictions:
            return

        try:
            # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ SQL Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ´Ğ»Ñ batch insert
            columns = list(self.batch_predictions[0].keys())
            placeholders = ", ".join([f"${i + 1}" for i in range(len(columns))])

            # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ
            insert_query = f"""
                INSERT INTO ml_predictions ({", ".join(columns)})
                VALUES ({placeholders})
                ON CONFLICT (symbol, timestamp) DO UPDATE SET
                    signal_confidence = EXCLUDED.signal_confidence,
                    updated_at = NOW()
            """

            # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ batch insert
            for record in self.batch_predictions:
                values = [record[col] for col in columns]
                # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ None Ğ² NULL Ğ´Ğ»Ñ PostgreSQL
                values = [json.dumps(v) if isinstance(v, (list, dict)) else v for v in values]

                await AsyncPGPool.execute(insert_query, *values)

            logger.info(f"âœ… Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾ {len(self.batch_predictions)} Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ‘Ğ”")

            # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ğ±Ğ°Ñ‚Ñ‡
            self.batch_predictions.clear()

        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ‘Ğ”: {e}")
            # ĞĞµ Ğ¾Ñ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ğ±Ğ°Ñ‚Ñ‡ Ğ¿Ñ€Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞµ, Ğ¿Ğ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ² ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·

    async def save_feature_importance(
        self, feature_names: list[str], importance_scores: np.ndarray
    ) -> None:
        """
        Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ‘Ğ”

        Args:
            feature_names: ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²
            importance_scores: ĞÑ†ĞµĞ½ĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸
        """
        try:
            # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸
            sorted_indices = np.argsort(importance_scores)[::-1]

            # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ¸
            insert_query = """
                INSERT INTO ml_feature_importance
                (feature_name, feature_index, importance_score, importance_rank,
                 model_version, calculated_at)
                VALUES ($1, $2, $3, $4, $5, $6)
            """

            calculated_at = datetime.now(UTC)

            for rank, idx in enumerate(sorted_indices[:100], 1):  # Ğ¢Ğ¾Ğ¿-100 Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²
                await AsyncPGPool.execute(
                    insert_query,
                    feature_names[idx],
                    int(idx),
                    float(importance_scores[idx]),
                    rank,
                    self.model_version,
                    calculated_at,
                )

            logger.info("âœ… Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ° Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾Ğ¿-100 Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ‘Ğ”")

        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²: {e}")

    async def flush(self) -> None:
        """ĞŸÑ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²ÑĞµ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ"""
        if self.batch_predictions:
            await self._save_batch_to_db()


# Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€ Ğ»Ğ¾Ğ³Ğ³ĞµÑ€Ğ°
ml_prediction_logger = MLPredictionLogger()
