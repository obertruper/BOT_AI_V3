#!/usr/bin/env python3
"""
ML Manager Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ PatchTST Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ² BOT Trading v3
"""

import os
import pickle
from datetime import UTC, datetime
from pathlib import Path
from typing import Any

import numpy as np
import pandas as pd
import torch

from core.logger import setup_logger
from core.system.signal_deduplicator import signal_deduplicator
from core.system.worker_coordinator import worker_coordinator
from ml.logic.feature_engineering_production import (  # Production Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¸Ğ· Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ°
    ProductionFeatureEngineer as FeatureEngineer,
)
from ml.logic.patchtst_model import create_unified_model
from ml.logic.signal_quality_analyzer import SignalQualityAnalyzer
from ml.ml_prediction_logger import ml_prediction_logger

logger = setup_logger("ml_manager")

# Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ GPU Ğ¿Ñ€Ğ¸ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ - Ğ¢ĞĞ§ĞĞĞ¯ ĞšĞĞŸĞ˜Ğ¯ Ğ¸Ğ· Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°
if torch.cuda.is_available():
    try:
        # Benchmark mode Ğ´Ğ»Ñ cudnn
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.enabled = True

        # Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° float32 matmul precision Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… GPU
        # torch.set_float32_matmul_precision('high')  # Ğ’Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½Ğ¾ Ğ¸Ğ·-Ğ·Ğ° warning

        # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ampere+ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ (RTX 5090)
        # torch.backends.cuda.matmul.allow_tf32 = True  # Deprecated
        # torch.backends.cudnn.allow_tf32 = True  # Deprecated

        logger.info("âœ… Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ GPU Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ñ‹")
    except Exception as e:
        logger.warning(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ GPU Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸: {e}")


class MLManager:
    """
    ĞœĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ML Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ.
    Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ PatchTST Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ€Ñ‹Ğ½ĞºĞ°.
    """

    def __init__(self, config: dict[str, Any]):
        """
        Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ML Ğ¼ĞµĞ½ĞµĞ´Ğ¶ĞµÑ€Ğ°.

        Args:
            config: ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹
        """
        self.config = config
        self.model = None
        self.scaler = None
        self.feature_engineer = None
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ device Ğ¸Ğ· ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸
        device_config = config.get("ml", {}).get("model", {}).get("device", "auto")

        # Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ GPU Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¾Ğ¹ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
        if device_config == "auto":
            try:
                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ CUDA
                if torch.cuda.is_available():
                    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ GPU
                    gpu_count = torch.cuda.device_count()
                    if gpu_count > 0:
                        # Ğ’Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ GPU Ñ Ğ½Ğ°Ğ¸Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸
                        best_gpu = 0
                        min_memory_used = float("inf")

                        for i in range(gpu_count):
                            try:
                                torch.cuda.set_device(i)
                                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ GPU
                                props = torch.cuda.get_device_properties(i)
                                logger.info(
                                    f"GPU {i}: {props.name}, "
                                    f"Compute Capability: {props.major}.{props.minor}, "
                                    f"Memory: {props.total_memory / 1024**3:.1f}GB"
                                )

                                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ (Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ MagicMock Ğ² Ñ‚ĞµÑÑ‚Ğ°Ñ…)
                                try:
                                    memory_used = torch.cuda.memory_allocated(i)
                                    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ‡Ğ¸ÑĞ»Ğ¾, Ğ° Ğ½Ğµ MagicMock
                                    if (
                                        isinstance(memory_used, (int, float))
                                        and memory_used < min_memory_used
                                    ):
                                        min_memory_used = memory_used
                                        best_gpu = i
                                except (TypeError, AttributeError):
                                    # Ğ’ Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ MagicMock - Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ GPU 0 Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ
                                    logger.debug(
                                        f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ GPU {i}, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ"
                                    )
                                    if i == 0:  # ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ GPU ĞºĞ°Ğº fallback
                                        best_gpu = i
                            except Exception as gpu_error:
                                logger.warning(f"GPU {i} Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½: {gpu_error}")
                                continue

                        # Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ GPU
                        torch.cuda.set_device(best_gpu)
                        self.device = torch.device(f"cuda:{best_gpu}")

                        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ GPU Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ¾Ğ¼
                        test_tensor = torch.zeros(1, 1).to(self.device)
                        _ = test_tensor * 2  # ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸

                        # RTX 5090 (Blackwell) Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸:
                        # - GPU Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ĞµĞ½ Ñ PyTorch 2.9.0+
                        # - torch.compile Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ sm_120
                        # - ĞœĞ¾Ğ¶ĞµÑ‚ Ğ´Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
                        gpu_name = props.name.upper()
                        if "RTX 5090" in gpu_name or props.major >= 12:
                            logger.info(
                                f"ğŸ¯ ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½ RTX 5090 ({gpu_name}, sm_{props.major}{props.minor})"
                            )

                            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºÑƒ torch.compile
                            try:
                                # Ğ¢ĞµÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
                                import torch.nn as nn

                                test_model = nn.Linear(1, 1).to(self.device)
                                compiled_test = torch.compile(test_model)
                                test_input = torch.randn(1, 1).to(self.device)
                                _ = compiled_test(test_input)

                                logger.info(
                                    "âœ… torch.compile Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ´Ğ»Ñ RTX 5090 - Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ!"
                                )
                                # ĞĞ• ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ TORCH_COMPILE_DISABLE - Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ torch.compile

                            except Exception as compile_error:
                                logger.warning(
                                    f"âš ï¸ torch.compile Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ´Ğ»Ñ RTX 5090: {compile_error}. "
                                    "ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼ Ğ±ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸."
                                )
                                os.environ["TORCH_COMPILE_DISABLE"] = "1"

                        logger.info(f"âœ… Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ GPU {best_gpu} ({props.name})")
                        logger.info(
                            f"ğŸ’¾ GPU Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: {props.total_memory / 1024**3:.2f} GB"
                        )
                    else:
                        logger.warning("CUDA Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°, Ğ½Ğ¾ GPU Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹")
                        self.device = torch.device("cpu")
                else:
                    logger.info("CUDA Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ CPU")
                    self.device = torch.device("cpu")

            except Exception as e:
                # Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸
                logger.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ GPU: {type(e).__name__}: {e}")
                logger.info("ĞŸĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ÑÑ Ğ½Ğ° CPU")
                self.device = torch.device("cpu")

                # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ CUDA ĞºĞµÑˆ Ğ¿Ñ€Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞµ
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        else:
            # Ğ ÑƒÑ‡Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° device
            try:
                self.device = torch.device(device_config)
                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ
                if "cuda" in device_config:
                    test_tensor = torch.zeros(1, 1).to(self.device)
                    _ = test_tensor * 2
                logger.info(f"Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ device: {device_config}")
            except Exception as e:
                logger.warning(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ device {device_config}: {e}")
                self.device = torch.device("cpu")

        # Ğ¤Ğ»Ğ°Ğ³ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
        self._initialized = False

        # ĞšÑÑˆ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Ğ´Ğ»Ñ MLManager ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ Ñ‚ĞµÑÑ‚Ğ°Ğ¼Ğ¸)
        self._model_cache = {}
        self._cache_size = config.get("ml", {}).get("model_cache_size", 3)
        self._memory_limit = config.get("ml", {}).get("memory_limit_mb", 1024)

        # ĞŸÑƒÑ‚Ğ¸ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ - Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸
        base_dir = Path(__file__).parent.parent  # ĞšĞ¾Ñ€ĞµĞ½ÑŒ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°
        model_dir = base_dir / config.get("ml", {}).get("model_directory", "models/saved")
        self.model_path = model_dir / "best_model_20250728_215703.pth"
        self.scaler_path = model_dir / "data_scaler.pkl"

        # ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        self.context_length = 96  # 24 Ñ‡Ğ°ÑĞ° Ğ¿Ñ€Ğ¸ 15-Ğ¼Ğ¸Ğ½ÑƒÑ‚Ğ½Ñ‹Ñ… ÑĞ²ĞµÑ‡Ğ°Ñ…
        self.num_features = 240  # ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 240 Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ñ… (Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ¾ Ğ² checkpoint)
        self.num_targets = 20  # ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ‹Ğ´Ğ°ĞµÑ‚ 20 Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ²

        # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ²
        self.quality_analyzer = SignalQualityAnalyzer(config)

        logger.info(f"MLManager initialized, device: {self.device}")

    async def initialize(self):
        """Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹"""
        try:
            # Ğ ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµĞ¼ÑÑ Ğ² ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ğµ Ğ²Ğ¾Ñ€ĞºĞµÑ€Ğ¾Ğ² Ñ soft-fail Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ¼
            await worker_coordinator.start()
            self.worker_id = await worker_coordinator.register_worker(
                worker_type="ml_manager",
                metadata={
                    "device": str(self.device),
                    "model_path": str(self.model_path),
                    "num_features": self.num_features,
                    "context_length": self.context_length,
                },
                allow_duplicates=True,  # Soft-fail Ñ€ĞµĞ¶Ğ¸Ğ¼: Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ĞµĞ¼ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ñ‹ Ñ Ğ¿Ñ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼
            )

            if not self.worker_id:
                logger.warning("âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ·Ğ°Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ² WorkerCoordinator, Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼")
                # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€ĞµĞ·ĞµÑ€Ğ²Ğ½Ñ‹Ğ¹ worker_id
                import time

                self.worker_id = f"ml_manager_fallback_{int(time.time())}"

            # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
            await self._load_model()

            # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ scaler
            await self._load_scaler()

            # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ feature engineer
            self.feature_engineer = FeatureEngineer(self.config)

            # Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ñ„Ğ»Ğ°Ğ³ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
            self._initialized = True

            # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ heartbeat Ğ¾ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸
            await worker_coordinator.heartbeat(self.worker_id, status="running")

            logger.info("âœ… ML components initialized successfully")

        except Exception as e:
            logger.error(f"Error initializing ML components: {e}")
            if hasattr(self, "worker_id") and self.worker_id:
                await worker_coordinator.unregister_worker(self.worker_id)
            raise

    async def _load_model(self):
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° PatchTST Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"""
        try:
            if not self.model_path.exists():
                raise FileNotFoundError(f"Model file not found: {self.model_path}")

            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ· ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ°
            model_config = {
                "model": {
                    "input_size": self.num_features,  # 98 Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²
                    "output_size": self.num_targets,  # 20 Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ²
                    "context_window": self.context_length,  # 96 Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº
                    "patch_len": 16,
                    "stride": 8,
                    "d_model": 256,  # Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
                    "n_heads": 4,
                    "e_layers": 3,
                    "d_ff": 512,
                    "dropout": 0.1,
                    "temperature_scaling": True,
                    "temperature": 2.0,
                }
            }
            self.model = create_unified_model(model_config)

            # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ²ĞµÑĞ° Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ CUDA Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº
            try:
                # ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾
                checkpoint = torch.load(self.model_path, map_location=self.device)
            except Exception as cuda_error:
                # Ğ•ÑĞ»Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° CUDA, Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ½Ğ° CPU
                logger.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ½Ğ° {self.device}, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ CPU: {cuda_error}")
                checkpoint = torch.load(self.model_path, map_location=torch.device("cpu"))
                self.device = torch.device("cpu")

            self.model.load_state_dict(checkpoint["model_state_dict"])

            # Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑ‰Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾
            try:
                self.model.to(self.device)
            except Exception as e:
                logger.warning(
                    f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑÑ‚Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° {self.device}, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ CPU: {e}"
                )
                self.device = torch.device("cpu")
                self.model.to(self.device)

            self.model.eval()

            # ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ torch.compile Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° ĞµÑĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾
            if os.environ.get("TORCH_COMPILE_DISABLE", "").lower() not in ("1", "true"):
                try:
                    logger.info("ğŸš€ ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ torch.compile Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸...")

                    # ĞšĞ¾Ğ¼Ğ¿Ğ¸Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°
                    self.model = torch.compile(
                        self.model,
                        mode="max-autotune",  # ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
                        fullgraph=False,  # ĞŸĞ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµĞ¼ graph breaks Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
                        dynamic=False,  # Static shapes Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
                    )

                    logger.info("âœ… torch.compile ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸!")

                    # Warm-up run Ğ´Ğ»Ñ JIT ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸
                    logger.info("ğŸ”¥ ĞŸÑ€Ğ¾Ğ³Ñ€ĞµĞ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ torch.compile...")
                    with torch.no_grad():
                        warmup_input = torch.randn(1, self.context_length, self.num_features).to(
                            self.device
                        )
                        _ = self.model(warmup_input)
                    logger.info("âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑ‚Ğ° Ğ¸ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ° Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ!")

                except Exception as compile_error:
                    logger.warning(f"âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ torch.compile: {compile_error}")
                    logger.info("ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ±ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸")
            else:
                logger.info("â„¹ï¸ torch.compile Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ")

            logger.info(f"Model loaded successfully from {self.model_path}")

        except Exception as e:
            logger.error(f"Error loading model: {e}")
            raise

    async def _load_scaler(self):
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° scaler Ğ´Ğ»Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""
        try:
            if not self.scaler_path.exists():
                raise FileNotFoundError(f"Scaler file not found: {self.scaler_path}")

            with open(self.scaler_path, "rb") as f:
                self.scaler = pickle.load(f)

            logger.info(f"Scaler loaded successfully from {self.scaler_path}")

        except Exception as e:
            logger.error(f"Error loading scaler: {e}")
            raise

    async def predict(
        self, input_data: pd.DataFrame | np.ndarray, symbol: str | None = None
    ) -> dict[str, Any]:
        """
        Ğ”ĞµĞ»Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….

        Args:
            input_data: DataFrame Ñ OHLCV Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ (Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ 96 ÑĞ²ĞµÑ‡ĞµĞ¹) Ğ¸Ğ»Ğ¸ numpy array Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸
            symbol: ĞĞ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ» Ğ´Ğ»Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ĞºĞ¾Ğ³Ğ´Ğ° input_data ÑÑ‚Ğ¾ numpy array)

        Returns:
            Dict Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸
        """
        try:
            # Ğ’ĞĞ›Ğ˜Ğ”ĞĞ¦Ğ˜Ğ¯: ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ°
            if not self._initialized or self.model is None:
                raise ValueError("ML model not initialized. Call initialize() first.")

            # Ğ’ĞĞ›Ğ˜Ğ”ĞĞ¦Ğ˜Ğ¯: ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‚Ğ¸Ğ¿ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
            if not isinstance(input_data, (pd.DataFrame, np.ndarray)):
                raise TypeError(
                    f"input_data must be pd.DataFrame or np.ndarray, got {type(input_data)}"
                )

            # Ğ•ÑĞ»Ğ¸ ÑÑ‚Ğ¾ numpy array - Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ĞºĞ°Ğº ĞµÑÑ‚ÑŒ (ÑƒĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸)
            if isinstance(input_data, np.ndarray):
                # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ 3D Ğ¼Ğ°ÑÑĞ¸Ğ²Ñ‹ (batch_size, sequence_length, features)
                if input_data.ndim == 3:
                    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ±Ğ°Ñ‚Ñ‡
                    if input_data.shape[0] == 1:
                        input_data = input_data.squeeze(0)  # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ»Ğ¸ÑˆĞ½ĞµĞµ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ
                    else:
                        raise ValueError(
                            f"Expected single batch, got batch_size={input_data.shape[0]}"
                        )

                # Ğ¢ĞµĞ¿ĞµÑ€ÑŒ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ 2D Ñ„Ğ¾Ñ€Ğ¼Ñƒ
                if (
                    input_data.shape[0] != self.context_length
                    or input_data.shape[1] != self.num_features
                ):
                    raise ValueError(
                        f"Expected shape ({self.context_length}, {self.num_features}), got {input_data.shape}"
                    )
                # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
                features = input_data
                # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼ numpy array Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ scaler
                features_scaled = self.scaler.transform(input_data)
                logger.info("âœ… Numpy array Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ scaler")

            # Ğ•ÑĞ»Ğ¸ ÑÑ‚Ğ¾ DataFrame - Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ ĞºĞ°Ğº OHLCV Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
            else:
                # Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ: ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° DataFrame Ñ async/await Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹
                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
                if len(input_data) < self.context_length:
                    raise ValueError(
                        f"Need at least {self.context_length} candles, got {len(input_data)}"
                    )

                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸ symbol
                if "symbol" not in input_data.columns:
                    logger.warning(
                        "âš ï¸ ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ° 'symbol' Ğ² Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…! Ğ­Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑĞ¼."
                    )
                    input_data = input_data.copy()
                    input_data["symbol"] = "UNKNOWN_SYMBOL"  # ĞŸĞ¾Ğ¼ĞµÑ‡Ğ°ĞµĞ¼ ĞºĞ°Ğº Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»

                # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ - Ñ‚ĞµĞ¿ĞµÑ€ÑŒ ÑÑ‚Ğ¾ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ²
                features_result = self.feature_engineer.create_features(input_data)

                # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ - Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ DataFrame Ğ¸Ğ»Ğ¸ ndarray
                if isinstance(features_result, pd.DataFrame):
                    # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ· DataFrame
                    numeric_cols = features_result.select_dtypes(include=[np.number]).columns
                    # Ğ˜ÑĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
                    feature_cols = [
                        col
                        for col in numeric_cols
                        if not col.startswith(("future_", "direction_", "profit_"))
                        and col not in ["id", "timestamp", "datetime", "symbol"]
                    ]
                    features_array = features_result[feature_cols].values

                    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²
                    if features_array.shape[1] != self.num_features:
                        logger.warning(
                            f"Feature count mismatch: expected {self.num_features}, got {features_array.shape[1]}"
                        )
                        # Ğ•ÑĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞµ - Ğ±ĞµÑ€ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ num_features
                        if features_array.shape[1] > self.num_features:
                            features_array = features_array[:, : self.num_features]
                        else:
                            # Ğ•ÑĞ»Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµ - Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ğ½ÑƒĞ»ÑĞ¼Ğ¸
                            padding = np.zeros(
                                (
                                    features_array.shape[0],
                                    self.num_features - features_array.shape[1],
                                )
                            )
                            features_array = np.hstack([features_array, padding])

                elif isinstance(features_result, np.ndarray):
                    features_array = features_result
                else:
                    raise ValueError(
                        f"Expected DataFrame or np.ndarray from create_features, got {type(features_result)}"
                    )

                # Ğ‘ĞµÑ€ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ context_length ÑÑ‚Ñ€Ğ¾Ğº
                if len(features_array) >= self.context_length:
                    features = features_array[-self.context_length :]

                    # Ğ”Ğ•Ğ¢ĞĞ›Ğ¬ĞĞĞ• Ğ›ĞĞ“Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ• Ğ’Ğ¥ĞĞ”ĞĞ«Ğ¥ ĞŸĞ Ğ˜Ğ—ĞĞĞšĞĞ’
                    if hasattr(self, "feature_engineer") and hasattr(
                        self.feature_engineer, "feature_names"
                    ):
                        feature_names = self.feature_engineer.feature_names
                    else:
                        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¸Ğ· ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸
                        from ml.config.features_240 import get_required_features_list

                        feature_names = get_required_features_list()

                    # Ğ‘ĞµÑ€ĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ÑÑ ÑÑ‚Ñ€Ğ¾ĞºÑƒ Ğ´Ğ»Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹
                    current_features = features[-1] if len(features) > 0 else features[0]

                    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ĞºÑ€Ğ°ÑĞ¸Ğ²ÑƒÑ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸
                    features_table = []
                    features_table.append(
                        "\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
                    )
                    features_table.append(
                        f"â•‘            Ğ’Ğ¥ĞĞ”ĞĞ«Ğ• ĞŸĞĞ ĞĞœĞ•Ğ¢Ğ Ğ« ĞœĞĞ”Ğ•Ğ›Ğ˜ - {len(current_features)} ĞŸĞ Ğ˜Ğ—ĞĞĞšĞĞ’             â•‘"
                    )
                    features_table.append(
                        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£"
                    )

                    # ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ°
                    key_indicators = [
                        ("returns", 0),
                        ("rsi", 9),
                        ("macd", 12),
                        ("bb_position", 19),
                        ("atr_pct", 24),
                        ("stoch_k", 25),
                        ("adx", 27),
                        ("volume_ratio", 4),
                        ("obv_trend", 71),
                        ("momentum_1h", 115),
                        ("trend_1h", 124),
                        ("signal_strength", 139),
                    ]

                    features_table.append(
                        "â•‘ ğŸ¯ ĞšĞ›Ğ®Ğ§Ğ•Ğ’Ğ«Ğ• Ğ˜ĞĞ”Ğ˜ĞšĞĞ¢ĞĞ Ğ«:                                             â•‘"
                    )
                    for i in range(0, len(key_indicators), 2):
                        if i < len(key_indicators):
                            name1, idx1 = key_indicators[i]
                            val1 = current_features[idx1] if idx1 < len(current_features) else 0

                            if i + 1 < len(key_indicators):
                                name2, idx2 = key_indicators[i + 1]
                                val2 = current_features[idx2] if idx2 < len(current_features) else 0
                                features_table.append(
                                    f"â•‘   â€¢ {name1:15s}: {val1:>8.4f}  â”‚  {name2:15s}: {val2:>8.4f}  â•‘"
                                )
                            else:
                                features_table.append(
                                    f"â•‘   â€¢ {name1:15s}: {val1:>8.4f}  â”‚                                  â•‘"
                                )

                    # Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼
                    nan_count = np.sum(np.isnan(current_features))
                    zero_count = np.sum(current_features == 0)
                    mean_val = np.nanmean(current_features)
                    std_val = np.nanstd(current_features)

                    features_table.append(
                        "â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢"
                    )
                    features_table.append(
                        "â•‘ ğŸ“Š Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ ĞŸĞ Ğ˜Ğ—ĞĞĞšĞĞ’:                                            â•‘"
                    )
                    features_table.append(
                        f"â•‘   â€¢ Ğ’ÑĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²: {len(current_features):<6} â€¢ NaN: {nan_count:<6} â€¢ Zeros: {zero_count:<6}         â•‘"
                    )
                    features_table.append(
                        f"â•‘   â€¢ Mean: {mean_val:>8.4f}  â€¢ Std: {std_val:>8.4f}                           â•‘"
                    )
                    features_table.append(
                        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
                    )

                    # Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ¼
                    logger.info("\n".join(features_table))

                else:
                    # Ğ•ÑĞ»Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‡ĞµĞ¼ Ğ½ÑƒĞ¶Ğ½Ğ¾ - Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ğ½ÑƒĞ»ÑĞ¼Ğ¸ (padding)
                    padding_size = self.context_length - len(features_array)
                    padding = np.zeros((padding_size, features_array.shape[1]))
                    features = np.vstack([padding, features_array])

                # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ scaler
                features_scaled = self.scaler.transform(features)
                logger.info("âœ… Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ scaler")
                
                # Ğ¤Ğ˜Ğ›Ğ¬Ğ¢Ğ ĞĞ¦Ğ˜Ğ¯ ZERO VARIANCE FEATURES (Ğ¸Ğ· BOT_AI_V2)
                # ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸ĞµĞ¹
                feature_stds = features_scaled.std(axis=0)
                zero_variance_mask = feature_stds < 1e-6
                zero_variance_count = zero_variance_mask.sum()
                
                if zero_variance_count > 0:
                    logger.warning(f"ğŸš¨ ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {zero_variance_count} Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸ĞµĞ¹")
                    
                    # Ğ—Ğ°Ğ¼ĞµĞ½ÑĞµĞ¼ zero variance Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ½Ğ° Ğ¼Ğ°Ğ»Ğ¾Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ
                    # (Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸Ğ· BOT_AI_V2: ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ÑˆÑƒĞ¼)
                    for i, is_zero_var in enumerate(zero_variance_mask):
                        if is_zero_var:
                            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ³Ğ°ÑƒÑÑĞ¾Ğ² ÑˆÑƒĞ¼ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹
                            noise = np.random.normal(0, 1e-4, features_scaled.shape[0])
                            features_scaled[:, i] = features_scaled[:, i] + noise
                    
                    logger.info(f"âœ… Zero variance Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ·Ğ°Ğ¼ĞµĞ½ĞµĞ½Ñ‹ Ğ½Ğ° ÑˆÑƒĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ML ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°")
                else:
                    logger.info("âœ… Zero variance Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ½Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ñ‹")

            # ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµĞ¼ Ğ² Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€
            x = torch.FloatTensor(features_scaled).unsqueeze(0).to(self.device)

            # Ğ ĞĞ¡Ğ¨Ğ˜Ğ Ğ•ĞĞĞĞ¯ Ğ”Ğ˜ĞĞ“ĞĞĞ¡Ğ¢Ğ˜ĞšĞ Ğ’Ğ¥ĞĞ”ĞĞ«Ğ¥ Ğ”ĞĞĞĞ«Ğ¥
            logger.warning(
                f"""
ğŸ” ML Ğ’Ğ¥ĞĞ”ĞĞ«Ğ• Ğ”ĞĞĞĞ«Ğ• - Ğ”Ğ•Ğ¢ĞĞ›Ğ¬ĞĞ«Ğ™ ĞĞĞĞ›Ğ˜Ğ—:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š Feature Statistics:
   Shape: {features_scaled.shape}
   Min: {features_scaled.min():.6f}
   Max: {features_scaled.max():.6f}
   Mean: {features_scaled.mean():.6f}
   Std: {features_scaled.std():.6f}

ğŸ“ˆ ĞŸĞµÑ€Ğ²Ñ‹Ğµ 10 Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² (Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ÑÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ñ‚Ğ¾Ñ‡ĞºĞ°):
   {features_scaled[-1, :10]}

ğŸ¯ ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…:
   NaN count: {np.isnan(features_scaled).sum()}
   Inf count: {np.isinf(features_scaled).sum()}
   Zero variance features: {(features_scaled.std(axis=0) < 1e-6).sum()}
   ğŸ” Variance statistics:
     Min std: {features_scaled.std(axis=0).min():.8f}
     Max std: {features_scaled.std(axis=0).max():.8f}
     Mean std: {features_scaled.std(axis=0).mean():.8f}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
            )
            logger.debug(f"Input tensor shape: {x.shape}")

            # ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ GPU Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ inference
            if self.device.type == "cuda":
                gpu_memory_before = torch.cuda.memory_allocated(self.device) / 1024**2  # MB
                gpu_memory_cached = torch.cuda.memory_reserved(self.device) / 1024**2  # MB
                logger.debug(
                    f"GPU Memory before inference: {gpu_memory_before:.1f}MB allocated, {gpu_memory_cached:.1f}MB cached"
                )

            # Ğ”ĞµĞ»Ğ°ĞµĞ¼ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ Ğ·Ğ°Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸
            import time

            start_time = time.time()

            with torch.no_grad():
                outputs = self.model(x)

            inference_time = (time.time() - start_time) * 1000  # Ğ² Ğ¼Ğ¸Ğ»Ğ»Ğ¸ÑĞµĞºÑƒĞ½Ğ´Ğ°Ñ…

            # ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ GPU Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ inference
            if self.device.type == "cuda":
                gpu_memory_after = torch.cuda.memory_allocated(self.device) / 1024**2  # MB
                logger.debug(f"GPU Memory after inference: {gpu_memory_after:.1f}MB allocated")
                logger.info(f"Inference time: {inference_time:.1f}ms on {self.device}")

            # ĞÑ‚Ğ»Ğ°Ğ´ĞºĞ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
            outputs_np = outputs.cpu().numpy()[0]
            logger.debug(
                f"Model outputs: min={outputs_np.min():.3f}, max={outputs_np.max():.3f}, mean={outputs_np.mean():.3f}"
            )
            logger.debug(f"Model outputs sample: {outputs_np[:10]}")

            # Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
            predictions = self._interpret_predictions(outputs)

            # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
            try:
                # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ symbol Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ, Ğ¸Ğ½Ğ°Ñ‡Ğµ Ğ¿Ñ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ğ¸Ğ· DataFrame
                if symbol:
                    # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»
                    pass
                elif isinstance(input_data, pd.DataFrame) and "symbol" in input_data.columns:
                    symbol = input_data["symbol"].iloc[-1] if not input_data.empty else "UNKNOWN"
                else:
                    symbol = "UNKNOWN"

                # ĞÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾ Ğ»Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ
                import asyncio

                if asyncio.iscoroutinefunction(ml_prediction_logger.log_prediction):
                    await ml_prediction_logger.log_prediction(
                        symbol=symbol,
                        features=features[
                            -1
                        ],  # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸, Ğ½Ğµ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ
                        model_outputs=outputs_np,
                        predictions=predictions,
                        market_data=input_data if isinstance(input_data, pd.DataFrame) else None,
                    )
                else:
                    # Ğ•ÑĞ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ
                    asyncio.create_task(
                        ml_prediction_logger.log_prediction(
                            symbol=symbol,
                            features=features[-1],  # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸
                            model_outputs=outputs_np,
                            predictions=predictions,
                            market_data=(
                                input_data if isinstance(input_data, pd.DataFrame) else None
                            ),
                        )
                    )
            except Exception as log_error:
                logger.warning(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ·Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ: {log_error}")

            # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ heartbeat Ğ¿Ğ¾ÑĞ»Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ
            try:
                if hasattr(self, "worker_id") and self.worker_id:
                    await worker_coordinator.heartbeat(
                        self.worker_id, status="running", active_tasks=1
                    )
            except Exception as heartbeat_error:
                logger.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ heartbeat: {heartbeat_error}")

            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ¿ĞµÑ€ĞµĞ´ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚Ğ¾Ğ¼
            try:
                # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸
                signal_data = {
                    "symbol": symbol,
                    "direction": predictions.get("primary_direction", "NEUTRAL"),
                    "strategy": "ml_patchtst",
                    "timestamp": datetime.now(),
                    "signal_strength": predictions.get("primary_confidence", 0.0),
                    "price_level": predictions.get("primary_returns", {}).get("15m", 0.0),
                }

                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°
                is_unique = await signal_deduplicator.check_and_register_signal(signal_data)
                if not is_unique:
                    logger.warning(f"ğŸ”„ Ğ”ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚ ML ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ´Ğ»Ñ {symbol}")
                    predictions["is_duplicate"] = True
                else:
                    predictions["is_duplicate"] = False

            except Exception as dedup_error:
                logger.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°: {dedup_error}")
                predictions["is_duplicate"] = False

            logger.info(f"ML Manager Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ: {predictions}")
            return predictions

        except Exception as e:
            logger.error(f"Error making prediction: {e}")
            # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ heartbeat Ğ¾Ğ± Ğ¾ÑˆĞ¸Ğ±ĞºĞµ
            try:
                if hasattr(self, "worker_id") and self.worker_id:
                    await worker_coordinator.heartbeat(self.worker_id, status="error")
            except Exception as heartbeat_error:
                logger.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ heartbeat Ğ¿Ñ€Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞµ: {heartbeat_error}")
            raise

    def _interpret_predictions(self, outputs: torch.Tensor) -> dict[str, Any]:
        """
        Ğ£Ğ›Ğ£Ğ§Ğ¨Ğ•ĞĞĞĞ¯ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ².

        ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜ Ğ’ĞĞ–ĞĞ: ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¾Ğ² Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ!
        Ğ’ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ»Ğ¾ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾:
        - ĞšĞ»Ğ°ÑÑ 0 = LONG (Ğ¿Ğ¾ĞºÑƒĞ¿ĞºĞ°, Ñ€Ğ¾ÑÑ‚ Ñ†ĞµĞ½Ñ‹)
        - ĞšĞ»Ğ°ÑÑ 1 = SHORT (Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ°, Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ Ñ†ĞµĞ½Ñ‹)
        - ĞšĞ»Ğ°ÑÑ 2 = NEUTRAL/FLAT (Ğ±Ğ¾ĞºĞ¾Ğ²Ğ¾Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ½ĞµÑ‚ Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ğ»Ğ¸)

        Args:
            outputs: Ğ¢ĞµĞ½Ğ·Ğ¾Ñ€ Ñ 20 Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸

        Returns:
            Dict Ñ ĞŸĞ ĞĞ’Ğ˜Ğ›Ğ¬ĞĞ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°
        """
        # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ñ‹ Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğ¸ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸
        timeframes = ["15m", "1h", "4h", "12h"]
        direction_names = {0: "LONGâ†—ï¸", 1: "SHORTâ†˜ï¸", 2: "FLATâ¡ï¸"}
        direction_map = {0: "LONG", 1: "SHORT", 2: "NEUTRAL"}

        # Ğ­Ñ‚Ğ°Ğ¿ 1: Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        outputs_np = outputs.cpu().numpy()[0]

        # Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² (20 Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹):
        # 0-3: future returns (15m, 1h, 4h, 12h)
        # 4-15: direction logits (12 values = 3 classes Ã— 4 timeframes)
        # 16-19: risk metrics

        future_returns = outputs_np[0:4]
        direction_logits = outputs_np[4:16]  # 12 Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹!
        risk_metrics = outputs_np[16:20]

        # ĞŸĞ ĞĞ’Ğ˜Ğ›Ğ¬ĞĞĞ¯ Ğ˜ĞĞ¢Ğ•Ğ ĞŸĞ Ğ•Ğ¢ĞĞ¦Ğ˜Ğ¯ DIRECTIONS (12 Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ = 4 Ñ‚Ğ°Ğ¹Ğ¼Ñ„Ñ€ĞµĞ¹Ğ¼Ğ° Ã— 3 ĞºĞ»Ğ°ÑÑĞ°)
        direction_logits_reshaped = direction_logits.reshape(4, 3)  # 4 Ñ‚Ğ°Ğ¹Ğ¼Ñ„Ñ€ĞµĞ¹Ğ¼Ğ° Ã— 3 ĞºĞ»Ğ°ÑÑĞ°

        # ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ softmax Ğº ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ñ‚Ğ°Ğ¹Ğ¼Ñ„Ñ€ĞµĞ¹Ğ¼Ñƒ
        directions = []
        direction_probs = []

        for i, logits in enumerate(direction_logits_reshaped):
            # Softmax Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹
            exp_logits = np.exp(logits - np.max(logits))  # Ğ”Ğ»Ñ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
            probs = exp_logits / exp_logits.sum()
            direction_probs.append(probs)

            # Argmax Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ° (0=LONG, 1=SHORT, 2=NEUTRAL)
            direction_class = np.argmax(probs)
            directions.append(direction_class)

        directions = np.array(directions)

        # Ğ”Ğ•Ğ¢ĞĞ›Ğ¬ĞĞĞ• Ğ›ĞĞ“Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ• Ğ’Ğ¡Ğ•Ğ¥ ĞŸĞĞ ĞĞœĞ•Ğ¢Ğ ĞĞ’ ĞœĞĞ”Ğ•Ğ›Ğ˜
        # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ ĞºÑ€Ğ°ÑĞ¸Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ
        outputs_formatted = [f"{x:.4f}" for x in outputs_np]

        logger.info(
            f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ¤– ML MODEL PREDICTION ANALYSIS                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ“Š RAW MODEL OUTPUTS (20 parameters):                                â•‘
â•‘  [0-4]:  {', '.join(outputs_formatted[0:5]):50s}    â•‘
â•‘  [5-9]:  {', '.join(outputs_formatted[5:10]):50s}    â•‘
â•‘  [10-14]: {', '.join(outputs_formatted[10:15]):50s}   â•‘
â•‘  [15-19]: {', '.join(outputs_formatted[15:20]):50s}   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ“ˆ FUTURE RETURNS (Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ñ‹Ğµ Ğ´Ğ¾Ñ…Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸):                           â•‘
â•‘   â€¢ 15m:  {future_returns[0]:+.6f} ({future_returns[0]*100:+6.3f}%)                       â•‘
â•‘   â€¢ 1h:   {future_returns[1]:+.6f} ({future_returns[1]*100:+6.3f}%)                       â•‘
â•‘   â€¢ 4h:   {future_returns[2]:+.6f} ({future_returns[2]*100:+6.3f}%)                       â•‘
â•‘   â€¢ 12h:  {future_returns[3]:+.6f} ({future_returns[3]*100:+6.3f}%)                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ¯ ĞĞĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ˜Ğ¯ ĞŸĞ Ğ¢ĞĞ™ĞœĞ¤Ğ Ğ•Ğ™ĞœĞĞœ:                                      â•‘
â•‘   â€¢ 15m:  {direction_names.get(directions[0], str(directions[0])):8s} | Conf: {direction_probs[0].max():.3f} |                      â•‘
â•‘      Probs: [LONG: {direction_probs[0][0]:.3f}, SHORT: {direction_probs[0][1]:.3f}, NEUTRAL: {direction_probs[0][2]:.3f}]          â•‘
â•‘   â€¢ 1h:   {direction_names.get(directions[1], str(directions[1])):8s} | Conf: {direction_probs[1].max():.3f} |                      â•‘
â•‘      Probs: [LONG: {direction_probs[1][0]:.3f}, SHORT: {direction_probs[1][1]:.3f}, NEUTRAL: {direction_probs[1][2]:.3f}]          â•‘
â•‘   â€¢ 4h:   {direction_names.get(directions[2], str(directions[2])):8s} | Conf: {direction_probs[2].max():.3f} |                      â•‘
â•‘      Probs: [LONG: {direction_probs[2][0]:.3f}, SHORT: {direction_probs[2][1]:.3f}, NEUTRAL: {direction_probs[2][2]:.3f}]          â•‘
â•‘   â€¢ 12h:  {direction_names.get(directions[3], str(directions[3])):8s} | Conf: {direction_probs[3].max():.3f} |                      â•‘
â•‘      Probs: [LONG: {direction_probs[3][0]:.3f}, SHORT: {direction_probs[3][1]:.3f}, NEUTRAL: {direction_probs[3][2]:.3f}]          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ âš¡ RISK METRICS (Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ñ€Ğ¸ÑĞºĞ°):                                     â•‘
â•‘   â€¢ Max Drawdown 1h:  {risk_metrics[0]:+.6f}                                  â•‘
â•‘   â€¢ Max Rally 1h:     {risk_metrics[1]:+.6f}                                  â•‘
â•‘   â€¢ Max Drawdown 4h:  {risk_metrics[2]:+.6f}                                  â•‘
â•‘   â€¢ Max Rally 4h:     {risk_metrics[3]:+.6f}                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
        )

        # Ğ”ĞĞŸĞĞ›ĞĞ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞĞ• Ğ›ĞĞ“Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ•: Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ğ²ÑĞµÑ… 20 Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        logger.info(
            f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘               ğŸ“Š Ğ”Ğ•Ğ¢ĞĞ›Ğ¬ĞĞĞ¯ Ğ˜ĞĞ¢Ğ•Ğ ĞŸĞ Ğ•Ğ¢ĞĞ¦Ğ˜Ğ¯ 20 Ğ’Ğ«Ğ¥ĞĞ”ĞĞ’ ĞœĞĞ”Ğ•Ğ›Ğ˜           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ”® ĞŸĞ Ğ•Ğ”Ğ¡ĞšĞĞ—ĞĞĞ˜Ğ¯ Ğ”ĞĞ¥ĞĞ”ĞĞĞ¡Ğ¢Ğ˜ (outputs 0-3):                           â•‘
â•‘   â€¢ Out[0] = {outputs_np[0]:+.6f} â†’ 15m return prediction            â•‘
â•‘   â€¢ Out[1] = {outputs_np[1]:+.6f} â†’ 1h return prediction             â•‘
â•‘   â€¢ Out[2] = {outputs_np[2]:+.6f} â†’ 4h return prediction             â•‘
â•‘   â€¢ Out[3] = {outputs_np[3]:+.6f} â†’ 12h return prediction            â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ¯ Ğ›ĞĞ“Ğ˜Ğ¢Ğ« ĞĞĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ˜Ğ¯ 15m (outputs 4-6):                            â•‘
â•‘   â€¢ Out[4] = {outputs_np[4]:+.6f} â†’ Logit for LONG                   â•‘
â•‘   â€¢ Out[5] = {outputs_np[5]:+.6f} â†’ Logit for SHORT                  â•‘
â•‘   â€¢ Out[6] = {outputs_np[6]:+.6f} â†’ Logit for NEUTRAL                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ¯ Ğ›ĞĞ“Ğ˜Ğ¢Ğ« ĞĞĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ˜Ğ¯ 1h (outputs 7-9):                             â•‘
â•‘   â€¢ Out[7] = {outputs_np[7]:+.6f} â†’ Logit for LONG                   â•‘
â•‘   â€¢ Out[8] = {outputs_np[8]:+.6f} â†’ Logit for SHORT                  â•‘
â•‘   â€¢ Out[9] = {outputs_np[9]:+.6f} â†’ Logit for NEUTRAL                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ¯ Ğ›ĞĞ“Ğ˜Ğ¢Ğ« ĞĞĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ˜Ğ¯ 4h (outputs 10-12):                           â•‘
â•‘   â€¢ Out[10] = {outputs_np[10]:+.6f} â†’ Logit for LONG                 â•‘
â•‘   â€¢ Out[11] = {outputs_np[11]:+.6f} â†’ Logit for SHORT                â•‘
â•‘   â€¢ Out[12] = {outputs_np[12]:+.6f} â†’ Logit for NEUTRAL              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ¯ Ğ›ĞĞ“Ğ˜Ğ¢Ğ« ĞĞĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ˜Ğ¯ 12h (outputs 13-15):                          â•‘
â•‘   â€¢ Out[13] = {outputs_np[13]:+.6f} â†’ Logit for LONG                 â•‘
â•‘   â€¢ Out[14] = {outputs_np[14]:+.6f} â†’ Logit for SHORT                â•‘
â•‘   â€¢ Out[15] = {outputs_np[15]:+.6f} â†’ Logit for NEUTRAL              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ âš ï¸ ĞœĞ•Ğ¢Ğ Ğ˜ĞšĞ˜ Ğ Ğ˜Ğ¡ĞšĞ (outputs 16-19):                                   â•‘
â•‘   â€¢ Out[16] = {outputs_np[16]:+.6f} â†’ Max Drawdown 1h                â•‘
â•‘   â€¢ Out[17] = {outputs_np[17]:+.6f} â†’ Max Rally 1h                   â•‘
â•‘   â€¢ Out[18] = {outputs_np[18]:+.6f} â†’ Max Drawdown 4h                â•‘
â•‘   â€¢ Out[19] = {outputs_np[19]:+.6f} â†’ Max Rally 4h                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
        )

        # Ğ­Ñ‚Ğ°Ğ¿ 2: Ğ Ğ°ÑÑ‡ĞµÑ‚ weighted_direction Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
        weights = np.array([0.4, 0.3, 0.2, 0.1])
        weighted_direction = np.sum(directions * weights)

        # Ğ­Ñ‚Ğ°Ğ¿ 3: ĞĞ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°
        filter_result = self.quality_analyzer.analyze_signal_quality(
            directions=directions,
            direction_probs=direction_probs,
            future_returns=future_returns,
            risk_metrics=risk_metrics,
            weighted_direction=weighted_direction,
        )

        # Ğ­Ñ‚Ğ°Ğ¿ 4: ĞŸÑ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°
        if not filter_result.passed:
            # Ğ¡Ğ¸Ğ³Ğ½Ğ°Ğ» Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑˆĞµĞ» Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°
            signal_type = "NEUTRAL"
            signal_strength = 0.25  # ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ
            combined_confidence = 0.25
            stop_loss_pct = None
            take_profit_pct = None

            logger.warning(
                f"ğŸš« Ğ¡Ğ¸Ğ³Ğ½Ğ°Ğ» Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. "
                f"ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹: {'; '.join(filter_result.rejection_reasons)}"
            )
        else:
            # Ğ¡Ğ¸Ğ³Ğ½Ğ°Ğ» Ğ¿Ñ€Ğ¾ÑˆĞµĞ» Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹ - Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
            signal_type = filter_result.signal_type
            metrics = filter_result.quality_metrics

            # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
            signal_strength = metrics.agreement_score
            combined_confidence = metrics.confidence_score

            # Ğ Ğ°ÑÑ‡ĞµÑ‚ SL/TP Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°
            if signal_type in ["LONG", "SHORT"]:
                # ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ SL/TP Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°
                base_sl = 0.01  # 1%
                base_tp = 0.02  # 2%

                # ĞšĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°
                quality_multiplier = 0.8 + (metrics.quality_score * 0.4)  # 0.8-1.2

                stop_loss_pct = base_sl * quality_multiplier
                take_profit_pct = base_tp * quality_multiplier

                # ĞšĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ½Ğ° Ğ²Ğ¾Ğ»Ğ°Ñ‚Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ
                volatility = np.std(future_returns[:2])  # Ğ‘Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ğµ Ğ¢Ğ¤
                if volatility > 0.01:
                    stop_loss_pct *= 1.2
                    take_profit_pct *= 1.2

                # Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ
                stop_loss_pct = np.clip(stop_loss_pct, 0.005, 0.025)  # 0.5% - 2.5%
                take_profit_pct = np.clip(take_profit_pct, 0.01, 0.05)  # 1% - 5%
            else:
                stop_loss_pct = None
                take_profit_pct = None

        # Ğ­Ñ‚Ğ°Ğ¿ 5: ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°
        # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
        confidence_scores = np.array([np.max(probs) for probs in direction_probs])
        model_confidence = float(np.mean(confidence_scores))
        avg_risk = float(np.mean(risk_metrics))
        risk_level = "LOW" if avg_risk < 0.3 else "MEDIUM" if avg_risk < 0.7 else "HIGH"

        # Focal weighting Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ Ğ»Ğ¾Ğ³Ğ³ĞµÑ€Ğ¾Ğ¼
        focal_alpha = 0.25
        focal_gamma = 2.0
        focal_weighted_confidence = focal_alpha * (1 - model_confidence) ** focal_gamma

        # Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°
        quality_score = filter_result.quality_metrics.quality_score if filter_result.passed else 0.0
        strategy_used = filter_result.strategy_used.value

        sl_str = f"{stop_loss_pct:.3f}" if stop_loss_pct else "Ğ½Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½"
        tp_str = f"{take_profit_pct:.3f}" if take_profit_pct else "Ğ½Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½"

        # Ğ¦Ğ²ĞµÑ‚Ğ¾Ğ²Ğ°Ñ Ğ¸Ğ½Ğ´Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°
        signal_emoji = "ğŸŸ¢" if signal_type == "LONG" else "ğŸ”´" if signal_type == "SHORT" else "âšª"
        passed_emoji = "âœ…" if filter_result.passed else "âŒ"

        logger.info(
            f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ“Š ML PREDICTION FINAL RESULT                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ {signal_emoji} SIGNAL TYPE: {signal_type:8s} | Strategy: {strategy_used:12s}        â•‘
â•‘ {passed_emoji} Quality Filter: {'PASSED' if filter_result.passed else 'REJECTED':8s} | Score: {quality_score:.3f}              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ“ˆ PREDICTIONS BY TIMEFRAME:                                         â•‘
â•‘   15m: {direction_map.get(int(directions[0]), 'N/A'):8s} | Ret: {future_returns[0]:+.4f} | Conf: {confidence_scores[0]:.3f}       â•‘
â•‘   1h:  {direction_map.get(int(directions[1]), 'N/A'):8s} | Ret: {future_returns[1]:+.4f} | Conf: {confidence_scores[1]:.3f}       â•‘
â•‘   4h:  {direction_map.get(int(directions[2]), 'N/A'):8s} | Ret: {future_returns[2]:+.4f} | Conf: {confidence_scores[2]:.3f}       â•‘
â•‘   12h: {direction_map.get(int(directions[3]), 'N/A'):8s} | Ret: {future_returns[3]:+.4f} | Conf: {confidence_scores[3]:.3f}       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ’ª SIGNAL STRENGTH: {signal_strength:.3f} | CONFIDENCE: {combined_confidence:.1%}           â•‘
â•‘ âš ï¸  RISK LEVEL: {risk_level:6s} | Score: {avg_risk:.3f}                         â•‘
â•‘ ğŸ›¡ï¸  STOP LOSS:  {sl_str:8s} | ğŸ¯ TAKE PROFIT: {tp_str:8s}          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
        )

        # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ

        return {
            # ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°
            "signal_type": signal_type,
            "signal_strength": float(signal_strength),
            "confidence": float(combined_confidence),
            "signal_confidence": float(combined_confidence),  # Ğ”Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
            "success_probability": float(combined_confidence),
            # SL/TP Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ñ‹
            "stop_loss_pct": stop_loss_pct,
            "take_profit_pct": take_profit_pct,
            # ĞÑ†ĞµĞ½ĞºĞ° Ñ€Ğ¸ÑĞºĞ°
            "risk_level": risk_level,
            "risk_score": float(avg_risk),
            "max_drawdown": float(risk_metrics[0]) if len(risk_metrics) > 0 else 0,
            "max_rally": float(risk_metrics[1]) if len(risk_metrics) > 1 else 0,
            # Ğ”ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ
            "returns_15m": float(future_returns[0]),
            "returns_1h": float(future_returns[1]),
            "returns_4h": float(future_returns[2]),
            "returns_12h": float(future_returns[3]),
            # ĞĞ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ Ñ‚Ğ°Ğ¹Ğ¼Ñ„Ñ€ĞµĞ¹Ğ¼Ğ°Ğ¼
            "direction_15m": direction_map.get(int(directions[0]), "NEUTRAL"),
            "direction_1h": direction_map.get(int(directions[1]), "NEUTRAL"),
            "direction_4h": direction_map.get(int(directions[2]), "NEUTRAL"),
            "direction_12h": direction_map.get(int(directions[3]), "NEUTRAL"),
            "confidence_15m": float(confidence_scores[0]),
            "confidence_1h": float(confidence_scores[1]),
            "confidence_4h": float(confidence_scores[2]),
            "confidence_12h": float(confidence_scores[3]),
            # ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°
            "quality_score": quality_score,
            "agreement_score": (
                filter_result.quality_metrics.agreement_score if filter_result.passed else 0.0
            ),
            "filter_strategy": strategy_used,
            "passed_quality_filters": filter_result.passed,
            "rejection_reasons": filter_result.rejection_reasons,
            # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
            "primary_timeframe": "4h",  # ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ‚Ğ°Ğ¹Ğ¼Ñ„Ñ€ĞµĞ¹Ğ¼
            "predictions": {
                "returns_15m": float(future_returns[0]),
                "returns_1h": float(future_returns[1]),
                "returns_4h": float(future_returns[2]),
                "returns_12h": float(future_returns[3]),
                "direction_score": float(weighted_direction),
                "directions_by_timeframe": directions.tolist(),
                "direction_probabilities": [p.tolist() for p in direction_probs],
            },
            "timestamp": datetime.now(UTC).isoformat(),
        }

    async def update_model(self, new_model_path: str):
        """
        ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ.

        Args:
            new_model_path: ĞŸÑƒÑ‚ÑŒ Ğº Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        """
        try:
            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ÑÑ‚Ğ°Ñ€ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ°Ğº Ñ€ĞµĞ·ĞµÑ€Ğ²Ğ½ÑƒÑ
            backup_path = self.model_path.with_suffix(".pth.backup")
            if self.model_path.exists():
                self.model_path.rename(backup_path)

            # ĞšĞ¾Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
            Path(new_model_path).rename(self.model_path)

            # ĞŸĞµÑ€ĞµĞ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
            await self._load_model()

            logger.info(f"Model updated successfully from {new_model_path}")

        except Exception as e:
            logger.error(f"Error updating model: {e}")
            # Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ ÑÑ‚Ğ°Ñ€ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
            if backup_path.exists():
                backup_path.rename(self.model_path)
            raise

    def get_model_info(self) -> dict[str, Any]:
        """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"""
        return {
            "model_type": "UnifiedPatchTST",
            "model_path": str(self.model_path),
            "context_length": self.context_length,
            "num_features": self.num_features,
            "num_targets": self.num_targets,
            "device": str(self.device),
            "model_loaded": self.model is not None,
            "scaler_loaded": self.scaler is not None,
        }

    def switch_filtering_strategy(self, strategy: str) -> bool:
        """
        ĞŸĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ²

        Args:
            strategy: ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ (conservative/moderate/aggressive)

        Returns:
            True ĞµÑĞ»Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¾
        """
        if self.quality_analyzer.switch_strategy(strategy):
            logger.info(f"âœ… Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ° Ğ½Ğ°: {strategy}")
            return True
        else:
            logger.error(f"âŒ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ½Ğ°: {strategy}")
            return False

    def get_filtering_statistics(self) -> dict[str, Any]:
        """
        ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸

        Returns:
            Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¾Ğ¹
        """
        return self.quality_analyzer.get_strategy_statistics()

    def get_available_strategies(self) -> list[str]:
        """
        ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¸ÑĞºĞ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸

        Returns:
            Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹
        """
        return ["conservative", "moderate", "aggressive"]

    def get_current_strategy_config(self) -> dict[str, Any]:
        """
        ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸

        Returns:
            ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸
        """
        return {
            "active_strategy": self.quality_analyzer.active_strategy.value,
            "strategy_params": self.quality_analyzer.strategy_params,
            "timeframe_weights": self.quality_analyzer.timeframe_weights.tolist(),
            "quality_weights": self.quality_analyzer.quality_weights,
        }
