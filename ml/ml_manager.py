#!/usr/bin/env python3
"""
ML Manager –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è PatchTST –º–æ–¥–µ–ª—å—é –≤ BOT Trading v3
"""

import os
import pickle
from datetime import UTC, datetime
from pathlib import Path
from typing import Any

import numpy as np
import pandas as pd
import torch

from core.logger import setup_logger
from core.system.signal_deduplicator import signal_deduplicator
from core.system.worker_coordinator import worker_coordinator
from ml.logic.feature_engineering import (  # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å 240+ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    FeatureEngineer,
)
from ml.logic.patchtst_model import create_unified_model
from ml.ml_prediction_logger import ml_prediction_logger

logger = setup_logger("ml_manager")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ GPU –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è - –¢–û–ß–ù–ê–Ø –ö–û–ü–ò–Ø –∏–∑ —Ä–∞–±–æ—á–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞
if torch.cuda.is_available():
    try:
        # Benchmark mode –¥–ª—è cudnn
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.enabled = True

        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ float32 matmul precision –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ –Ω–æ–≤—ã—Ö GPU
        # torch.set_float32_matmul_precision('high')  # –í—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–æ –∏–∑-–∑–∞ warning

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è Ampere+ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (RTX 5090)
        # torch.backends.cuda.matmul.allow_tf32 = True  # Deprecated
        # torch.backends.cudnn.allow_tf32 = True  # Deprecated

        logger.info("‚úÖ –ì–ª–æ–±–∞–ª—å–Ω—ã–µ GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–∫–ª—é—á–µ–Ω—ã")
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤–∫–ª—é—á–∏—Ç—å GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {e}")


class MLManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è ML –º–æ–¥–µ–ª—è–º–∏ –≤ —Ç–æ—Ä–≥–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–µ.
    –†–∞–±–æ—Ç–∞–µ—Ç —Å PatchTST –º–æ–¥–µ–ª—å—é –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–π —Ä—ã–Ω–∫–∞.
    """

    def __init__(self, config: dict[str, Any]):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ML –º–µ–Ω–µ–¥–∂–µ—Ä–∞.

        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã
        """
        self.config = config
        self.model = None
        self.scaler = None
        self.feature_engineer = None
        # –ü–æ–ª—É—á–∞–µ–º device –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        device_config = config.get("ml", {}).get("model", {}).get("device", "auto")

        # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GPU —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        if device_config == "auto":
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ CUDA
                if torch.cuda.is_available():
                    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU
                    gpu_count = torch.cuda.device_count()
                    if gpu_count > 0:
                        # –í—ã–±–∏—Ä–∞–µ–º GPU —Å –Ω–∞–∏–º–µ–Ω—å—à–µ–π –∑–∞–≥—Ä—É–∑–∫–æ–π –ø–∞–º—è—Ç–∏
                        best_gpu = 0
                        min_memory_used = float("inf")

                        for i in range(gpu_count):
                            try:
                                torch.cuda.set_device(i)
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å GPU
                                props = torch.cuda.get_device_properties(i)
                                logger.info(
                                    f"GPU {i}: {props.name}, "
                                    f"Compute Capability: {props.major}.{props.minor}, "
                                    f"Memory: {props.total_memory / 1024**3:.1f}GB"
                                )

                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
                                if torch.cuda.memory_allocated(i) < min_memory_used:
                                    min_memory_used = torch.cuda.memory_allocated(i)
                                    best_gpu = i
                            except Exception as gpu_error:
                                logger.warning(f"GPU {i} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {gpu_error}")
                                continue

                        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ª—É—á—à–∏–π GPU
                        torch.cuda.set_device(best_gpu)
                        self.device = torch.device(f"cuda:{best_gpu}")

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å GPU —Ç–µ—Å—Ç–æ–≤—ã–º —Ç–µ–Ω–∑–æ—Ä–æ–º
                        test_tensor = torch.zeros(1, 1).to(self.device)
                        _ = test_tensor * 2  # –ü—Ä–æ—Å—Ç–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏

                        # RTX 5090 (Blackwell) –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
                        # - GPU –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–µ–Ω –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ —Å–∏—Å—Ç–µ–º—ã
                        # - torch.compile –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É sm_120
                        # - –≠—Ç–æ –Ω–µ –≤–ª–∏—è–µ—Ç –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–ª–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
                        gpu_name = props.name.upper()
                        if "RTX 5090" in gpu_name or props.major >= 12:
                            logger.info(
                                f"üéØ –û–±–Ω–∞—Ä—É–∂–µ–Ω RTX 5090 ({gpu_name}, sm_{props.major}{props.minor})"
                            )
                            logger.warning(
                                "‚ö†Ô∏è torch.compile –æ—Ç–∫–ª—é—á–µ–Ω –¥–ª—è RTX 5090 (sm_120) - –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–µ–π PyTorch. –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –∏ –Ω–µ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å."
                            )
                            # –û—Ç–∫–ª—é—á–∞–µ–º –∫–æ–º–ø–∏–ª—è—Ü–∏—é –¥–ª—è –Ω–æ–≤—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
                            os.environ["TORCH_COMPILE_DISABLE"] = "1"

                        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω GPU {best_gpu} ({props.name})")
                        logger.info(
                            f"üíæ GPU –ø–∞–º—è—Ç—å –¥–æ—Å—Ç—É–ø–Ω–∞: {props.total_memory / 1024**3:.2f} GB"
                        )
                    else:
                        logger.warning("CUDA –¥–æ—Å—Ç—É–ø–Ω–∞, –Ω–æ GPU –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                        self.device = torch.device("cpu")
                else:
                    logger.info("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")
                    self.device = torch.device("cpu")

            except Exception as e:
                # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–∫–∏
                logger.warning(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ GPU: {type(e).__name__}: {e}")
                logger.info("–ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ CPU")
                self.device = torch.device("cpu")

                # –û—á–∏—â–∞–µ–º CUDA –∫–µ—à –ø—Ä–∏ –æ—à–∏–±–∫–µ
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        else:
            # –†—É—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ device
            try:
                self.device = torch.device(device_config)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å
                if "cuda" in device_config:
                    test_tensor = torch.zeros(1, 1).to(self.device)
                    _ = test_tensor * 2
                logger.info(f"–£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω device: {device_config}")
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å device {device_config}: {e}")
                self.device = torch.device("cpu")

        # –§–ª–∞–≥ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        self._initialized = False

        # –ö—ç—à –º–æ–¥–µ–ª–µ–π (–¥–ª—è MLManager —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Ç–µ—Å—Ç–∞–º–∏)
        self._model_cache = {}
        self._cache_size = config.get("ml", {}).get("model_cache_size", 3)
        self._memory_limit = config.get("ml", {}).get("memory_limit_mb", 1024)

        # –ü—É—Ç–∏ –∫ –º–æ–¥–µ–ª—è–º - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –ø—É—Ç–∏
        base_dir = Path(__file__).parent.parent  # –ö–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞
        model_dir = base_dir / config.get("ml", {}).get("model_directory", "models/saved")
        self.model_path = model_dir / "best_model_20250728_215703.pth"
        self.scaler_path = model_dir / "data_scaler.pkl"

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        self.context_length = 96  # 24 —á–∞—Å–∞ –ø—Ä–∏ 15-–º–∏–Ω—É—Ç–Ω—ã—Ö —Å–≤–µ—á–∞—Ö
        self.num_features = 240  # –í–µ—Ä–Ω—É–ª–∏ –æ–±—Ä–∞—Ç–Ω–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –º–æ–¥–µ–ª—å—é
        self.num_targets = 20  # –ú–æ–¥–µ–ª—å –≤—ã–¥–∞–µ—Ç 20 –≤—ã—Ö–æ–¥–æ–≤

        logger.info(f"MLManager initialized, device: {self.device}")

    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π"""
        try:
            # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º—Å—è –≤ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–µ –≤–æ—Ä–∫–µ—Ä–æ–≤
            await worker_coordinator.start()
            self.worker_id = await worker_coordinator.register_worker(
                worker_type="ml_manager",
                metadata={
                    "device": str(self.device),
                    "model_path": str(self.model_path),
                    "num_features": self.num_features,
                    "context_length": self.context_length,
                },
            )

            if not self.worker_id:
                logger.error("‚ùå –î—Ä—É–≥–æ–π ML Manager —É–∂–µ –∞–∫—Ç–∏–≤–µ–Ω. –ó–∞–≤–µ—Ä—à–∞–µ–º —Ä–∞–±–æ—Ç—É.")
                raise RuntimeError("Duplicate ML Manager detected")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            await self._load_model()

            # –ó–∞–≥—Ä—É–∂–∞–µ–º scaler
            await self._load_scaler()

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º feature engineer
            self.feature_engineer = FeatureEngineer(self.config)

            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
            self._initialized = True

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º heartbeat –æ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
            await worker_coordinator.heartbeat(self.worker_id, status="running")

            logger.info("‚úÖ ML components initialized successfully")

        except Exception as e:
            logger.error(f"Error initializing ML components: {e}")
            if hasattr(self, "worker_id") and self.worker_id:
                await worker_coordinator.unregister_worker(self.worker_id)
            raise

    async def _load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ PatchTST –º–æ–¥–µ–ª–∏"""
        try:
            if not self.model_path.exists():
                raise FileNotFoundError(f"Model file not found: {self.model_path}")

            # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –º–æ–¥–µ–ª–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            model_config = {
                "model": {
                    "input_size": self.num_features,  # 98 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    "output_size": self.num_targets,  # 20 –≤—ã—Ö–æ–¥–æ–≤
                    "context_window": self.context_length,  # 96 –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–æ—á–µ–∫
                    "patch_len": 16,
                    "stride": 8,
                    "d_model": 256,  # –°–æ–≥–ª–∞—Å–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
                    "n_heads": 4,
                    "e_layers": 3,
                    "d_ff": 512,
                    "dropout": 0.1,
                    "temperature_scaling": True,
                    "temperature": 2.0,
                }
            }
            self.model = create_unified_model(model_config)

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π CUDA –æ—à–∏–±–æ–∫
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                checkpoint = torch.load(self.model_path, map_location=self.device)
            except Exception as cuda_error:
                # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ CUDA, –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞ CPU
                logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ {self.device}, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU: {cuda_error}")
                checkpoint = torch.load(self.model_path, map_location=torch.device("cpu"))
                self.device = torch.device("cpu")

            self.model.load_state_dict(checkpoint["model_state_dict"])

            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            try:
                self.model.to(self.device)
            except Exception as e:
                logger.warning(
                    f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ {self.device}, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU: {e}"
                )
                self.device = torch.device("cpu")
                self.model.to(self.device)

            self.model.eval()

            logger.info(f"Model loaded successfully from {self.model_path}")

        except Exception as e:
            logger.error(f"Error loading model: {e}")
            raise

    async def _load_scaler(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ scaler –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            if not self.scaler_path.exists():
                raise FileNotFoundError(f"Scaler file not found: {self.scaler_path}")

            with open(self.scaler_path, "rb") as f:
                self.scaler = pickle.load(f)

            logger.info(f"Scaler loaded successfully from {self.scaler_path}")

        except Exception as e:
            logger.error(f"Error loading scaler: {e}")
            raise

    async def predict(
        self, input_data: pd.DataFrame | np.ndarray, symbol: str | None = None
    ) -> dict[str, Any]:
        """
        –î–µ–ª–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö.

        Args:
            input_data: DataFrame —Å OHLCV –¥–∞–Ω–Ω—ã–º–∏ (–º–∏–Ω–∏–º—É–º 96 —Å–≤–µ—á–µ–π) –∏–ª–∏ numpy array —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            symbol: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–∏–º–≤–æ–ª –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–≥–¥–∞ input_data —ç—Ç–æ numpy array)

        Returns:
            Dict —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
        """
        try:
            # –í–ê–õ–ò–î–ê–¶–ò–Ø: –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞
            if not self._initialized or self.model is None:
                raise ValueError("ML model not initialized. Call initialize() first.")

            # –í–ê–õ–ò–î–ê–¶–ò–Ø: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if not isinstance(input_data, (pd.DataFrame, np.ndarray)):
                raise TypeError(
                    f"input_data must be pd.DataFrame or np.ndarray, got {type(input_data)}"
                )

            # –ï—Å–ª–∏ —ç—Ç–æ numpy array - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å (—É–∂–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)
            if isinstance(input_data, np.ndarray):
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º 3D –º–∞—Å—Å–∏–≤—ã (batch_size, sequence_length, features)
                if input_data.ndim == 3:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –æ–¥–∏–Ω–æ—á–Ω—ã–π –±–∞—Ç—á
                    if input_data.shape[0] == 1:
                        input_data = input_data.squeeze(0)  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–µ–µ –∏–∑–º–µ—Ä–µ–Ω–∏–µ
                    else:
                        raise ValueError(
                            f"Expected single batch, got batch_size={input_data.shape[0]}"
                        )

                # –¢–µ–ø–µ—Ä—å –ø—Ä–æ–≤–µ—Ä—è–µ–º 2D —Ñ–æ—Ä–º—É
                if (
                    input_data.shape[0] != self.context_length
                    or input_data.shape[1] != self.num_features
                ):
                    raise ValueError(
                        f"Expected shape ({self.context_length}, {self.num_features}), got {input_data.shape}"
                    )
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º numpy array —Å –ø–æ–º–æ—â—å—é scaler
                features_scaled = self.scaler.transform(input_data)
                logger.info("‚úÖ Numpy array –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω —Å –ø–æ–º–æ—â—å—é scaler")

            # –ï—Å–ª–∏ —ç—Ç–æ DataFrame - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ OHLCV –¥–∞–Ω–Ω—ã–µ
            else:
                # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ DataFrame —Å async/await –ª–æ–≥–∏–∫–æ–π
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
                if len(input_data) < self.context_length:
                    raise ValueError(
                        f"Need at least {self.context_length} candles, got {len(input_data)}"
                    )

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–∫–∏ symbol
                if "symbol" not in input_data.columns:
                    logger.warning(
                        "‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'symbol' –≤ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö! –≠—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –Ω–µ—Ç–æ—á–Ω—ã–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º."
                    )
                    input_data = input_data.copy()
                    input_data["symbol"] = "UNKNOWN_SYMBOL"  # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–∏–º–≤–æ–ª

                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ - —Ç–µ–ø–µ—Ä—å —ç—Ç–æ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤
                features_result = self.feature_engineer.create_features(input_data)

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç - –º–æ–∂–µ—Ç –±—ã—Ç—å DataFrame –∏–ª–∏ ndarray
                if isinstance(features_result, pd.DataFrame):
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ DataFrame
                    numeric_cols = features_result.select_dtypes(include=[np.number]).columns
                    # –ò—Å–∫–ª—é—á–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    feature_cols = [
                        col
                        for col in numeric_cols
                        if not col.startswith(("future_", "direction_", "profit_"))
                        and col not in ["id", "timestamp", "datetime", "symbol"]
                    ]
                    features_array = features_result[feature_cols].values

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    if features_array.shape[1] != self.num_features:
                        logger.warning(
                            f"Feature count mismatch: expected {self.num_features}, got {features_array.shape[1]}"
                        )
                        # –ï—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –±–æ–ª—å—à–µ - –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ num_features
                        if features_array.shape[1] > self.num_features:
                            features_array = features_array[:, : self.num_features]
                        else:
                            # –ï—Å–ª–∏ –º–µ–Ω—å—à–µ - –¥–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
                            padding = np.zeros(
                                (
                                    features_array.shape[0],
                                    self.num_features - features_array.shape[1],
                                )
                            )
                            features_array = np.hstack([features_array, padding])

                elif isinstance(features_result, np.ndarray):
                    features_array = features_result
                else:
                    raise ValueError(
                        f"Expected DataFrame or np.ndarray from create_features, got {type(features_result)}"
                    )

                # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ context_length —Å—Ç—Ä–æ–∫
                if len(features_array) >= self.context_length:
                    features = features_array[-self.context_length :]
                else:
                    # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–µ–Ω—å—à–µ —á–µ–º –Ω—É–∂–Ω–æ - –¥–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ (padding)
                    padding_size = self.context_length - len(features_array)
                    padding = np.zeros((padding_size, features_array.shape[1]))
                    features = np.vstack([padding, features_array])

                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ scaler
                features_scaled = self.scaler.transform(features)
                logger.info("‚úÖ –î–∞–Ω–Ω—ã–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã —Å –ø–æ–º–æ—â—å—é scaler")

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä
            x = torch.FloatTensor(features_scaled).unsqueeze(0).to(self.device)

            # –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –í–•–û–î–ù–´–• –î–ê–ù–ù–´–•
            logger.warning(
                f"""
üîç ML –í–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï - –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìä Feature Statistics:
   Shape: {features_scaled.shape}
   Min: {features_scaled.min():.6f}
   Max: {features_scaled.max():.6f}
   Mean: {features_scaled.mean():.6f}
   Std: {features_scaled.std():.6f}

üìà –ü–µ—Ä–≤—ã–µ 10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø–æ—Å–ª–µ–¥–Ω—è—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è —Ç–æ—á–∫–∞):
   {features_scaled[-1, :10]}

üéØ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö:
   NaN count: {np.isnan(features_scaled).sum()}
   Inf count: {np.isinf(features_scaled).sum()}
   Zero variance features: {(features_scaled.std(axis=0) < 1e-6).sum()}
   üîç Variance statistics:
     Min std: {features_scaled.std(axis=0).min():.8f}
     Max std: {features_scaled.std(axis=0).max():.8f}
     Mean std: {features_scaled.std(axis=0).mean():.8f}
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
"""
            )
            logger.debug(f"Input tensor shape: {x.shape}")

            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ inference
            if self.device.type == "cuda":
                gpu_memory_before = torch.cuda.memory_allocated(self.device) / 1024**2  # MB
                gpu_memory_cached = torch.cuda.memory_reserved(self.device) / 1024**2  # MB
                logger.debug(
                    f"GPU Memory before inference: {gpu_memory_before:.1f}MB allocated, {gpu_memory_cached:.1f}MB cached"
                )

            # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∑–∞–º–µ—Ä–æ–º –≤—Ä–µ–º–µ–Ω–∏
            import time

            start_time = time.time()

            with torch.no_grad():
                outputs = self.model(x)

            inference_time = (time.time() - start_time) * 1000  # –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö

            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ inference
            if self.device.type == "cuda":
                gpu_memory_after = torch.cuda.memory_allocated(self.device) / 1024**2  # MB
                logger.debug(f"GPU Memory after inference: {gpu_memory_after:.1f}MB allocated")
                logger.info(f"Inference time: {inference_time:.1f}ms on {self.device}")

            # –û—Ç–ª–∞–¥–∫–∞ –≤—ã—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏
            outputs_np = outputs.cpu().numpy()[0]
            logger.debug(
                f"Model outputs: min={outputs_np.min():.3f}, max={outputs_np.max():.3f}, mean={outputs_np.mean():.3f}"
            )
            logger.debug(f"Model outputs sample: {outputs_np[:10]}")

            # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            predictions = self._interpret_predictions(outputs)

            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π symbol –ø–∞—Ä–∞–º–µ—Ç—Ä –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ DataFrame
                if symbol:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π —Å–∏–º–≤–æ–ª
                    pass
                elif isinstance(input_data, pd.DataFrame) and "symbol" in input_data.columns:
                    symbol = input_data["symbol"].iloc[-1] if not input_data.empty else "UNKNOWN"
                else:
                    symbol = "UNKNOWN"

                # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ª–æ–≥–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                import asyncio

                if asyncio.iscoroutinefunction(ml_prediction_logger.log_prediction):
                    await ml_prediction_logger.log_prediction(
                        symbol=symbol,
                        features=features_scaled[-1],  # –ü–æ—Å–ª–µ–¥–Ω—è—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è —Ç–æ—á–∫–∞
                        model_outputs=outputs_np,
                        predictions=predictions,
                        market_data=input_data if isinstance(input_data, pd.DataFrame) else None,
                    )
                else:
                    # –ï—Å–ª–∏ –º–µ—Ç–æ–¥ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π, —Å–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É
                    asyncio.create_task(
                        ml_prediction_logger.log_prediction(
                            symbol=symbol,
                            features=features_scaled[-1],
                            model_outputs=outputs_np,
                            predictions=predictions,
                            market_data=(
                                input_data if isinstance(input_data, pd.DataFrame) else None
                            ),
                        )
                    )
            except Exception as log_error:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {log_error}")

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º heartbeat –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            try:
                if hasattr(self, "worker_id") and self.worker_id:
                    await worker_coordinator.heartbeat(
                        self.worker_id, status="running", active_tasks=1
                    )
            except Exception as heartbeat_error:
                logger.warning(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ heartbeat: {heartbeat_error}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Å–∏–≥–Ω–∞–ª–∞ –ø–µ—Ä–µ–¥ –≤–æ–∑–≤—Ä–∞—Ç–æ–º
            try:
                # –°–æ–∑–¥–∞–µ–º —Å–∏–≥–Ω–∞–ª –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
                signal_data = {
                    "symbol": symbol,
                    "direction": predictions.get("primary_direction", "NEUTRAL"),
                    "strategy": "ml_patchtst",
                    "timestamp": datetime.now(),
                    "signal_strength": predictions.get("primary_confidence", 0.0),
                    "price_level": predictions.get("primary_returns", {}).get("15m", 0.0),
                }

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Å–∏–≥–Ω–∞–ª–∞
                is_unique = await signal_deduplicator.check_and_register_signal(signal_data)
                if not is_unique:
                    logger.warning(f"üîÑ –î—É–±–ª–∏–∫–∞—Ç ML —Å–∏–≥–Ω–∞–ª–∞ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω –¥–ª—è {symbol}")
                    predictions["is_duplicate"] = True
                else:
                    predictions["is_duplicate"] = False

            except Exception as dedup_error:
                logger.warning(f"–û—à–∏–±–∫–∞ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ —Å–∏–≥–Ω–∞–ª–∞: {dedup_error}")
                predictions["is_duplicate"] = False

            logger.info(f"ML Manager –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {predictions}")
            return predictions

        except Exception as e:
            logger.error(f"Error making prediction: {e}")
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º heartbeat –æ–± –æ—à–∏–±–∫–µ
            try:
                if hasattr(self, "worker_id") and self.worker_id:
                    await worker_coordinator.heartbeat(self.worker_id, status="error")
            except Exception as heartbeat_error:
                logger.warning(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ heartbeat –ø—Ä–∏ –æ—à–∏–±–∫–µ: {heartbeat_error}")
            raise

    def _interpret_predictions(self, outputs: torch.Tensor) -> dict[str, Any]:
        """
        –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≤—ã—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏.

        Args:
            outputs: –¢–µ–Ω–∑–æ—Ä —Å 32 –≤—ã—Ö–æ–¥–∞–º–∏ –º–æ–¥–µ–ª–∏

        Returns:
            Dict —Å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏
        """
        outputs_np = outputs.cpu().numpy()[0]

        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤—ã—Ö–æ–¥–æ–≤ (20 –∑–Ω–∞—á–µ–Ω–∏–π):
        # 0-3: future returns (15m, 1h, 4h, 12h)
        # 4-15: direction logits (12 values = 3 classes √ó 4 timeframes)
        # 16-19: risk metrics

        # –í–ê–ñ–ù–û: –≤ –º–æ–¥–µ–ª–∏ —Å 20 –≤—ã—Ö–æ–¥–∞–º–∏ –Ω–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã—Ö long/short levels!
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º future returns –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —É—Ä–æ–≤–Ω–µ–π

        future_returns = outputs_np[0:4]
        direction_logits = outputs_np[4:16]  # 12 –∑–Ω–∞—á–µ–Ω–∏–π!
        risk_metrics = outputs_np[16:20]

        # –í –º–æ–¥–µ–ª–∏ —Å 20 –≤—ã—Ö–æ–¥–∞–º–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º future_returns –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —É—Ä–æ–≤–Ω–µ–π
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç—ã –¥–ª—è SL/TP
        long_levels = future_returns  # –ò—Å–ø–æ–ª—å–∑—É–µ–º future returns –∫–∞–∫ –±–∞–∑—É –¥–ª—è —É—Ä–æ–≤–Ω–µ–π
        short_levels = -future_returns  # –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø–æ–∑–∏—Ü–∏–π

        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º risk_metrics –∫–∞–∫ confidence_scores
        # –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –∫–∞–∫ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å—Ä–µ–¥–∏ –∫–ª–∞—Å—Å–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
        confidence_scores = np.zeros(4)
        for i in range(4):
            logits = direction_logits[i * 3 : (i + 1) * 3]
            probs = np.exp(logits) / np.sum(np.exp(logits))
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–∞–∫ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            confidence_scores[i] = np.max(probs)

        # –î–ò–ê–ì–ù–û–°–¢–ò–ß–ï–°–ö–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï
        logger.warning(
            f"""
üîç ML –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìä Raw Model Outputs (–≤—Å–µ 20 –∑–Ω–∞—á–µ–Ω–∏–π):
   {outputs_np}

üìà Future Returns (0-3):
   15m: {future_returns[0]:.6f}
   1h:  {future_returns[1]:.6f}
   4h:  {future_returns[2]:.6f}
   12h: {future_returns[3]:.6f}

üéØ Direction Logits (4-15) - 12 –∑–Ω–∞—á–µ–Ω–∏–π:
   Raw logits: {direction_logits}
   –°—Ç—Ä—É–∫—Ç—É—Ä–∞: 4 —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ √ó 3 –∫–ª–∞—Å—Å–∞ (LONG=0, SHORT=1, NEUTRAL=2)

‚ö° Risk Metrics (16-19):
   {risk_metrics}
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
"""
        )

        # –ü–†–ê–í–ò–õ–¨–ù–ê–Ø –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø DIRECTIONS (12 –∑–Ω–∞—á–µ–Ω–∏–π = 4 —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ √ó 3 –∫–ª–∞—Å—Å–∞)
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ 4 –≥—Ä—É–ø–ø—ã –ø–æ 3 –ª–æ–≥–∏—Ç–∞
        direction_logits_reshaped = direction_logits.reshape(4, 3)  # 4 —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ √ó 3 –∫–ª–∞—Å—Å–∞

        # –ü—Ä–∏–º–µ–Ω—è–µ–º softmax –∫ –∫–∞–∂–¥–æ–º—É —Ç–∞–π–º—Ñ—Ä–µ–π–º—É
        directions = []
        direction_probs = []

        for i, logits in enumerate(direction_logits_reshaped):
            # Softmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            exp_logits = np.exp(logits - np.max(logits))  # –î–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            probs = exp_logits / exp_logits.sum()
            direction_probs.append(probs)

            # Argmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–∞ (0=LONG, 1=SHORT, 2=NEUTRAL)
            direction_class = np.argmax(probs)
            directions.append(direction_class)

            logger.info(
                f"–¢–∞–π–º—Ñ—Ä–µ–π–º {i + 1}: logits={logits}, probs={probs}, class={direction_class}"
            )

        directions = np.array(directions)
        logger.info(f"Direction predictions: {directions}")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Å–∏–≥–Ω–∞–ª
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∫–ª–∞—Å—Å–æ–≤ —Å –±–æ–ª—å—à–∏–º –≤–µ—Å–æ–º –Ω–∞ –±–ª–∏–∂–∞–π—à–∏–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º—ã
        weights = np.array([0.4, 0.3, 0.2, 0.1])
        weighted_direction = np.sum(directions * weights)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–∏–ª—É —Å–∏–≥–Ω–∞–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        # –ß–µ–º –±–æ–ª—å—à–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ —Å–æ–≥–ª–∞—Å–Ω—ã, —Ç–µ–º —Å–∏–ª—å–Ω–µ–µ —Å–∏–≥–Ω–∞–ª
        direction_counts = np.bincount(directions, minlength=3)
        max_agreement = direction_counts.max() / len(directions)
        signal_strength = max_agreement  # –æ—Ç 0.25 –¥–æ 1.0

        logger.info(f"Weighted direction: {weighted_direction:.3f}")
        logger.info(
            f"Direction counts: LONG={direction_counts[0]}, SHORT={direction_counts[1]}, NEUTRAL={direction_counts[2]}"
        )
        logger.info(f"Signal strength (agreement): {signal_strength:.3f}")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–∏–≥–Ω–∞–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–ª–∞—Å—Å–æ–≤ –º–æ–¥–µ–ª–∏ + —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–æ—Ä–æ–≥–∏
        # –í –æ–±—É—á–µ–Ω–∏–∏: 0=LONG (–ø–æ–∫—É–ø–∫–∞), 1=SHORT (–ø—Ä–æ–¥–∞–∂–∞), 2=FLAT (–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ)
        #
        # –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê –° –£–ß–ï–¢–û–ú –°–ò–õ–´ –°–ò–ì–ù–ê–õ–ê:
        # weighted_direction –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0.0 - 2.0
        # –≥–¥–µ 0.0 = –≤—Å–µ LONG, 1.0 = –≤—Å–µ SHORT, 2.0 = –≤—Å–µ NEUTRAL
        #
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ñ–∞–∫—Ç–æ—Ä
        # –ï—Å–ª–∏ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ —Å–æ–≥–ª–∞—Å–Ω—ã - —Å–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–æ–±–∞–≤–ª—è–µ–º multiframe_confirmation –∫–∞–∫ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        # –§–æ–∫—É—Å –Ω–∞ 4h —Ç–∞–π–º—Ñ—Ä–µ–π–º (–æ—Å–Ω–æ–≤–Ω–æ–π) + –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –æ—Ç –¥—Ä—É–≥–∏—Ö
        timeframe_names = ["15m", "1h", "4h", "12h"]
        main_timeframe_idx = 2  # 4h - –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–π–º—Ñ—Ä–µ–π–º –∫–∞–∫ –≤ –æ–±—É—á–µ–Ω–∏–∏

        near_term_directions = directions[:2]  # 15m –∏ 1h
        far_term_directions = directions[2:]  # 4h –∏ 12h
        near_term_returns = future_returns[:2]  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ 15m –∏ 1h –¥–ª—è —Ä–∞—Å—á–µ—Ç–æ–≤

        # –°—á–∏—Ç–∞–µ–º –≥–æ–ª–æ—Å–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –Ω–∞ –±–ª–∏–∂–∞–π—à–∏–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º—ã
        near_long = np.sum(near_term_directions == 0)
        near_short = np.sum(near_term_directions == 1)
        near_neutral = np.sum(near_term_directions == 2)

        far_long = np.sum(far_term_directions == 0)
        far_short = np.sum(far_term_directions == 1)

        # –£–õ–£–ß–®–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê –° –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–û–ô:
        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
        max_probs = []
        for probs in direction_probs:
            max_probs.append(np.max(probs))
        avg_max_prob = np.mean(max_probs)

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (0.4 = 40%)
        if avg_max_prob < 0.4:  # –ü–æ—Ä–æ–≥ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ direction_confidence_threshold
            signal_type = "NEUTRAL"
            signal_strength = 0.25  # –ë–∞–∑–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∫–∞–∫ –≤ –æ–±—É—á–µ–Ω–∏–∏
            logger.info(f"–ú–æ–¥–µ–ª—å –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–∞, avg_max_prob={avg_max_prob:.3f} < 0.4")

        # 2. –ï—Å–ª–∏ –æ–±–∞ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ —Å–æ–≥–ª–∞—Å–Ω—ã - —Å–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª
        elif near_long == 2:  # –û–±–∞ –±–ª–∏–∂–∞–π—à–∏—Ö –∑–∞ LONG
            signal_type = "LONG"
            signal_strength = 0.9  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        elif near_short == 2:  # –û–±–∞ –±–ª–∏–∂–∞–π—à–∏—Ö –∑–∞ SHORT
            signal_type = "SHORT"
            signal_strength = 0.9  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å

        # 3. –ï—Å–ª–∏ –±–ª–∏–∂–∞–π—à–∏–µ —Ä–∞–∑–¥–µ–ª–∏–ª–∏—Å—å, —Å–º–æ—Ç—Ä–∏–º –Ω–∞ –¥–∞–ª—å–Ω–∏–µ
        elif near_long == 1 and near_short == 0:
            # –û–¥–∏–Ω LONG –≤ –±–ª–∏–∂–∞–π—à–∏—Ö, –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É –¥–∞–ª—å–Ω–∏—Ö
            if far_long >= 1:  # –ï—Å—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –æ—Ç –¥–∞–ª—å–Ω–∏—Ö
                signal_type = "LONG"
                signal_strength = 0.7
            else:
                signal_type = "NEUTRAL"  # –ù–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∫–∏
                signal_strength = 0.4

        elif near_short == 1 and near_long == 0:
            # –û–¥–∏–Ω SHORT –≤ –±–ª–∏–∂–∞–π—à–∏—Ö, –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É –¥–∞–ª—å–Ω–∏—Ö
            if far_short >= 1:  # –ï—Å—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –æ—Ç –¥–∞–ª—å–Ω–∏—Ö
                signal_type = "SHORT"
                signal_strength = 0.7
            else:
                signal_type = "NEUTRAL"  # –ù–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∫–∏
                signal_strength = 0.4

        # 4. –ö–æ–Ω—Ñ–ª–∏–∫—Ç –≤ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞—Ö –∏–ª–∏ –≤—Å–µ NEUTRAL
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è
            avg_near_return = np.mean(near_term_returns)

            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º focal weighting –¥–ª—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–∫–∞–∫ –≤ –æ–±—É—á–µ–Ω–∏–∏)
            # –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
            focal_threshold = 0.005  # 0.5% - –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π –ø–æ—Ä–æ–≥ –∫–∞–∫ –≤ –æ–±—É—á–µ–Ω–∏–∏
            if abs(avg_near_return) > focal_threshold:
                if avg_near_return > 0:
                    signal_type = "LONG"
                else:
                    signal_type = "SHORT"
                signal_strength = 0.5  # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            else:
                signal_type = "NEUTRAL"
                signal_strength = 0.25  # –ë–∞–∑–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∫–∞–∫ –≤ –æ–±—É—á–µ–Ω–∏–∏

        # –£–ü–†–û–©–ï–ù–ù–´–ô –†–ê–°–ß–ï–¢ SL/TP –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∏–ª—ã —Å–∏–≥–Ω–∞–ª–∞ –∏ –±–ª–∏–∂–∞–π—à–∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        if signal_type in ["LONG", "SHORT"]:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ SL/TP –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–∏–ª—ã —Å–∏–≥–Ω–∞–ª–∞

            # –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ)
            base_sl = 0.01  # 1%
            base_tp = 0.02  # 2%

            # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∏–ª—ã —Å–∏–≥–Ω–∞–ª–∞
            if signal_strength >= 0.8:  # –°–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª
                # –ú–æ–∂–µ–º –ø–æ–∑–≤–æ–ª–∏—Ç—å —á—É—Ç—å –±–æ–ª—å—à–∏–π —Å—Ç–æ–ø –∏ –±–æ–ª—å—à–∏–π —Ç–µ–π–∫
                stop_loss_pct = base_sl * 1.5  # 1.5%
                take_profit_pct = base_tp * 1.5  # 3%
            elif signal_strength >= 0.6:  # –°—Ä–µ–¥–Ω–∏–π —Å–∏–≥–Ω–∞–ª
                stop_loss_pct = base_sl * 1.2  # 1.2%
                take_profit_pct = base_tp * 1.2  # 2.4%
            else:  # –°–ª–∞–±—ã–π —Å–∏–≥–Ω–∞–ª
                # –û—á–µ–Ω—å –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                stop_loss_pct = base_sl * 0.8  # 0.8%
                take_profit_pct = base_tp * 0.8  # 1.6%

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π
            avg_near_return = np.mean(near_term_returns)
            volatility = np.std(near_term_returns)

            # –ï—Å–ª–∏ –≤—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å - —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—Ç–æ–ø—ã
            if volatility > 0.01:  # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å > 1%
                stop_loss_pct *= 1.2
                take_profit_pct *= 1.2

            # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤
            stop_loss_pct = np.clip(stop_loss_pct, 0.005, 0.02)  # 0.5% - 2%
            take_profit_pct = np.clip(take_profit_pct, 0.01, 0.04)  # 1% - 4%

        else:
            stop_loss_pct = None
            take_profit_pct = None

        # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ü–µ–Ω—ã SL/TP –±—É–¥—É—Ç —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã –≤ ml_signal_processor
        # –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–π —Ü–µ–Ω—ã –∏ —ç—Ç–∏—Ö –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤

        # –û—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–∞
        avg_risk = float(np.mean(risk_metrics))
        risk_level = "LOW" if avg_risk < 0.3 else "MEDIUM" if avg_risk < 0.7 else "HIGH"

        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ:
        # 1. –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (signal_strength)
        # 2. Confidence scores –æ—Ç –º–æ–¥–µ–ª–∏
        # 3. –†–∏—Å–∫ –º–µ—Ç—Ä–∏–∫

        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: confidence_scores —É–∂–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (0-1), –Ω–µ –Ω—É–∂–µ–Ω sigmoid
        model_confidence = float(np.mean(confidence_scores))

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∏–º–µ–Ω—è–µ–º focal weighting –∫–∞–∫ –≤ –æ–±—É—á–µ–Ω–∏–∏ (focal_alpha=0.25)
        # Focal Loss formula: alpha * (1 - p)^gamma, –≥–¥–µ gamma=2.0 –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        focal_alpha = 0.25
        focal_gamma = 2.0

        # –ü—Ä–∏–º–µ–Ω—è–µ–º focal weighting –∫ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
        focal_weighted_confidence = focal_alpha * (1 - model_confidence) ** focal_gamma

        # –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø —Ñ–æ—Ä–º—É–ª–∞ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ —Å focal weighting
        # –ë–∞–∑–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø–æ—Ä–æ–≥—É –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (0.3)
        base_confidence = 0.3 if signal_type in ["LONG", "SHORT"] else 0.25

        # –ë–æ–Ω—É—Å –∑–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π (–∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –Ω–æ–≤–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞)
        consistency_bonus = 0.0
        if signal_type in ["LONG", "SHORT"]:
            # –ï—Å–ª–∏ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ —Å–æ–≥–ª–∞—Å–Ω—ã - –¥–∞–µ–º –±–æ–Ω—É—Å
            if len(set(near_term_directions)) == 1:  # –í—Å–µ –±–ª–∏–∂–∞–π—à–∏–µ —Å–æ–≥–ª–∞—Å–Ω—ã
                consistency_bonus = 0.3
            elif near_term_directions[0] == near_term_directions[1]:  # –•–æ—Ç—è –±—ã –¥–≤–∞ —Å–æ–≥–ª–∞—Å–Ω—ã
                consistency_bonus = 0.15

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Å focal weighting –∏ multiframe confirmation
        # –î–æ–±–∞–≤–ª—è–µ–º –º—É–ª—å—Ç–∏—Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤–æ–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
        multiframe_bonus = 0.0
        if signal_type in ["LONG", "SHORT"]:
            main_direction = directions[main_timeframe_idx]  # 4h –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
            main_signal = 0 if signal_type == "LONG" else 1

            if main_direction == main_signal:
                multiframe_bonus += 0.2  # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–π–º—Ñ—Ä–µ–π–º –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç

                # –°—á–∏—Ç–∞–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É –æ—Ç –¥—Ä—É–≥–∏—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
                other_support = sum(1 for d in directions if d == main_signal)
                if other_support >= 3:  # 3+ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ —Å–æ–≥–ª–∞—Å–Ω—ã
                    multiframe_bonus += 0.15
                elif other_support >= 2:  # 2+ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ —Å–æ–≥–ª–∞—Å–Ω—ã
                    multiframe_bonus += 0.1

        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Å focal weighting
        combined_confidence = min(
            0.95,
            base_confidence
            + signal_strength * 0.25
            + model_confidence * 0.2
            + focal_weighted_confidence * 0.1
            + (1.0 - avg_risk) * 0.1
            + consistency_bonus
            + multiframe_bonus,
        )

        # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞
        success_probability = combined_confidence

        # –õ–æ–≥–∏—Ä—É–µ–º –≤—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        sl_str = f"{stop_loss_pct:.3f}" if stop_loss_pct is not None else "–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω"
        tp_str = f"{take_profit_pct:.3f}" if take_profit_pct is not None else "–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω"

        logger.info(
            f"""
üìä ML –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (–ü–†–ê–í–ò–õ–¨–ù–ê–Ø –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø):
   –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {signal_type} (–≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ: {weighted_direction:.3f})
   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º: {directions}
   –°–∏–ª–∞ —Å–∏–≥–Ω–∞–ª–∞ (—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å): {signal_strength:.3f}
   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {model_confidence:.1%}
   –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {combined_confidence:.1%}
   –†–∏—Å–∫: {risk_level} ({avg_risk:.3f})
   –ë—É–¥—É—â–∏–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏: 15–º={future_returns[0]:.3f}, 1—á={future_returns[1]:.3f}, 4—á={future_returns[2]:.3f}, 12—á={future_returns[3]:.3f}
   Stop Loss %: {sl_str}
   Take Profit %: {tp_str}
"""
        )

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        direction_map = {0: "LONG", 1: "SHORT", 2: "NEUTRAL"}

        return {
            "signal_type": signal_type,
            "signal_strength": float(signal_strength),
            "confidence": float(combined_confidence),
            "signal_confidence": float(combined_confidence),  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ª–æ–≥–≥–µ—Ä–æ–º
            "success_probability": float(success_probability),
            "stop_loss_pct": stop_loss_pct,  # –ü—Ä–æ—Ü–µ–Ω—Ç, –Ω–µ –∞–±—Å–æ–ª—é—Ç–Ω–∞—è —Ü–µ–Ω–∞!
            "take_profit_pct": take_profit_pct,  # –ü—Ä–æ—Ü–µ–Ω—Ç, –Ω–µ –∞–±—Å–æ–ª—é—Ç–Ω–∞—è —Ü–µ–Ω–∞!
            "risk_level": risk_level,
            "risk_score": float(avg_risk),  # –î–ª—è –ª–æ–≥–≥–µ—Ä–∞
            "max_drawdown": float(risk_metrics[0]) if len(risk_metrics) > 0 else 0,
            "max_rally": float(risk_metrics[1]) if len(risk_metrics) > 1 else 0,
            "primary_timeframe": "15m",  # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–π–º—Ñ—Ä–µ–π–º
            # –î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –ª–æ–≥–≥–µ—Ä–∞
            "returns_15m": float(future_returns[0]),
            "returns_1h": float(future_returns[1]),
            "returns_4h": float(future_returns[2]),
            "returns_12h": float(future_returns[3]),
            # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º
            "direction_15m": direction_map.get(int(directions[0]), "NEUTRAL"),
            "direction_1h": direction_map.get(int(directions[1]), "NEUTRAL"),
            "direction_4h": direction_map.get(int(directions[2]), "NEUTRAL"),
            "direction_12h": direction_map.get(int(directions[3]), "NEUTRAL"),
            "confidence_15m": float(confidence_scores[0]),
            "confidence_1h": float(confidence_scores[1]),
            "confidence_4h": float(confidence_scores[2]),
            "confidence_12h": float(confidence_scores[3]),
            "predictions": {
                "returns_15m": float(future_returns[0]),
                "returns_1h": float(future_returns[1]),
                "returns_4h": float(future_returns[2]),
                "returns_12h": float(future_returns[3]),
                "direction_score": float(weighted_direction),
                "directions_by_timeframe": directions.tolist(),  # [15m, 1h, 4h, 12h]
                "direction_probabilities": [p.tolist() for p in direction_probs],
            },
            "timestamp": datetime.now(UTC).isoformat(),
        }

    async def update_model(self, new_model_path: str):
        """
        –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é.

        Args:
            new_model_path: –ü—É—Ç—å –∫ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        """
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ä—É—é –º–æ–¥–µ–ª—å –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤–Ω—É—é
            backup_path = self.model_path.with_suffix(".pth.backup")
            if self.model_path.exists():
                self.model_path.rename(backup_path)

            # –ö–æ–ø–∏—Ä—É–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
            Path(new_model_path).rename(self.model_path)

            # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            await self._load_model()

            logger.info(f"Model updated successfully from {new_model_path}")

        except Exception as e:
            logger.error(f"Error updating model: {e}")
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—Ç–∞—Ä—É—é –º–æ–¥–µ–ª—å
            if backup_path.exists():
                backup_path.rename(self.model_path)
            raise

    def get_model_info(self) -> dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        return {
            "model_type": "UnifiedPatchTST",
            "model_path": str(self.model_path),
            "context_length": self.context_length,
            "num_features": self.num_features,
            "num_targets": self.num_targets,
            "device": str(self.device),
            "model_loaded": self.model is not None,
            "scaler_loaded": self.scaler is not None,
        }
