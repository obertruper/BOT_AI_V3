#!/usr/bin/env python3
"""
ML Manager –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è PatchTST –º–æ–¥–µ–ª—å—é –≤ BOT Trading v3
"""

import os
import pickle
from datetime import UTC, datetime
from pathlib import Path
from typing import Any

import numpy as np
import pandas as pd
import torch

from core.logger import setup_logger
from core.system.signal_deduplicator import signal_deduplicator
from core.system.worker_coordinator import worker_coordinator
from ml.logic.feature_engineering import (  # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å 240+ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    FeatureEngineer,
)
from ml.logic.patchtst_model import create_unified_model
from ml.logic.signal_quality_analyzer import SignalQualityAnalyzer
from ml.ml_prediction_logger import ml_prediction_logger

logger = setup_logger("ml_manager")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ GPU –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è - –¢–û–ß–ù–ê–Ø –ö–û–ü–ò–Ø –∏–∑ —Ä–∞–±–æ—á–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞
if torch.cuda.is_available():
    try:
        # Benchmark mode –¥–ª—è cudnn
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.enabled = True

        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ float32 matmul precision –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ –Ω–æ–≤—ã—Ö GPU
        # torch.set_float32_matmul_precision('high')  # –í—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–æ –∏–∑-–∑–∞ warning

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è Ampere+ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (RTX 5090)
        # torch.backends.cuda.matmul.allow_tf32 = True  # Deprecated
        # torch.backends.cudnn.allow_tf32 = True  # Deprecated

        logger.info("‚úÖ –ì–ª–æ–±–∞–ª—å–Ω—ã–µ GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–∫–ª—é—á–µ–Ω—ã")
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤–∫–ª—é—á–∏—Ç—å GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {e}")


class MLManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è ML –º–æ–¥–µ–ª—è–º–∏ –≤ —Ç–æ—Ä–≥–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–µ.
    –†–∞–±–æ—Ç–∞–µ—Ç —Å PatchTST –º–æ–¥–µ–ª—å—é –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–π —Ä—ã–Ω–∫–∞.
    """

    def __init__(self, config: dict[str, Any]):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ML –º–µ–Ω–µ–¥–∂–µ—Ä–∞.

        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã
        """
        self.config = config
        self.model = None
        self.scaler = None
        self.feature_engineer = None
        # –ü–æ–ª—É—á–∞–µ–º device –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        device_config = config.get("ml", {}).get("model", {}).get("device", "auto")

        # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GPU —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        if device_config == "auto":
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ CUDA
                if torch.cuda.is_available():
                    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU
                    gpu_count = torch.cuda.device_count()
                    if gpu_count > 0:
                        # –í—ã–±–∏—Ä–∞–µ–º GPU —Å –Ω–∞–∏–º–µ–Ω—å—à–µ–π –∑–∞–≥—Ä—É–∑–∫–æ–π –ø–∞–º—è—Ç–∏
                        best_gpu = 0
                        min_memory_used = float("inf")

                        for i in range(gpu_count):
                            try:
                                torch.cuda.set_device(i)
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å GPU
                                props = torch.cuda.get_device_properties(i)
                                logger.info(
                                    f"GPU {i}: {props.name}, "
                                    f"Compute Capability: {props.major}.{props.minor}, "
                                    f"Memory: {props.total_memory / 1024**3:.1f}GB"
                                )

                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
                                if torch.cuda.memory_allocated(i) < min_memory_used:
                                    min_memory_used = torch.cuda.memory_allocated(i)
                                    best_gpu = i
                            except Exception as gpu_error:
                                logger.warning(f"GPU {i} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {gpu_error}")
                                continue

                        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ª—É—á—à–∏–π GPU
                        torch.cuda.set_device(best_gpu)
                        self.device = torch.device(f"cuda:{best_gpu}")

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å GPU —Ç–µ—Å—Ç–æ–≤—ã–º —Ç–µ–Ω–∑–æ—Ä–æ–º
                        test_tensor = torch.zeros(1, 1).to(self.device)
                        _ = test_tensor * 2  # –ü—Ä–æ—Å—Ç–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏

                        # RTX 5090 (Blackwell) –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
                        # - GPU –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–µ–Ω –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ —Å–∏—Å—Ç–µ–º—ã
                        # - torch.compile –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É sm_120
                        # - –≠—Ç–æ –Ω–µ –≤–ª–∏—è–µ—Ç –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–ª–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
                        gpu_name = props.name.upper()
                        if "RTX 5090" in gpu_name or props.major >= 12:
                            logger.info(
                                f"üéØ –û–±–Ω–∞—Ä—É–∂–µ–Ω RTX 5090 ({gpu_name}, sm_{props.major}{props.minor})"
                            )
                            logger.warning(
                                "‚ö†Ô∏è torch.compile –æ—Ç–∫–ª—é—á–µ–Ω –¥–ª—è RTX 5090 (sm_120) - –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–µ–π PyTorch. –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –∏ –Ω–µ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å."
                            )
                            # –û—Ç–∫–ª—é—á–∞–µ–º –∫–æ–º–ø–∏–ª—è—Ü–∏—é –¥–ª—è –Ω–æ–≤—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
                            os.environ["TORCH_COMPILE_DISABLE"] = "1"

                        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω GPU {best_gpu} ({props.name})")
                        logger.info(
                            f"üíæ GPU –ø–∞–º—è—Ç—å –¥–æ—Å—Ç—É–ø–Ω–∞: {props.total_memory / 1024**3:.2f} GB"
                        )
                    else:
                        logger.warning("CUDA –¥–æ—Å—Ç—É–ø–Ω–∞, –Ω–æ GPU –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                        self.device = torch.device("cpu")
                else:
                    logger.info("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")
                    self.device = torch.device("cpu")

            except Exception as e:
                # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–∫–∏
                logger.warning(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ GPU: {type(e).__name__}: {e}")
                logger.info("–ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ CPU")
                self.device = torch.device("cpu")

                # –û—á–∏—â–∞–µ–º CUDA –∫–µ—à –ø—Ä–∏ –æ—à–∏–±–∫–µ
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        else:
            # –†—É—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ device
            try:
                self.device = torch.device(device_config)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å
                if "cuda" in device_config:
                    test_tensor = torch.zeros(1, 1).to(self.device)
                    _ = test_tensor * 2
                logger.info(f"–£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω device: {device_config}")
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å device {device_config}: {e}")
                self.device = torch.device("cpu")

        # –§–ª–∞–≥ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        self._initialized = False

        # –ö—ç—à –º–æ–¥–µ–ª–µ–π (–¥–ª—è MLManager —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Ç–µ—Å—Ç–∞–º–∏)
        self._model_cache = {}
        self._cache_size = config.get("ml", {}).get("model_cache_size", 3)
        self._memory_limit = config.get("ml", {}).get("memory_limit_mb", 1024)

        # –ü—É—Ç–∏ –∫ –º–æ–¥–µ–ª—è–º - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –ø—É—Ç–∏
        base_dir = Path(__file__).parent.parent  # –ö–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞
        model_dir = base_dir / config.get("ml", {}).get("model_directory", "models/saved")
        self.model_path = model_dir / "best_model_20250728_215703.pth"
        self.scaler_path = model_dir / "data_scaler.pkl"

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        self.context_length = 96  # 24 —á–∞—Å–∞ –ø—Ä–∏ 15-–º–∏–Ω—É—Ç–Ω—ã—Ö —Å–≤–µ—á–∞—Ö
        self.num_features = 240  # –í–µ—Ä–Ω—É–ª–∏ –æ–±—Ä–∞—Ç–Ω–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –º–æ–¥–µ–ª—å—é
        self.num_targets = 20  # –ú–æ–¥–µ–ª—å –≤—ã–¥–∞–µ—Ç 20 –≤—ã—Ö–æ–¥–æ–≤

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–≥–Ω–∞–ª–æ–≤
        self.quality_analyzer = SignalQualityAnalyzer(config)

        logger.info(f"MLManager initialized, device: {self.device}")

    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π"""
        try:
            # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º—Å—è –≤ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–µ –≤–æ—Ä–∫–µ—Ä–æ–≤
            await worker_coordinator.start()
            self.worker_id = await worker_coordinator.register_worker(
                worker_type="ml_manager",
                metadata={
                    "device": str(self.device),
                    "model_path": str(self.model_path),
                    "num_features": self.num_features,
                    "context_length": self.context_length,
                },
            )

            if not self.worker_id:
                logger.error("‚ùå –î—Ä—É–≥–æ–π ML Manager —É–∂–µ –∞–∫—Ç–∏–≤–µ–Ω. –ó–∞–≤–µ—Ä—à–∞–µ–º —Ä–∞–±–æ—Ç—É.")
                raise RuntimeError("Duplicate ML Manager detected")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            await self._load_model()

            # –ó–∞–≥—Ä—É–∂–∞–µ–º scaler
            await self._load_scaler()

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º feature engineer
            self.feature_engineer = FeatureEngineer(self.config)

            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
            self._initialized = True

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º heartbeat –æ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
            await worker_coordinator.heartbeat(self.worker_id, status="running")

            logger.info("‚úÖ ML components initialized successfully")

        except Exception as e:
            logger.error(f"Error initializing ML components: {e}")
            if hasattr(self, "worker_id") and self.worker_id:
                await worker_coordinator.unregister_worker(self.worker_id)
            raise

    async def _load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ PatchTST –º–æ–¥–µ–ª–∏"""
        try:
            if not self.model_path.exists():
                raise FileNotFoundError(f"Model file not found: {self.model_path}")

            # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –º–æ–¥–µ–ª–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            model_config = {
                "model": {
                    "input_size": self.num_features,  # 98 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    "output_size": self.num_targets,  # 20 –≤—ã—Ö–æ–¥–æ–≤
                    "context_window": self.context_length,  # 96 –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–æ—á–µ–∫
                    "patch_len": 16,
                    "stride": 8,
                    "d_model": 256,  # –°–æ–≥–ª–∞—Å–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
                    "n_heads": 4,
                    "e_layers": 3,
                    "d_ff": 512,
                    "dropout": 0.1,
                    "temperature_scaling": True,
                    "temperature": 2.0,
                }
            }
            self.model = create_unified_model(model_config)

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π CUDA –æ—à–∏–±–æ–∫
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                checkpoint = torch.load(self.model_path, map_location=self.device)
            except Exception as cuda_error:
                # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ CUDA, –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞ CPU
                logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ {self.device}, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU: {cuda_error}")
                checkpoint = torch.load(self.model_path, map_location=torch.device("cpu"))
                self.device = torch.device("cpu")

            self.model.load_state_dict(checkpoint["model_state_dict"])

            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            try:
                self.model.to(self.device)
            except Exception as e:
                logger.warning(
                    f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ {self.device}, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU: {e}"
                )
                self.device = torch.device("cpu")
                self.model.to(self.device)

            self.model.eval()

            logger.info(f"Model loaded successfully from {self.model_path}")

        except Exception as e:
            logger.error(f"Error loading model: {e}")
            raise

    async def _load_scaler(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ scaler –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            if not self.scaler_path.exists():
                raise FileNotFoundError(f"Scaler file not found: {self.scaler_path}")

            with open(self.scaler_path, "rb") as f:
                self.scaler = pickle.load(f)

            logger.info(f"Scaler loaded successfully from {self.scaler_path}")

        except Exception as e:
            logger.error(f"Error loading scaler: {e}")
            raise

    async def predict(
        self, input_data: pd.DataFrame | np.ndarray, symbol: str | None = None
    ) -> dict[str, Any]:
        """
        –î–µ–ª–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö.

        Args:
            input_data: DataFrame —Å OHLCV –¥–∞–Ω–Ω—ã–º–∏ (–º–∏–Ω–∏–º—É–º 96 —Å–≤–µ—á–µ–π) –∏–ª–∏ numpy array —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            symbol: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–∏–º–≤–æ–ª –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–≥–¥–∞ input_data —ç—Ç–æ numpy array)

        Returns:
            Dict —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
        """
        try:
            # –í–ê–õ–ò–î–ê–¶–ò–Ø: –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞
            if not self._initialized or self.model is None:
                raise ValueError("ML model not initialized. Call initialize() first.")

            # –í–ê–õ–ò–î–ê–¶–ò–Ø: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if not isinstance(input_data, (pd.DataFrame, np.ndarray)):
                raise TypeError(
                    f"input_data must be pd.DataFrame or np.ndarray, got {type(input_data)}"
                )

            # –ï—Å–ª–∏ —ç—Ç–æ numpy array - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å (—É–∂–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)
            if isinstance(input_data, np.ndarray):
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º 3D –º–∞—Å—Å–∏–≤—ã (batch_size, sequence_length, features)
                if input_data.ndim == 3:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –æ–¥–∏–Ω–æ—á–Ω—ã–π –±–∞—Ç—á
                    if input_data.shape[0] == 1:
                        input_data = input_data.squeeze(0)  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–µ–µ –∏–∑–º–µ—Ä–µ–Ω–∏–µ
                    else:
                        raise ValueError(
                            f"Expected single batch, got batch_size={input_data.shape[0]}"
                        )

                # –¢–µ–ø–µ—Ä—å –ø—Ä–æ–≤–µ—Ä—è–µ–º 2D —Ñ–æ—Ä–º—É
                if (
                    input_data.shape[0] != self.context_length
                    or input_data.shape[1] != self.num_features
                ):
                    raise ValueError(
                        f"Expected shape ({self.context_length}, {self.num_features}), got {input_data.shape}"
                    )
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º numpy array —Å –ø–æ–º–æ—â—å—é scaler
                features_scaled = self.scaler.transform(input_data)
                logger.info("‚úÖ Numpy array –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω —Å –ø–æ–º–æ—â—å—é scaler")

            # –ï—Å–ª–∏ —ç—Ç–æ DataFrame - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ OHLCV –¥–∞–Ω–Ω—ã–µ
            else:
                # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ DataFrame —Å async/await –ª–æ–≥–∏–∫–æ–π
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
                if len(input_data) < self.context_length:
                    raise ValueError(
                        f"Need at least {self.context_length} candles, got {len(input_data)}"
                    )

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–∫–∏ symbol
                if "symbol" not in input_data.columns:
                    logger.warning(
                        "‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'symbol' –≤ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö! –≠—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –Ω–µ—Ç–æ—á–Ω—ã–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º."
                    )
                    input_data = input_data.copy()
                    input_data["symbol"] = "UNKNOWN_SYMBOL"  # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–∏–º–≤–æ–ª

                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ - —Ç–µ–ø–µ—Ä—å —ç—Ç–æ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤
                features_result = self.feature_engineer.create_features(input_data)

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç - –º–æ–∂–µ—Ç –±—ã—Ç—å DataFrame –∏–ª–∏ ndarray
                if isinstance(features_result, pd.DataFrame):
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ DataFrame
                    numeric_cols = features_result.select_dtypes(include=[np.number]).columns
                    # –ò—Å–∫–ª—é—á–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    feature_cols = [
                        col
                        for col in numeric_cols
                        if not col.startswith(("future_", "direction_", "profit_"))
                        and col not in ["id", "timestamp", "datetime", "symbol"]
                    ]
                    features_array = features_result[feature_cols].values

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    if features_array.shape[1] != self.num_features:
                        logger.warning(
                            f"Feature count mismatch: expected {self.num_features}, got {features_array.shape[1]}"
                        )
                        # –ï—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –±–æ–ª—å—à–µ - –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ num_features
                        if features_array.shape[1] > self.num_features:
                            features_array = features_array[:, : self.num_features]
                        else:
                            # –ï—Å–ª–∏ –º–µ–Ω—å—à–µ - –¥–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
                            padding = np.zeros(
                                (
                                    features_array.shape[0],
                                    self.num_features - features_array.shape[1],
                                )
                            )
                            features_array = np.hstack([features_array, padding])

                elif isinstance(features_result, np.ndarray):
                    features_array = features_result
                else:
                    raise ValueError(
                        f"Expected DataFrame or np.ndarray from create_features, got {type(features_result)}"
                    )

                # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ context_length —Å—Ç—Ä–æ–∫
                if len(features_array) >= self.context_length:
                    features = features_array[-self.context_length :]
                else:
                    # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–µ–Ω—å—à–µ —á–µ–º –Ω—É–∂–Ω–æ - –¥–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ (padding)
                    padding_size = self.context_length - len(features_array)
                    padding = np.zeros((padding_size, features_array.shape[1]))
                    features = np.vstack([padding, features_array])

                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ scaler
                features_scaled = self.scaler.transform(features)
                logger.info("‚úÖ –î–∞–Ω–Ω—ã–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã —Å –ø–æ–º–æ—â—å—é scaler")

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä
            x = torch.FloatTensor(features_scaled).unsqueeze(0).to(self.device)

            # –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –í–•–û–î–ù–´–• –î–ê–ù–ù–´–•
            logger.warning(
                f"""
üîç ML –í–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï - –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìä Feature Statistics:
   Shape: {features_scaled.shape}
   Min: {features_scaled.min():.6f}
   Max: {features_scaled.max():.6f}
   Mean: {features_scaled.mean():.6f}
   Std: {features_scaled.std():.6f}

üìà –ü–µ—Ä–≤—ã–µ 10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø–æ—Å–ª–µ–¥–Ω—è—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è —Ç–æ—á–∫–∞):
   {features_scaled[-1, :10]}

üéØ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö:
   NaN count: {np.isnan(features_scaled).sum()}
   Inf count: {np.isinf(features_scaled).sum()}
   Zero variance features: {(features_scaled.std(axis=0) < 1e-6).sum()}
   üîç Variance statistics:
     Min std: {features_scaled.std(axis=0).min():.8f}
     Max std: {features_scaled.std(axis=0).max():.8f}
     Mean std: {features_scaled.std(axis=0).mean():.8f}
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
"""
            )
            logger.debug(f"Input tensor shape: {x.shape}")

            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ inference
            if self.device.type == "cuda":
                gpu_memory_before = torch.cuda.memory_allocated(self.device) / 1024**2  # MB
                gpu_memory_cached = torch.cuda.memory_reserved(self.device) / 1024**2  # MB
                logger.debug(
                    f"GPU Memory before inference: {gpu_memory_before:.1f}MB allocated, {gpu_memory_cached:.1f}MB cached"
                )

            # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∑–∞–º–µ—Ä–æ–º –≤—Ä–µ–º–µ–Ω–∏
            import time

            start_time = time.time()

            with torch.no_grad():
                outputs = self.model(x)

            inference_time = (time.time() - start_time) * 1000  # –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö

            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ inference
            if self.device.type == "cuda":
                gpu_memory_after = torch.cuda.memory_allocated(self.device) / 1024**2  # MB
                logger.debug(f"GPU Memory after inference: {gpu_memory_after:.1f}MB allocated")
                logger.info(f"Inference time: {inference_time:.1f}ms on {self.device}")

            # –û—Ç–ª–∞–¥–∫–∞ –≤—ã—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏
            outputs_np = outputs.cpu().numpy()[0]
            logger.debug(
                f"Model outputs: min={outputs_np.min():.3f}, max={outputs_np.max():.3f}, mean={outputs_np.mean():.3f}"
            )
            logger.debug(f"Model outputs sample: {outputs_np[:10]}")

            # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            predictions = self._interpret_predictions(outputs)

            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π symbol –ø–∞—Ä–∞–º–µ—Ç—Ä –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ DataFrame
                if symbol:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π —Å–∏–º–≤–æ–ª
                    pass
                elif isinstance(input_data, pd.DataFrame) and "symbol" in input_data.columns:
                    symbol = input_data["symbol"].iloc[-1] if not input_data.empty else "UNKNOWN"
                else:
                    symbol = "UNKNOWN"

                # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ª–æ–≥–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                import asyncio

                if asyncio.iscoroutinefunction(ml_prediction_logger.log_prediction):
                    await ml_prediction_logger.log_prediction(
                        symbol=symbol,
                        features=features_scaled[-1],  # –ü–æ—Å–ª–µ–¥–Ω—è—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è —Ç–æ—á–∫–∞
                        model_outputs=outputs_np,
                        predictions=predictions,
                        market_data=input_data if isinstance(input_data, pd.DataFrame) else None,
                    )
                else:
                    # –ï—Å–ª–∏ –º–µ—Ç–æ–¥ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π, —Å–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É
                    asyncio.create_task(
                        ml_prediction_logger.log_prediction(
                            symbol=symbol,
                            features=features_scaled[-1],
                            model_outputs=outputs_np,
                            predictions=predictions,
                            market_data=(
                                input_data if isinstance(input_data, pd.DataFrame) else None
                            ),
                        )
                    )
            except Exception as log_error:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {log_error}")

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º heartbeat –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            try:
                if hasattr(self, "worker_id") and self.worker_id:
                    await worker_coordinator.heartbeat(
                        self.worker_id, status="running", active_tasks=1
                    )
            except Exception as heartbeat_error:
                logger.warning(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ heartbeat: {heartbeat_error}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Å–∏–≥–Ω–∞–ª–∞ –ø–µ—Ä–µ–¥ –≤–æ–∑–≤—Ä–∞—Ç–æ–º
            try:
                # –°–æ–∑–¥–∞–µ–º —Å–∏–≥–Ω–∞–ª –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
                signal_data = {
                    "symbol": symbol,
                    "direction": predictions.get("primary_direction", "NEUTRAL"),
                    "strategy": "ml_patchtst",
                    "timestamp": datetime.now(),
                    "signal_strength": predictions.get("primary_confidence", 0.0),
                    "price_level": predictions.get("primary_returns", {}).get("15m", 0.0),
                }

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Å–∏–≥–Ω–∞–ª–∞
                is_unique = await signal_deduplicator.check_and_register_signal(signal_data)
                if not is_unique:
                    logger.warning(f"üîÑ –î—É–±–ª–∏–∫–∞—Ç ML —Å–∏–≥–Ω–∞–ª–∞ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω –¥–ª—è {symbol}")
                    predictions["is_duplicate"] = True
                else:
                    predictions["is_duplicate"] = False

            except Exception as dedup_error:
                logger.warning(f"–û—à–∏–±–∫–∞ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ —Å–∏–≥–Ω–∞–ª–∞: {dedup_error}")
                predictions["is_duplicate"] = False

            logger.info(f"ML Manager –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {predictions}")
            return predictions

        except Exception as e:
            logger.error(f"Error making prediction: {e}")
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º heartbeat –æ–± –æ—à–∏–±–∫–µ
            try:
                if hasattr(self, "worker_id") and self.worker_id:
                    await worker_coordinator.heartbeat(self.worker_id, status="error")
            except Exception as heartbeat_error:
                logger.warning(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ heartbeat –ø—Ä–∏ –æ—à–∏–±–∫–µ: {heartbeat_error}")
            raise

    def _interpret_predictions(self, outputs: torch.Tensor) -> dict[str, Any]:
        """
        –£–õ–£–ß–®–ï–ù–ù–ê–Ø –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≤—ã—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏ —Å –∞–Ω–∞–ª–∏–∑–æ–º –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–≥–Ω–∞–ª–æ–≤.

        –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–ª–∞—Å—Å–æ–≤ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è!
        –í –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ –±—ã–ª–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ:
        - –ö–ª–∞—Å—Å 0 = LONG (–ø–æ–∫—É–ø–∫–∞, —Ä–æ—Å—Ç —Ü–µ–Ω—ã)
        - –ö–ª–∞—Å—Å 1 = SHORT (–ø—Ä–æ–¥–∞–∂–∞, –ø–∞–¥–µ–Ω–∏–µ —Ü–µ–Ω—ã)
        - –ö–ª–∞—Å—Å 2 = NEUTRAL/FLAT (–±–æ–∫–æ–≤–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ, –Ω–µ—Ç —Ç–æ—Ä–≥–æ–≤–ª–∏)

        Args:
            outputs: –¢–µ–Ω–∑–æ—Ä —Å 20 –≤—ã—Ö–æ–¥–∞–º–∏ –º–æ–¥–µ–ª–∏

        Returns:
            Dict —Å –ü–†–ê–í–ò–õ–¨–ù–û –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        """
        # –≠—Ç–∞–ø 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–∏
        outputs_np = outputs.cpu().numpy()[0]

        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤—ã—Ö–æ–¥–æ–≤ (20 –∑–Ω–∞—á–µ–Ω–∏–π):
        # 0-3: future returns (15m, 1h, 4h, 12h)
        # 4-15: direction logits (12 values = 3 classes √ó 4 timeframes)
        # 16-19: risk metrics

        future_returns = outputs_np[0:4]
        direction_logits = outputs_np[4:16]  # 12 –∑–Ω–∞—á–µ–Ω–∏–π!
        risk_metrics = outputs_np[16:20]

        # –ü–†–ê–í–ò–õ–¨–ù–ê–Ø –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø DIRECTIONS (12 –∑–Ω–∞—á–µ–Ω–∏–π = 4 —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ √ó 3 –∫–ª–∞—Å—Å–∞)
        direction_logits_reshaped = direction_logits.reshape(4, 3)  # 4 —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ √ó 3 –∫–ª–∞—Å—Å–∞

        # –ü—Ä–∏–º–µ–Ω—è–µ–º softmax –∫ –∫–∞–∂–¥–æ–º—É —Ç–∞–π–º—Ñ—Ä–µ–π–º—É
        directions = []
        direction_probs = []

        for i, logits in enumerate(direction_logits_reshaped):
            # Softmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            exp_logits = np.exp(logits - np.max(logits))  # –î–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            probs = exp_logits / exp_logits.sum()
            direction_probs.append(probs)

            # Argmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–∞ (0=LONG, 1=SHORT, 2=NEUTRAL)
            direction_class = np.argmax(probs)
            directions.append(direction_class)

        directions = np.array(directions)

        # –î–ò–ê–ì–ù–û–°–¢–ò–ß–ï–°–ö–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –í–•–û–î–ù–´–• –î–ê–ù–ù–´–•
        logger.info(
            f"""
üîç ML –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê - –í–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï –ú–û–î–ï–õ–ò:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìä Raw Model Outputs (–≤—Å–µ 20 –∑–Ω–∞—á–µ–Ω–∏–π): {outputs_np}
üìà Future Returns (0-3):
   15m: {future_returns[0]:.6f}, 1h: {future_returns[1]:.6f}
   4h: {future_returns[2]:.6f}, 12h: {future_returns[3]:.6f}
üéØ Direction Predictions: {directions} [0=LONG, 1=SHORT, 2=NEUTRAL]
‚ö° Risk Metrics: {risk_metrics}
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
"""
        )

        # –≠—Ç–∞–ø 2: –†–∞—Å—á–µ—Ç weighted_direction –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        weights = np.array([0.4, 0.3, 0.2, 0.1])
        weighted_direction = np.sum(directions * weights)

        # –≠—Ç–∞–ø 3: –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–≥–Ω–∞–ª–∞ —Å –ø–æ–º–æ—â—å—é –Ω–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        filter_result = self.quality_analyzer.analyze_signal_quality(
            directions=directions,
            direction_probs=direction_probs,
            future_returns=future_returns,
            risk_metrics=risk_metrics,
            weighted_direction=weighted_direction
        )

        # –≠—Ç–∞–ø 4: –ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        if not filter_result.passed:
            # –°–∏–≥–Ω–∞–ª –Ω–µ –ø—Ä–æ—à–µ–ª —Ñ–∏–ª—å—Ç—Ä—ã –∫–∞—á–µ—Å—Ç–≤–∞
            signal_type = "NEUTRAL"
            signal_strength = 0.25  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            combined_confidence = 0.25
            stop_loss_pct = None
            take_profit_pct = None
            
            logger.warning(
                f"üö´ –°–∏–≥–Ω–∞–ª –æ—Ç–∫–ª–æ–Ω–µ–Ω –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–º –∫–∞—á–µ—Å—Ç–≤–∞. "
                f"–ü—Ä–∏—á–∏–Ω—ã: {'; '.join(filter_result.rejection_reasons)}"
            )
        else:
            # –°–∏–≥–Ω–∞–ª –ø—Ä–æ—à–µ–ª —Ñ–∏–ª—å—Ç—Ä—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
            signal_type = filter_result.signal_type
            metrics = filter_result.quality_metrics
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            signal_strength = metrics.agreement_score
            combined_confidence = metrics.confidence_score
            
            # –†–∞—Å—á–µ—Ç SL/TP –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–≥–Ω–∞–ª–∞
            if signal_type in ["LONG", "SHORT"]:
                # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ SL/TP –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—á–µ—Å—Ç–≤–∞
                base_sl = 0.01  # 1%
                base_tp = 0.02  # 2%
                
                # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–≥–Ω–∞–ª–∞
                quality_multiplier = 0.8 + (metrics.quality_score * 0.4)  # 0.8-1.2
                
                stop_loss_pct = base_sl * quality_multiplier
                take_profit_pct = base_tp * quality_multiplier
                
                # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–∞ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
                volatility = np.std(future_returns[:2])  # –ë–ª–∏–∂–∞–π—à–∏–µ –¢–§
                if volatility > 0.01:
                    stop_loss_pct *= 1.2
                    take_profit_pct *= 1.2
                
                # –§–∏–Ω–∞–ª—å–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
                stop_loss_pct = np.clip(stop_loss_pct, 0.005, 0.025)  # 0.5% - 2.5%
                take_profit_pct = np.clip(take_profit_pct, 0.01, 0.05)   # 1% - 5%
            else:
                stop_loss_pct = None
                take_profit_pct = None

        # –≠—Ç–∞–ø 5: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        confidence_scores = np.array([np.max(probs) for probs in direction_probs])
        model_confidence = float(np.mean(confidence_scores))
        avg_risk = float(np.mean(risk_metrics))
        risk_level = "LOW" if avg_risk < 0.3 else "MEDIUM" if avg_risk < 0.7 else "HIGH"
        
        # Focal weighting –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ª–æ–≥–≥–µ—Ä–æ–º
        focal_alpha = 0.25
        focal_gamma = 2.0
        focal_weighted_confidence = focal_alpha * (1 - model_confidence) ** focal_gamma

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        quality_score = filter_result.quality_metrics.quality_score if filter_result.passed else 0.0
        strategy_used = filter_result.strategy_used.value
        
        sl_str = f"{stop_loss_pct:.3f}" if stop_loss_pct else "–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω"
        tp_str = f"{take_profit_pct:.3f}" if take_profit_pct else "–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω"

        logger.info(
            f"""
üìä ML –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï - –†–ï–ó–£–õ–¨–¢–ê–¢ –ê–ù–ê–õ–ò–ó–ê –ö–ê–ß–ï–°–¢–í–ê:
   üéØ –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {signal_type} (—Å—Ç—Ä–∞—Ç–µ–≥–∏—è: {strategy_used})
   üìà –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ –¢–§: {directions} [0=LONG, 1=SHORT, 2=NEUTRAL]
   ‚≠ê –ö–∞—á–µ—Å—Ç–≤–æ —Å–∏–≥–Ω–∞–ª–∞: {quality_score:.3f}
   üî• –°–∏–ª–∞ —Å–∏–≥–Ω–∞–ª–∞: {signal_strength:.3f}
   üé≤ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {combined_confidence:.1%}
   ‚ö†Ô∏è –†–∏—Å–∫: {risk_level} ({avg_risk:.3f})
   üìä –î–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏: 15–º={future_returns[0]:.3f}, 1—á={future_returns[1]:.3f}, 4—á={future_returns[2]:.3f}, 12—á={future_returns[3]:.3f}
   üõ°Ô∏è SL: {sl_str}, üéØ TP: {tp_str}
   ‚úÖ –ü—Ä–æ—à–µ–ª —Ñ–∏–ª—å—Ç—Ä—ã: {'–î–∞' if filter_result.passed else '–ù–µ—Ç'}
"""
        )

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        direction_map = {0: "LONG", 1: "SHORT", 2: "NEUTRAL"}

        return {
            # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–∏–≥–Ω–∞–ª–∞
            "signal_type": signal_type,
            "signal_strength": float(signal_strength),
            "confidence": float(combined_confidence),
            "signal_confidence": float(combined_confidence),  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            "success_probability": float(combined_confidence),
            
            # SL/TP –ø—Ä–æ—Ü–µ–Ω—Ç—ã
            "stop_loss_pct": stop_loss_pct,
            "take_profit_pct": take_profit_pct,
            
            # –û—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–∞
            "risk_level": risk_level,
            "risk_score": float(avg_risk),
            "max_drawdown": float(risk_metrics[0]) if len(risk_metrics) > 0 else 0,
            "max_rally": float(risk_metrics[1]) if len(risk_metrics) > 1 else 0,
            
            # –î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            "returns_15m": float(future_returns[0]),
            "returns_1h": float(future_returns[1]),
            "returns_4h": float(future_returns[2]),
            "returns_12h": float(future_returns[3]),
            
            # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º
            "direction_15m": direction_map.get(int(directions[0]), "NEUTRAL"),
            "direction_1h": direction_map.get(int(directions[1]), "NEUTRAL"),
            "direction_4h": direction_map.get(int(directions[2]), "NEUTRAL"),
            "direction_12h": direction_map.get(int(directions[3]), "NEUTRAL"),
            "confidence_15m": float(confidence_scores[0]),
            "confidence_1h": float(confidence_scores[1]),
            "confidence_4h": float(confidence_scores[2]),
            "confidence_12h": float(confidence_scores[3]),
            
            # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
            "quality_score": quality_score,
            "agreement_score": filter_result.quality_metrics.agreement_score if filter_result.passed else 0.0,
            "filter_strategy": strategy_used,
            "passed_quality_filters": filter_result.passed,
            "rejection_reasons": filter_result.rejection_reasons,
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            "primary_timeframe": "4h",  # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–π–º—Ñ—Ä–µ–π–º
            "predictions": {
                "returns_15m": float(future_returns[0]),
                "returns_1h": float(future_returns[1]),
                "returns_4h": float(future_returns[2]),
                "returns_12h": float(future_returns[3]),
                "direction_score": float(weighted_direction),
                "directions_by_timeframe": directions.tolist(),
                "direction_probabilities": [p.tolist() for p in direction_probs],
            },
            "timestamp": datetime.now(UTC).isoformat(),
        }

    async def update_model(self, new_model_path: str):
        """
        –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é.

        Args:
            new_model_path: –ü—É—Ç—å –∫ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        """
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ä—É—é –º–æ–¥–µ–ª—å –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤–Ω—É—é
            backup_path = self.model_path.with_suffix(".pth.backup")
            if self.model_path.exists():
                self.model_path.rename(backup_path)

            # –ö–æ–ø–∏—Ä—É–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
            Path(new_model_path).rename(self.model_path)

            # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            await self._load_model()

            logger.info(f"Model updated successfully from {new_model_path}")

        except Exception as e:
            logger.error(f"Error updating model: {e}")
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—Ç–∞—Ä—É—é –º–æ–¥–µ–ª—å
            if backup_path.exists():
                backup_path.rename(self.model_path)
            raise

    def get_model_info(self) -> dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        return {
            "model_type": "UnifiedPatchTST",
            "model_path": str(self.model_path),
            "context_length": self.context_length,
            "num_features": self.num_features,
            "num_targets": self.num_targets,
            "device": str(self.device),
            "model_loaded": self.model is not None,
            "scaler_loaded": self.scaler is not None,
        }

    def switch_filtering_strategy(self, strategy: str) -> bool:
        """
        –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å–∏–≥–Ω–∞–ª–æ–≤
        
        Args:
            strategy: –ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ (conservative/moderate/aggressive)
            
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ
        """
        if self.quality_analyzer.switch_strategy(strategy):
            logger.info(f"‚úÖ –°—Ç—Ä–∞—Ç–µ–≥–∏—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∞ –Ω–∞: {strategy}")
            return True
        else:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –Ω–∞: {strategy}")
            return False

    def get_filtering_statistics(self) -> dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        return self.quality_analyzer.get_strategy_statistics()

    def get_available_strategies(self) -> list[str]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
        """
        return ["conservative", "moderate", "aggressive"]

    def get_current_strategy_config(self) -> dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        
        Returns:
            –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–∫—Ç–∏–≤–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        """
        return {
            "active_strategy": self.quality_analyzer.active_strategy.value,
            "strategy_params": self.quality_analyzer.strategy_params,
            "timeframe_weights": self.quality_analyzer.timeframe_weights.tolist(),
            "quality_weights": self.quality_analyzer.quality_weights,
        }
