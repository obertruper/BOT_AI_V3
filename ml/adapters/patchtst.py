#!/usr/bin/env python3
"""
–ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è PatchTST –º–æ–¥–µ–ª–∏.
–ò–Ω–∫–∞–ø—Å—É–ª–∏—Ä—É–µ—Ç –ª–æ–≥–∏–∫—É –∑–∞–≥—Ä—É–∑–∫–∏, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –¥–ª—è PatchTST –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã.
"""

import os
import pickle
from datetime import UTC, datetime
from pathlib import Path
from typing import Any, Optional, Union

import numpy as np
import pandas as pd
import torch

from core.logger import setup_logger
from ml.adapters.base import (
    BaseModelAdapter,
    RiskLevel,
    RiskMetrics,
    SignalDirection,
    TimeframePrediction,
    UnifiedPrediction,
)
from ml.logic.feature_engineering_production import (
    ProductionFeatureEngineer as FeatureEngineer,
)
from ml.logic.patchtst_model import create_unified_model
from ml.logic.signal_quality_analyzer import SignalQualityAnalyzer

logger = setup_logger(__name__)


class PatchTSTAdapter(BaseModelAdapter):
    """
    –ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è UnifiedPatchTST –º–æ–¥–µ–ª–∏.
    –ü–µ—Ä–µ–Ω–æ—Å –ª–æ–≥–∏–∫–∏ –∏–∑ MLManager —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ–ª–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏.
    """
    
    def __init__(self, config: dict[str, Any]):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è PatchTST –∞–¥–∞–ø—Ç–µ—Ä–∞.
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        """
        super().__init__(config)
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è PatchTST –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.context_length = 96  # 24 —á–∞—Å–∞ –ø—Ä–∏ 15-–º–∏–Ω—É—Ç–Ω—ã—Ö —Å–≤–µ—á–∞—Ö
        self.num_features = 240  # –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ 240 –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
        self.num_targets = 20  # –ú–æ–¥–µ–ª—å –≤—ã–¥–∞–µ—Ç 20 –≤—ã—Ö–æ–¥–æ–≤
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.scaler = None
        self.feature_engineer = None
        self.quality_analyzer = None
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.use_torch_compile = not os.environ.get("TORCH_COMPILE_DISABLE", "").lower() in ("1", "true")
        
        logger.info(f"PatchTSTAdapter initialized with context_length={self.context_length}, "
                   f"num_features={self.num_features}, device={self.device}")
    
    async def load(self) -> None:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å PatchTST –∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            await self._load_model()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º scaler
            await self._load_scaler()
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º feature engineer
            self.feature_engineer = FeatureEngineer(self.config)
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞
            self.quality_analyzer = SignalQualityAnalyzer(self.config)
            
            logger.info("‚úÖ PatchTST components loaded successfully")
            
        except Exception as e:
            logger.error(f"Error loading PatchTST components: {e}")
            raise
    
    async def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å PatchTST"""
        try:
            if not self.model_path.exists():
                raise FileNotFoundError(f"Model file not found: {self.model_path}")
            
            # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –º–æ–¥–µ–ª–∏
            model_config = {
                "model": {
                    "input_size": self.num_features,
                    "output_size": self.num_targets,
                    "context_window": self.context_length,
                    "patch_len": 16,
                    "stride": 8,
                    "d_model": 256,
                    "n_heads": 4,
                    "e_layers": 3,
                    "d_ff": 512,
                    "dropout": 0.1,
                    "temperature_scaling": True,
                    "temperature": 2.0,
                }
            }
            self.model = create_unified_model(model_config)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
            checkpoint = torch.load(self.model_path, map_location=self.device)
            self.model.load_state_dict(checkpoint["model_state_dict"])
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            self.model.to(self.device)
            self.model.eval()
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º torch.compile –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
            if self.use_torch_compile:
                try:
                    logger.info("üöÄ Applying torch.compile optimization...")
                    self.model = torch.compile(
                        self.model,
                        mode="max-autotune",
                        fullgraph=False,
                        dynamic=False,
                    )
                    
                    # Warm-up
                    with torch.no_grad():
                        warmup_input = torch.randn(1, self.context_length, self.num_features).to(self.device)
                        _ = self.model(warmup_input)
                    
                    logger.info("‚úÖ torch.compile optimization applied successfully")
                    
                except Exception as e:
                    logger.warning(f"Could not apply torch.compile: {e}")
            
            logger.info(f"Model loaded from {self.model_path}")
            
        except Exception as e:
            logger.error(f"Error loading model: {e}")
            raise
    
    async def _load_scaler(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç scaler –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            if not self.scaler_path.exists():
                raise FileNotFoundError(f"Scaler file not found: {self.scaler_path}")
            
            with open(self.scaler_path, "rb") as f:
                self.scaler = pickle.load(f)
            
            logger.info(f"Scaler loaded from {self.scaler_path}")
            
        except Exception as e:
            logger.error(f"Error loading scaler: {e}")
            raise
    
    async def predict(self, data: Union[np.ndarray, pd.DataFrame], **kwargs) -> np.ndarray:
        """
        –î–µ–ª–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            data: –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (numpy array —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏–ª–∏ DataFrame —Å OHLCV)
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            –°—ã—Ä—ã–µ –≤—ã—Ö–æ–¥—ã –º–æ–¥–µ–ª–∏ (20 –∑–Ω–∞—á–µ–Ω–∏–π)
        """
        if not self._initialized:
            raise ValueError("Adapter not initialized. Call initialize() first.")
        
        if not self.validate_input(data):
            raise ValueError("Invalid input data")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if isinstance(data, np.ndarray):
            # –£–∂–µ –≥–æ—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = self._prepare_features_from_array(data)
        else:
            # DataFrame —Å OHLCV - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = self._prepare_features_from_dataframe(data)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        features_scaled = self.scaler.transform(features)
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è zero variance features
        features_scaled = self._handle_zero_variance(features_scaled)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä
        x = torch.FloatTensor(features_scaled).unsqueeze(0).to(self.device)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        with torch.no_grad():
            outputs = self.model(x)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º numpy array
        return outputs.cpu().numpy()[0]
    
    def _prepare_features_from_array(self, data: np.ndarray) -> np.ndarray:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ numpy array"""
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º 3D –º–∞—Å—Å–∏–≤—ã
        if data.ndim == 3:
            if data.shape[0] == 1:
                data = data.squeeze(0)
            else:
                raise ValueError(f"Expected single batch, got batch_size={data.shape[0]}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        if data.shape[0] != self.context_length or data.shape[1] != self.num_features:
            raise ValueError(
                f"Expected shape ({self.context_length}, {self.num_features}), got {data.shape}"
            )
        
        return data
    
    def _prepare_features_from_dataframe(self, df: pd.DataFrame) -> np.ndarray:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ DataFrame —Å OHLCV"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
        if len(df) < self.context_length:
            raise ValueError(f"Need at least {self.context_length} candles, got {len(df)}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º symbol –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
        if "symbol" not in df.columns:
            df = df.copy()
            df["symbol"] = "UNKNOWN"
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        features_result = self.feature_engineer.create_features(df)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if isinstance(features_result, pd.DataFrame):
            numeric_cols = features_result.select_dtypes(include=[np.number]).columns
            feature_cols = [
                col for col in numeric_cols
                if not col.startswith(("future_", "direction_", "profit_"))
                and col not in ["id", "timestamp", "datetime", "symbol"]
            ]
            features_array = features_result[feature_cols].values
            
            # –ü–æ–¥–≥–æ–Ω—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if features_array.shape[1] != self.num_features:
                if features_array.shape[1] > self.num_features:
                    features_array = features_array[:, :self.num_features]
                else:
                    padding = np.zeros((features_array.shape[0], self.num_features - features_array.shape[1]))
                    features_array = np.hstack([features_array, padding])
        else:
            features_array = features_result
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ context_length —Å—Ç—Ä–æ–∫
        if len(features_array) >= self.context_length:
            features = features_array[-self.context_length:]
        else:
            # Padding –µ—Å–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
            padding_size = self.context_length - len(features_array)
            padding = np.zeros((padding_size, features_array.shape[1]))
            features = np.vstack([padding, features_array])
        
        return features
    
    def _handle_zero_variance(self, features: np.ndarray) -> np.ndarray:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π"""
        feature_stds = features.std(axis=0)
        zero_variance_mask = feature_stds < 1e-6
        zero_variance_count = zero_variance_mask.sum()
        
        if zero_variance_count > 0:
            logger.debug(f"Found {zero_variance_count} zero variance features, adding noise")
            for i, is_zero_var in enumerate(zero_variance_mask):
                if is_zero_var:
                    noise = np.random.normal(0, 1e-4, features.shape[0])
                    features[:, i] = features[:, i] + noise
        
        return features
    
    def interpret_outputs(
        self,
        raw_outputs: np.ndarray,
        symbol: Optional[str] = None,
        current_price: Optional[float] = None,
        **kwargs
    ) -> UnifiedPrediction:
        """
        –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ—Ç –≤—ã—Ö–æ–¥—ã PatchTST –º–æ–¥–µ–ª–∏ –≤ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç.
        –ü–µ—Ä–µ–Ω–æ—Å –ª–æ–≥–∏–∫–∏ –∏–∑ MLManager._interpret_predictions.
        
        Args:
            raw_outputs: 20 –≤—ã—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏
            symbol: –¢–æ—Ä–≥–æ–≤—ã–π —Å–∏–º–≤–æ–ª
            current_price: –¢–µ–∫—É—â–∞—è —Ü–µ–Ω–∞
            
        Returns:
            UnifiedPrediction —Å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        """
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤—ã—Ö–æ–¥–æ–≤:
        # 0-3: future returns (15m, 1h, 4h, 12h)
        # 4-15: direction logits (12 values = 3 classes √ó 4 timeframes)
        # 16-19: risk metrics
        
        future_returns = raw_outputs[0:4]
        direction_logits = raw_outputs[4:16]
        risk_metrics_raw = raw_outputs[16:20]
        
        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        direction_logits_reshaped = direction_logits.reshape(4, 3)  # 4 —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ √ó 3 –∫–ª–∞—Å—Å–∞
        
        timeframes = ["15m", "1h", "4h", "12h"]
        timeframe_predictions = {}
        directions = []
        direction_probs = []
        
        for i, (tf, logits) in enumerate(zip(timeframes, direction_logits_reshaped)):
            # Softmax –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            exp_logits = np.exp(logits - np.max(logits))
            probs = exp_logits / exp_logits.sum()
            direction_probs.append(probs)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
            direction_class = np.argmax(probs)
            directions.append(direction_class)
            
            # –ú–∞–ø–ø–∏–Ω–≥ –∫–ª–∞—Å—Å–æ–≤
            if direction_class == 0:
                direction = SignalDirection.LONG
            elif direction_class == 1:
                direction = SignalDirection.SHORT
            else:
                direction = SignalDirection.NEUTRAL
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
            timeframe_predictions[tf] = TimeframePrediction(
                timeframe=tf,
                direction=direction,
                confidence=float(np.max(probs)),
                expected_return=float(future_returns[i]),
                direction_probabilities={
                    "LONG": float(probs[0]),
                    "SHORT": float(probs[1]),
                    "NEUTRAL": float(probs[2]),
                }
            )
        
        directions = np.array(directions)
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–≥–Ω–∞–ª–∞
        weighted_direction = np.sum(directions * np.array([0.4, 0.3, 0.2, 0.1]))
        
        filter_result = self.quality_analyzer.analyze_signal_quality(
            directions=directions,
            direction_probs=direction_probs,
            future_returns=future_returns,
            risk_metrics=risk_metrics_raw,
            weighted_direction=weighted_direction,
        )
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if not filter_result.passed:
            signal_type = "NEUTRAL"
            signal_strength = 0.25
            confidence = 0.25
            stop_loss_pct = None
            take_profit_pct = None
        else:
            signal_type = filter_result.signal_type
            metrics = filter_result.quality_metrics
            signal_strength = metrics.agreement_score
            confidence = metrics.confidence_score
            
            # –†–∞—Å—á–µ—Ç SL/TP –¥–ª—è —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
            if signal_type in ["LONG", "SHORT"]:
                base_sl = 0.01  # 1%
                base_tp = 0.02  # 2%
                
                quality_multiplier = 0.8 + (metrics.quality_score * 0.4)
                
                stop_loss_pct = base_sl * quality_multiplier
                take_profit_pct = base_tp * quality_multiplier
                
                # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–∞ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
                volatility = np.std(future_returns[:2])
                if volatility > 0.01:
                    stop_loss_pct *= 1.2
                    take_profit_pct *= 1.2
                
                # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
                stop_loss_pct = np.clip(stop_loss_pct, 0.005, 0.025)
                take_profit_pct = np.clip(take_profit_pct, 0.01, 0.05)
            else:
                stop_loss_pct = None
                take_profit_pct = None
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞
        avg_risk = float(np.mean(risk_metrics_raw))
        if avg_risk < 0.3:
            risk_level = RiskLevel.LOW
        elif avg_risk < 0.7:
            risk_level = RiskLevel.MEDIUM
        else:
            risk_level = RiskLevel.HIGH
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç —Ä–∏—Å–∫-–º–µ—Ç—Ä–∏–∫
        risk_metrics = RiskMetrics(
            max_drawdown_1h=float(risk_metrics_raw[0]),
            max_rally_1h=float(risk_metrics_raw[1]),
            max_drawdown_4h=float(risk_metrics_raw[2]),
            max_rally_4h=float(risk_metrics_raw[3]),
            avg_risk=avg_risk,
            risk_level=risk_level
        )
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
        direction_counts = {0: 0, 1: 0, 2: 0}
        for d in directions:
            direction_counts[d] += 1
        
        primary_direction_idx = max(direction_counts, key=direction_counts.get)
        primary_direction_map = {0: "LONG", 1: "SHORT", 2: "NEUTRAL"}
        primary_direction = primary_direction_map[primary_direction_idx]
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤–æ–∑–≤—Ä–∞—Ç—ã –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º
        primary_returns = {
            "15m": float(future_returns[0]),
            "1h": float(future_returns[1]),
            "4h": float(future_returns[2]),
            "12h": float(future_returns[3]),
        }
        
        # –°–æ–∑–¥–∞–µ–º —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        unified_prediction = UnifiedPrediction(
            # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            signal_type=signal_type,
            confidence=float(confidence),
            signal_strength=float(signal_strength),
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º
            timeframe_predictions=timeframe_predictions,
            primary_direction=primary_direction,
            primary_confidence=float(confidence),
            primary_returns=primary_returns,
            
            # –†–∏—Å–∫
            risk_level=risk_level.value,
            risk_metrics=risk_metrics,
            
            # SL/TP
            stop_loss_pct=stop_loss_pct,
            take_profit_pct=take_profit_pct,
            
            # –ö–∞—á–µ—Å—Ç–≤–æ
            quality_score=filter_result.quality_metrics.quality_score if filter_result.passed else 0.0,
            agreement_score=filter_result.quality_metrics.agreement_score if filter_result.passed else 0.0,
            filter_passed=filter_result.passed,
            filter_strategy=filter_result.strategy_used.value,
            rejection_reasons=filter_result.rejection_reasons,
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            model_name="UnifiedPatchTST",
            model_version="2.0",
            timestamp=datetime.now(UTC),
            
            # –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
            success_probability=float(confidence),
            
            # –°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            raw_outputs={
                "future_returns": future_returns.tolist(),
                "direction_logits": direction_logits.tolist(),
                "risk_metrics": risk_metrics_raw.tolist(),
                "weighted_direction": float(weighted_direction),
            }
        )
        
        logger.info(f"PatchTST interpretation complete: {signal_type} signal with confidence {confidence:.2f}")
        
        return unified_prediction
    
    def get_model_info(self) -> dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏"""
        return {
            "model_type": "UnifiedPatchTST",
            "model_name": self.model_name,
            "model_version": self.model_version,
            "model_path": str(self.model_path),
            "context_length": self.context_length,
            "num_features": self.num_features,
            "num_targets": self.num_targets,
            "device": str(self.device),
            "model_loaded": self.model is not None,
            "scaler_loaded": self.scaler is not None,
            "torch_compile_enabled": self.use_torch_compile,
            "initialized": self._initialized,
        }
    
    def switch_filtering_strategy(self, strategy: str) -> bool:
        """
        –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å–∏–≥–Ω–∞–ª–æ–≤.
        
        Args:
            strategy: –ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ (conservative/moderate/aggressive)
            
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ
        """
        if self.quality_analyzer and self.quality_analyzer.switch_strategy(strategy):
            logger.info(f"‚úÖ Filtering strategy switched to: {strategy}")
            return True
        else:
            logger.error(f"‚ùå Failed to switch strategy to: {strategy}")
            return False
    
    def get_filtering_statistics(self) -> dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"""
        if self.quality_analyzer:
            return self.quality_analyzer.get_strategy_statistics()
        return {}
    
    def get_available_strategies(self) -> list[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"""
        return ["conservative", "moderate", "aggressive"]
    
    def get_current_strategy_config(self) -> dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        if self.quality_analyzer:
            return {
                "active_strategy": self.quality_analyzer.active_strategy.value,
                "strategy_params": self.quality_analyzer.strategy_params,
                "timeframe_weights": self.quality_analyzer.timeframe_weights.tolist(),
                "quality_weights": self.quality_analyzer.quality_weights,
            }
        return {}