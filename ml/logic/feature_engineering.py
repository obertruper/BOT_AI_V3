"""
–ò–Ω–∂–µ–Ω–µ—Ä–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
"""

# –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
import warnings
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import ta
from sklearn.preprocessing import RobustScaler, StandardScaler
from tqdm import tqdm

warnings.filterwarnings("ignore")

from core.logger import setup_logger

# –°–æ–∑–¥–∞–µ–º –ª–æ–≥–≥–µ—Ä –¥–ª—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è
logger = setup_logger("FeatureEngineer")


class FeatureEngineer:
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è"""

    def __init__(self, config: Dict):
        self.config = config
        self.logger = logger  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π –ª–æ–≥–≥–µ—Ä –º–æ–¥—É–ª—è
        self.feature_config = config.get("features", {})
        self.scalers = {}
        self.process_position = (
            None  # –ü–æ–∑–∏—Ü–∏—è –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–≤ –ø—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ
        )
        self.disable_progress = False  # –§–ª–∞–≥ –¥–ª—è –æ—Ç–∫–ª—é—á–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–≤

    @staticmethod
    def safe_divide(
        numerator: pd.Series,
        denominator: pd.Series,
        fill_value=0.0,
        max_value=1000.0,
        min_denominator=1e-8,
    ) -> pd.Series:
        """–ò–°–ü–†–ê–í–õ–ï–ù–û: –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –¥–µ–ª–µ–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –º–∞–ª—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∑–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å
        safe_denominator = denominator.copy()

        # –ó–∞–º–µ–Ω—è–µ–º –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        mask_small = safe_denominator.abs() < min_denominator
        safe_denominator = safe_denominator.astype(
            float
        )  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ float –ø–µ—Ä–µ–¥ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–Ω–∏–µ–º
        safe_denominator[mask_small] = min_denominator

        # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–ª–µ–Ω–∏–µ
        result = numerator / safe_denominator

        # –ö–ª–∏–ø–ø–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        result = result.clip(lower=-max_value, upper=max_value)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ inf –∏ nan
        result = result.replace([np.inf, -np.inf], [fill_value, fill_value])
        result = result.fillna(fill_value)

        return result

    def calculate_vwap(self, df: pd.DataFrame) -> pd.Series:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç VWAP —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏"""
        # –ë–∞–∑–æ–≤—ã–π —Ä–∞—Å—á–µ—Ç VWAP
        vwap = self.safe_divide(df["turnover"], df["volume"], fill_value=df["close"])

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: VWAP –Ω–µ –¥–æ–ª–∂–µ–Ω —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –æ—Ç close
        # –ï—Å–ª–∏ VWAP —Å–ª–∏—à–∫–æ–º –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç close (–±–æ–ª–µ–µ —á–µ–º –≤ 2 —Ä–∞–∑–∞), –∏—Å–ø–æ–ª—å–∑—É–µ–º close
        mask_invalid = (vwap < df["close"] * 0.5) | (vwap > df["close"] * 2.0)
        vwap[mask_invalid] = df["close"][mask_invalid]

        return vwap

    def create_features(
        self,
        df: pd.DataFrame,
        train_end_date: Optional[str] = None,
        use_enhanced_features: bool = False,
    ) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π

        Args:
            df: DataFrame —Å raw –¥–∞–Ω–Ω—ã–º–∏
            train_end_date: –¥–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –¥–ª—è walk-forward –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            use_enhanced_features: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è direction prediction
        """
        if not self.disable_progress:
            self.logger.info(
                f"üöÄ –ù–∞—á–∞–ª–æ feature engineering –¥–ª—è {df['symbol'].nunique()} —Å–∏–º–≤–æ–ª–æ–≤"
            )

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        self._validate_data(df)

        featured_dfs = []
        all_symbols_data = {}  # –î–ª—è enhanced features

        # –ü–µ—Ä–≤—ã–π –ø—Ä–æ—Ö–æ–¥ - –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        for symbol in df["symbol"].unique():
            symbol_data = df[df["symbol"] == symbol].copy()
            symbol_data = symbol_data.sort_values("datetime")

            symbol_data = self._create_basic_features(symbol_data)
            symbol_data = self._create_technical_indicators(symbol_data)
            symbol_data = self._create_microstructure_features(symbol_data)
            symbol_data = self._create_rally_detection_features(symbol_data)
            symbol_data = self._create_signal_quality_features(symbol_data)
            symbol_data = self._create_futures_specific_features(symbol_data)
            symbol_data = self._create_ml_optimized_features(symbol_data)
            symbol_data = self._create_temporal_features(symbol_data)
            symbol_data = self._create_target_variables(symbol_data)

            featured_dfs.append(symbol_data)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è enhanced features
            if use_enhanced_features:
                all_symbols_data[symbol] = symbol_data.copy()

        result_df = pd.concat(featured_dfs, ignore_index=True)

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: cross-asset features –Ω—É–∂–Ω—ã –≤—Å–µ —Å–∏–º–≤–æ–ª—ã, –Ω–æ –µ—Å–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ –æ–¥–Ω–æ–º—É - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        # –ï—Å–ª–∏ –≤ df –±–æ–ª—å—à–µ –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ - —Å–æ–∑–¥–∞–µ–º cross-asset features
        if df["symbol"].nunique() > 1:
            result_df = self._create_cross_asset_features(result_df)

        # –î–æ–±–∞–≤–ª—è–µ–º enhanced features –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–æ
        if use_enhanced_features:
            result_df = self._add_enhanced_features(result_df, all_symbols_data)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ NaN –∑–Ω–∞—á–µ–Ω–∏–π
        result_df = self._handle_missing_values(result_df)

        # Walk-forward –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ –¥–∞—Ç–∞ (–∏–Ω–∞—á–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç –≤ prepare_trading_data.py)
        if train_end_date:
            result_df = self._normalize_walk_forward(result_df, train_end_date)

        self._log_feature_statistics(result_df)

        if not self.disable_progress:
            self.logger.info(
                f"‚úÖ Feature engineering –∑–∞–≤–µ—Ä—à–µ–Ω. –í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(result_df.columns)}"
            )

        return result_df

    def _validate_data(self, df: pd.DataFrame):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö"""
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–∏–ø—ã
        numeric_columns = ["open", "high", "low", "close", "volume", "turnover"]
        for col in numeric_columns:
            if col in df.columns:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø, –∑–∞–º–µ–Ω—è—è –æ—à–∏–±–∫–∏ –Ω–∞ NaN
                df[col] = pd.to_numeric(df[col], errors="coerce")
                # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                df[col] = df[col].ffill().bfill()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        if df.isnull().any().any():
            if not self.disable_progress:
                self.logger.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∞–Ω–Ω—ã—Ö")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ —Ü–µ–Ω—ã
        price_changes = df.groupby("symbol")["close"].pct_change()
        extreme_moves = abs(price_changes) > 0.15  # >15% –∑–∞ 15 –º–∏–Ω—É—Ç

        if extreme_moves.sum() > 0:
            if not self.disable_progress:
                self.logger.warning(
                    f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {extreme_moves.sum()} —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π —Ü–µ–Ω—ã"
                )

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≥—ç–ø–æ–≤ (—Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞–∑—Ä—ã–≤—ã > 2 —á–∞—Å–æ–≤)
        for symbol in df["symbol"].unique():
            symbol_data = df[df["symbol"] == symbol]
            time_diff = symbol_data["datetime"].diff()
            expected_diff = pd.Timedelta("15 minutes")
            # –°—á–∏—Ç–∞–µ–º –±–æ–ª—å—à–∏–º–∏ —Ç–æ–ª—å–∫–æ —Ä–∞–∑—Ä—ã–≤—ã –±–æ–ª—å—à–µ 2 —á–∞—Å–æ–≤ (8 –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤)
            large_gaps = time_diff > expected_diff * 8

            if large_gaps.sum() > 0:
                if not self.disable_progress:
                    self.logger.warning(
                        f"–°–∏–º–≤–æ–ª {symbol}: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ {large_gaps.sum()} –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞–∑—Ä—ã–≤–æ–≤ (> 2 —á–∞—Å–æ–≤)"
                    )

    def _create_basic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ OHLCV –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ look-ahead bias"""
        df["returns"] = np.log(df["close"] / df["close"].shift(1))

        # –î–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –∑–∞ —Ä–∞–∑–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã
        for period in [5, 10, 20]:
            df[f"returns_{period}"] = np.log(df["close"] / df["close"].shift(period))

        # –¶–µ–Ω–æ–≤—ã–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è
        df["high_low_ratio"] = df["high"] / df["low"]
        df["close_open_ratio"] = df["close"] / df["open"]

        # –ü–æ–∑–∏—Ü–∏—è –∑–∞–∫—Ä—ã—Ç–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ
        df["close_position"] = (df["close"] - df["low"]) / (
            df["high"] - df["low"] + 1e-10
        )

        # –û–±—ä–µ–º–Ω—ã–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–æ–ª—å–∫–æ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        df["volume_ratio"] = self.safe_divide(
            df["volume"],
            df["volume"].rolling(20, min_periods=20).mean(),
            fill_value=1.0,
        )
        df["turnover_ratio"] = self.safe_divide(
            df["turnover"],
            df["turnover"].rolling(20, min_periods=20).mean(),
            fill_value=1.0,
        )

        # VWAP —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ä–∞—Å—á–µ—Ç–æ–º
        df["vwap"] = self.calculate_vwap(df)

        # –ë–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π —Ä–∞—Å—á–µ—Ç close_vwap_ratio
        # –ù–æ—Ä–º–∞–ª—å–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ close/vwap –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–∫–æ–ª–æ 1.0
        # VWAP —É–∂–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω –≤ calculate_vwap()

        # –ü—Ä–æ—Å—Ç–æ–π –∏ –Ω–∞–¥–µ–∂–Ω—ã–π —Ä–∞—Å—á–µ—Ç
        df["close_vwap_ratio"] = df["close"] / df["vwap"]

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç (¬±30%)
        # –ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã –º–æ–≥—É—Ç –æ—Ç–∫–ª–æ–Ω—è—Ç—å—Å—è –æ—Ç VWAP –Ω–∞ 20-50% –≤ –ø–µ—Ä–∏–æ–¥—ã –≤—ã—Å–æ–∫–æ–π –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        df["close_vwap_ratio"] = df["close_vwap_ratio"].clip(lower=0.7, upper=1.3)

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –æ—Ç VWAP
        df["vwap_extreme_deviation"] = (
            (df["close_vwap_ratio"] < 0.85) | (df["close_vwap_ratio"] > 1.15)
        ).astype(int)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–Ω–æ–º–∞–ª–∏–∏
        # –ï—Å–ª–∏ ratio –≤—Å–µ –µ—â–µ –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ —Ä–∞–∑—É–º–Ω—ã–µ –ø—Ä–µ–¥–µ–ª—ã, –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ 1.0
        mask_invalid = (df["close_vwap_ratio"] < 0.95) | (df["close_vwap_ratio"] > 1.05)
        if mask_invalid.sum() > 0:
            self.logger.debug(
                f"–ó–∞–º–µ–Ω–µ–Ω–æ {mask_invalid.sum()} –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö close_vwap_ratio –Ω–∞ 1.0"
            )
            df.loc[mask_invalid, "close_vwap_ratio"] = 1.0

        return df

    def _create_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã"""
        tech_config = self.feature_config["technical"]

        # SMA
        sma_config = next((c for c in tech_config if c["name"] == "sma"), None)
        if sma_config:
            for period in sma_config["periods"]:
                df[f"sma_{period}"] = ta.trend.sma_indicator(df["close"], period)
                df[f"close_sma_{period}_ratio"] = df["close"] / df[f"sma_{period}"]

        # EMA
        ema_config = next((c for c in tech_config if c["name"] == "ema"), None)
        if ema_config:
            for period in ema_config["periods"]:
                df[f"ema_{period}"] = ta.trend.ema_indicator(df["close"], period)
                df[f"close_ema_{period}_ratio"] = df["close"] / df[f"ema_{period}"]

        # RSI
        rsi_config = next((c for c in tech_config if c["name"] == "rsi"), None)
        if rsi_config:
            df["rsi"] = ta.momentum.RSIIndicator(
                df["close"], window=rsi_config["period"]
            ).rsi()

            df["rsi_oversold"] = (df["rsi"] < 30).astype(int)
            df["rsi_overbought"] = (df["rsi"] > 70).astype(int)

        # MACD
        macd_config = next((c for c in tech_config if c["name"] == "macd"), None)
        if macd_config:
            macd = ta.trend.MACD(
                df["close"],
                window_slow=macd_config["slow"],
                window_fast=macd_config["fast"],
                window_sign=macd_config["signal"],
            )
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º MACD –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ü–µ–Ω—ã –¥–ª—è —Å—Ä–∞–≤–Ω–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –∞–∫—Ç–∏–≤–∞–º–∏
            # MACD –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–º –¥–ª—è –¥–æ—Ä–æ–≥–∏—Ö –∞–∫—Ç–∏–≤–æ–≤
            df["macd"] = macd.macd() / df["close"] * 100  # –í –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö –æ—Ç —Ü–µ–Ω—ã
            df["macd_signal"] = macd.macd_signal() / df["close"] * 100
            df["macd_diff"] = macd.macd_diff() / df["close"] * 100

        # Bollinger Bands
        bb_config = next(
            (c for c in tech_config if c["name"] == "bollinger_bands"), None
        )
        if bb_config:
            bb = ta.volatility.BollingerBands(
                df["close"], window=bb_config["period"], window_dev=bb_config["std_dev"]
            )
            df["bb_high"] = bb.bollinger_hband()
            df["bb_low"] = bb.bollinger_lband()
            df["bb_middle"] = bb.bollinger_mavg()
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: bb_width –∫–∞–∫ –ø—Ä–æ—Ü–µ–Ω—Ç –æ—Ç —Ü–µ–Ω—ã
            df["bb_width"] = self.safe_divide(
                df["bb_high"] - df["bb_low"],
                df["close"],
                fill_value=0.02,  # 2% –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                max_value=0.5,  # –ú–∞–∫—Å–∏–º—É–º 50% –æ—Ç —Ü–µ–Ω—ã
            )

            # –ò–°–ü–†–ê–í–õ–ï–ù–û: bb_position —Ç–µ–ø–µ—Ä—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–±—Å–æ–ª—é—Ç–Ω–æ–π —à–∏—Ä–∏–Ω—ã
            # bb_position –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —Ü–µ–Ω–∞ –≤–Ω—É—Ç—Ä–∏ –∫–∞–Ω–∞–ª–∞ Bollinger
            bb_range = df["bb_high"] - df["bb_low"]
            df["bb_position"] = self.safe_divide(
                df["close"] - df["bb_low"],
                bb_range,
                fill_value=0.5,
                max_value=2.0,  # –ü–æ–∑–≤–æ–ª—è–µ–º –≤—ã—Ö–æ–¥—ã –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ—Ä—ã–≤–æ–≤
            )

            # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –ø—Ä–æ—Ä—ã–≤–æ–≤ –ü–ï–†–ï–î –∫–ª–∏–ø–ø–∏–Ω–≥–æ–º
            df["bb_breakout_upper"] = (df["bb_position"] > 1).astype(int)
            df["bb_breakout_lower"] = (df["bb_position"] < 0).astype(int)
            df["bb_breakout_strength"] = (
                np.abs(df["bb_position"] - 0.5) * 2
            )  # –°–∏–ª–∞ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –æ—Ç —Ü–µ–Ω—Ç—Ä–∞

            # –¢–µ–ø–µ—Ä—å –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            df["bb_position"] = df["bb_position"].clip(0, 1)

        # ATR
        atr_config = next((c for c in tech_config if c["name"] == "atr"), None)
        if atr_config:
            df["atr"] = ta.volatility.AverageTrueRange(
                df["high"], df["low"], df["close"], window=atr_config["period"]
            ).average_true_range()

            # ATR –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö –æ—Ç —Ü–µ–Ω—ã —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            df["atr_pct"] = self.safe_divide(
                df["atr"],
                df["close"],
                fill_value=0.01,  # 1% –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                max_value=0.2,  # –ú–∞–∫—Å–∏–º—É–º 20% –æ—Ç —Ü–µ–Ω—ã
            )

        # Stochastic
        stoch = ta.momentum.StochasticOscillator(
            df["high"], df["low"], df["close"], window=14, smooth_window=3
        )
        df["stoch_k"] = stoch.stoch()
        df["stoch_d"] = stoch.stoch_signal()

        # ADX
        adx = ta.trend.ADXIndicator(df["high"], df["low"], df["close"])
        df["adx"] = adx.adx()
        df["adx_pos"] = adx.adx_pos()
        df["adx_neg"] = adx.adx_neg()

        # Parabolic SAR
        psar = ta.trend.PSARIndicator(df["high"], df["low"], df["close"])
        df["psar"] = psar.psar()
        # –í–º–µ—Å—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö psar_up –∏ psar_down, —Å–æ–∑–¥–∞–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        df["psar_trend"] = (df["close"] > df["psar"]).astype(float)

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ PSAR –ø–æ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        # –î–µ–ª–µ–Ω–∏–µ –Ω–∞ ATR –¥–µ–ª–∞–µ—Ç –º–µ—Ç—Ä–∏–∫—É —Å—Ä–∞–≤–Ω–∏–º–æ–π –º–µ–∂–¥—É –∞–∫—Ç–∏–≤–∞–º–∏
        df["psar_distance"] = (df["close"] - df["psar"]) / df["close"]
        if "atr" in df.columns:
            df["psar_distance_normalized"] = (df["close"] - df["psar"]) / (
                df["atr"] + 1e-10
            )
        else:
            df["psar_distance_normalized"] = df["psar_distance"]

        # ===== –ù–û–í–´–ï –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –ò–ù–î–ò–ö–ê–¢–û–†–´ (2024 best practices) =====

        # 1. Ichimoku Cloud - –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –≤ –∫—Ä–∏–ø—Ç–æ
        try:
            ichimoku = ta.trend.IchimokuIndicator(
                high=df["high"],
                low=df["low"],
                window1=9,  # Tenkan-sen
                window2=26,  # Kijun-sen
                window3=52,  # Senkou Span B
            )
            df["ichimoku_conversion"] = ichimoku.ichimoku_conversion_line()
            df["ichimoku_base"] = ichimoku.ichimoku_base_line()
            df["ichimoku_span_a"] = ichimoku.ichimoku_a()
            df["ichimoku_span_b"] = ichimoku.ichimoku_b()
            # –û–±–ª–∞–∫–æ - —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É span A –∏ B
            df["ichimoku_cloud_thickness"] = (
                df["ichimoku_span_a"] - df["ichimoku_span_b"]
            ) / df["close"]
            # –ü–æ–∑–∏—Ü–∏—è —Ü–µ–Ω—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –æ–±–ª–∞–∫–∞
            df["price_vs_cloud"] = (
                df["close"] - (df["ichimoku_span_a"] + df["ichimoku_span_b"]) / 2
            ) / df["close"]
        except:
            pass

        # 2. Keltner Channels - –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ Bollinger Bands
        try:
            keltner = ta.volatility.KeltnerChannel(
                high=df["high"],
                low=df["low"],
                close=df["close"],
                window=20,
                window_atr=10,
            )
            df["keltner_upper"] = keltner.keltner_channel_hband()
            df["keltner_middle"] = keltner.keltner_channel_mband()
            df["keltner_lower"] = keltner.keltner_channel_lband()
            df["keltner_position"] = (df["close"] - df["keltner_lower"]) / (
                df["keltner_upper"] - df["keltner_lower"]
            )
        except:
            pass

        # 3. Donchian Channels - –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–æ—Ä—ã–≤–æ–≤
        try:
            donchian = ta.volatility.DonchianChannel(
                high=df["high"], low=df["low"], close=df["close"], window=20
            )
            df["donchian_upper"] = donchian.donchian_channel_hband()
            df["donchian_middle"] = donchian.donchian_channel_mband()
            df["donchian_lower"] = donchian.donchian_channel_lband()
            # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ—Ä—ã–≤–∞
            df["donchian_breakout"] = (
                (df["close"] > df["donchian_upper"].shift(1))
                | (df["close"] < df["donchian_lower"].shift(1))
            ).astype(int)
        except:
            pass

        # 4. Volume Weighted Moving Average (VWMA)
        df["vwma_20"] = (df["close"] * df["volume"]).rolling(20).sum() / df[
            "volume"
        ].rolling(20).sum()
        df["close_vwma_ratio"] = df["close"] / df["vwma_20"]

        # 5. Money Flow Index (MFI) - –æ–±—ä–µ–º–Ω—ã–π –æ—Å—Ü–∏–ª–ª—è—Ç–æ—Ä
        try:
            mfi = ta.volume.MFIIndicator(
                high=df["high"],
                low=df["low"],
                close=df["close"],
                volume=df["volume"],
                window=14,
            )
            df["mfi"] = mfi.money_flow_index()
            df["mfi_overbought"] = (df["mfi"] > 80).astype(int)
            df["mfi_oversold"] = (df["mfi"] < 20).astype(int)
        except:
            pass

        # 6. Commodity Channel Index (CCI)
        try:
            cci = ta.trend.CCIIndicator(
                high=df["high"], low=df["low"], close=df["close"], window=20
            )
            df["cci"] = cci.cci()
            df["cci_overbought"] = (df["cci"] > 100).astype(int)
            df["cci_oversold"] = (df["cci"] < -100).astype(int)
        except:
            pass

        # 7. Williams %R
        try:
            williams = ta.momentum.WilliamsRIndicator(
                high=df["high"], low=df["low"], close=df["close"], lbp=14
            )
            df["williams_r"] = williams.williams_r()
        except:
            pass

        # 8. Ultimate Oscillator - –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–µ—Ä–∏–æ–¥–æ–≤
        try:
            ultimate = ta.momentum.UltimateOscillator(
                high=df["high"],
                low=df["low"],
                close=df["close"],
                window1=7,
                window2=14,
                window3=28,
            )
            df["ultimate_oscillator"] = ultimate.ultimate_oscillator()
        except:
            pass

        # 9. Accumulation/Distribution Index
        try:
            adl = ta.volume.AccDistIndexIndicator(
                high=df["high"], low=df["low"], close=df["close"], volume=df["volume"]
            )
            df["accumulation_distribution"] = adl.acc_dist_index()
        except:
            pass

        # 10. On Balance Volume (OBV)
        try:
            obv = ta.volume.OnBalanceVolumeIndicator(
                close=df["close"], volume=df["volume"]
            )
            df["obv"] = obv.on_balance_volume()
            # OBV trend
            df["obv_ema"] = df["obv"].ewm(span=20).mean()
            df["obv_trend"] = (df["obv"] > df["obv_ema"]).astype(int)
        except:
            pass

        # 11. Chaikin Money Flow (CMF)
        try:
            cmf = ta.volume.ChaikinMoneyFlowIndicator(
                high=df["high"],
                low=df["low"],
                close=df["close"],
                volume=df["volume"],
                window=20,
            )
            df["cmf"] = cmf.chaikin_money_flow()
        except:
            pass

        # 12. Average Directional Movement Index Rating (ADXR)
        try:
            adxr = ta.trend.ADXIndicator(
                high=df["high"], low=df["low"], close=df["close"], window=14
            )
            df["adxr"] = adxr.adx().rolling(14).mean()  # ADXR = —Å—Ä–µ–¥–Ω–µ–µ ADX
        except:
            pass

        # 13. Aroon Indicator
        try:
            aroon = ta.trend.AroonIndicator(close=df["close"], window=25)
            df["aroon_up"] = aroon.aroon_up()
            df["aroon_down"] = aroon.aroon_down()
            df["aroon_oscillator"] = df["aroon_up"] - df["aroon_down"]
        except:
            pass

        # 14. Pivot Points (–ø–æ–¥–¥–µ—Ä–∂–∫–∞/—Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ)
        df["pivot"] = (df["high"] + df["low"] + df["close"]) / 3
        df["resistance1"] = 2 * df["pivot"] - df["low"]
        df["support1"] = 2 * df["pivot"] - df["high"]
        df["resistance2"] = df["pivot"] + (df["high"] - df["low"])
        df["support2"] = df["pivot"] - (df["high"] - df["low"])

        # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —É—Ä–æ–≤–Ω–µ–π
        df["dist_to_resistance1"] = (df["resistance1"] - df["close"]) / df["close"]
        df["dist_to_support1"] = (df["close"] - df["support1"]) / df["close"]

        # 15. Rate of Change (ROC)
        try:
            roc = ta.momentum.ROCIndicator(close=df["close"], window=10)
            df["roc"] = roc.roc()
        except:
            pass

        # 16. Trix - —Ç—Ä–æ–π–Ω–æ–µ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ
        try:
            trix = ta.trend.TRIXIndicator(close=df["close"], window=15)
            df["trix"] = trix.trix()
        except:
            pass

        return df

    def _create_microstructure_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–∏–∑–Ω–∞–∫–∏ –º–∏–∫—Ä–æ—Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ä—ã–Ω–∫–∞"""
        # –°–ø—Ä–µ–¥ high-low
        df["hl_spread"] = self.safe_divide(
            df["high"] - df["low"], df["close"], fill_value=0.0
        )
        df["hl_spread_ma"] = df["hl_spread"].rolling(20).mean()

        # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏ –æ–±—ä–µ–º
        df["price_direction"] = np.sign(df["close"] - df["open"])
        df["directed_volume"] = df["volume"] * df["price_direction"]
        df["volume_imbalance"] = (
            df["directed_volume"].rolling(10).sum() / df["volume"].rolling(10).sum()
        )

        # –¶–µ–Ω–æ–≤–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ - —É–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º dollar volume –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏
        df["dollar_volume"] = df["volume"] * df["close"]
        # –ò–°–ü–†–ê–í–õ–ï–ù–û v3: –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º price_impact –¥–ª—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç
        # –≥–¥–µ dollar_volume –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç $10K –¥–æ $100M+
        # log10($10K) ‚âà 4, log10($1M) ‚âà 6, log10($100M) ‚âà 8
        # –£–º–Ω–æ–∂–∞–µ–º –Ω–∞ 100 –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∑–Ω–∞—á–∏–º—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π price_impact
        df["price_impact"] = self.safe_divide(
            df["returns"].abs() * 100,  # –£–º–Ω–æ–∂–∞–µ–º –Ω–∞ 100 –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞
            np.log10(df["dollar_volume"] + 100),  # log10 –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞
            fill_value=0.0,
            max_value=0.1,  # –õ–∏–º–∏—Ç –¥–ª—è –Ω–æ–≤–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞
        )

        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ —Å –ª–æ–≥–∞—Ä–∏—Ñ–º–æ–º –æ–±—ä–µ–º–∞
        df["price_impact_log"] = self.safe_divide(
            df["returns"].abs(),
            np.log(df["volume"] + 10),  # –£–≤–µ–ª–∏—á–µ–Ω —Å–¥–≤–∏–≥ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            fill_value=0.0,
            max_value=10.0,
        )

        # –ò–°–ü–†–ê–í–õ–ï–ù–û v3: –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É–ª—É –¥–ª—è toxicity
        # toxicity = exp(-price_impact * 20)
        # –° –Ω–æ–≤—ã–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º price_impact:
        # –ü—Ä–∏ price_impact=0.04: toxicity‚âà0.45
        # –ü—Ä–∏ price_impact=0.02: toxicity‚âà0.67
        # –ü—Ä–∏ price_impact=0.01: toxicity‚âà0.82
        df["toxicity"] = np.exp(-df["price_impact"] * 20)
        df["toxicity"] = df["toxicity"].clip(0.3, 1.0)

        # –ê–º–∏—Ö—É–¥ –Ω–µ–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å - —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞
        # –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞: |returns| / dollar_volume
        # –ù–æ –º—ã –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –Ω–∞ –º–∏–ª–ª–∏–æ–Ω –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∑–Ω–∞—á–∏–º—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        df["amihud_illiquidity"] = self.safe_divide(
            df["returns"].abs() * 1e6,  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –Ω–∞ –º–∏–ª–ª–∏–æ–Ω
            df["turnover"],
            fill_value=0.0,
            max_value=100.0,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑—É–º–Ω—ã–º –º–∞–∫—Å–∏–º—É–º–æ–º
        )
        df["amihud_ma"] = df["amihud_illiquidity"].rolling(20).mean()

        # –ö–∞–π–ª –ª—è–º–±–¥–∞ - –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: |price_change| / volume, –∞ –Ω–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ std
        df["kyle_lambda"] = self.safe_divide(
            df["returns"].abs(),
            np.log(df["volume"] + 1),
            fill_value=0.0,
            max_value=10.0,
        )

        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –≤–µ—Ä—Å–∏—è - –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–µ–π
        df["volatility_volume_ratio"] = self.safe_divide(
            df["returns"].rolling(10).std(),
            df["volume"].rolling(10).std(),
            fill_value=0.0,
            max_value=10.0,
        )

        # –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å - –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –∞–Ω–Ω—É–∞–ª–∏–∑–∞—Ü–∏—è
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –†–∞–∑–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã –∞–Ω–Ω—É–∞–ª–∏–∑–∞—Ü–∏–∏
        # –î–ª—è 15-–º–∏–Ω—É—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: 96 –ø–µ—Ä–∏–æ–¥–æ–≤ –≤ –¥–µ–Ω—å, 365 –¥–Ω–µ–π –≤ –≥–æ–¥—É
        df["realized_vol_1h"] = df["returns"].rolling(4).std() * np.sqrt(
            96
        )  # –ß–∞—Å–æ–≤–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å -> –¥–Ω–µ–≤–Ω–∞—è
        df["realized_vol_daily"] = df["returns"].rolling(96).std() * np.sqrt(
            96
        )  # –î–Ω–µ–≤–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        df["realized_vol_annual"] = df["returns"].rolling(96).std() * np.sqrt(
            96 * 365
        )  # –ì–æ–¥–æ–≤–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å

        # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –æ—Å—Ç–∞–≤–ª—è–µ–º —Å—Ç–∞—Ä–æ–µ –∏–º—è
        df["realized_vol"] = df["realized_vol_daily"]

        # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –æ–±—ä–µ–º–∞ –∫ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º log –æ–±—ä–µ–º–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞ —Å—Ä–µ–¥–Ω–∏–π –æ–±—ä–µ–º
        avg_volume = df["volume"].rolling(96).mean()
        normalized_volume = df["volume"] / (avg_volume + 1)  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –æ–±—ä–µ–º

        df["volume_volatility_ratio"] = self.safe_divide(
            normalized_volume,
            df["realized_vol"] * 100,  # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
            fill_value=1.0,
            max_value=100.0,  # –†–∞–∑—É–º–Ω—ã–π –ª–∏–º–∏—Ç
        )

        return df

    def _create_rally_detection_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–ª–ª–∏ –∏ –∫—Ä—É–ø–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π"""
        if not self.disable_progress:
            self.logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–ª–ª–∏...")
        initial_cols = len(df.columns)
        features_created = []

        # 1. –ù–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –æ–±—ä–µ–º –∑–∞ —Ä–∞–∑–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã (8 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º log-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤
        for hours in [4, 8, 12, 24]:
            periods = hours * 4  # 15-–º–∏–Ω—É—Ç–Ω—ã–µ —Å–≤–µ—á–∏
            col_cumsum = f"volume_cumsum_{hours}h"
            col_ratio = f"volume_cumsum_{hours}h_ratio"

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º log1p –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
            # log1p(x) = log(1 + x), –±–µ–∑–æ–ø–∞—Å–µ–Ω –¥–ª—è x=0
            df[col_cumsum] = np.log1p(df["volume"].rolling(periods).sum())

            # –û—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ —Å—Ä–µ–¥–Ω–µ–º—É –æ–±—ä–µ–º—É –∑–∞ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥
            avg_volume_long = df["volume"].rolling(periods * 4).mean()
            df[col_ratio] = self.safe_divide(
                df["volume"].rolling(periods).sum(),
                avg_volume_long * periods,  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞ –æ–∂–∏–¥–∞–µ–º—É—é —Å—É–º–º—É
                fill_value=1.0,
                max_value=10.0,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –≤—Å–ø–ª–µ—Å–∫–∏
            )
            features_created.extend([col_cumsum, col_ratio])

        if not self.disable_progress:
            self.logger.info(
                f"  ‚úì –ù–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –æ–±—ä–µ–º: —Å–æ–∑–¥–∞–Ω–æ {len([f for f in features_created if 'volume_cumsum' in f])} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
            )

        # 2. –ê–Ω–æ–º–∞–ª—å–Ω—ã–µ –≤—Å–ø–ª–µ—Å–∫–∏ –æ–±—ä–µ–º–∞ (3 –ø—Ä–∏–∑–Ω–∞–∫–∞)
        volume_mean = df["volume"].rolling(96).mean()  # —Å—Ä–µ–¥–Ω–∏–π –æ–±—ä–µ–º –∑–∞ 24—á
        volume_std = df["volume"].rolling(96).std()
        df["volume_zscore"] = self.safe_divide(
            df["volume"] - volume_mean,
            volume_std,
            fill_value=0.0,
            max_value=50.0,  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –í –∫—Ä–∏–ø—Ç–æ Z-score –º–æ–∂–µ—Ç –¥–æ—Å—Ç–∏–≥–∞—Ç—å 20-50
        )
        df["volume_spike"] = (df["volume_zscore"] > 3).astype(int)
        df["volume_spike_magnitude"] = df["volume_zscore"].clip(0, 10)
        features_created.extend(
            ["volume_zscore", "volume_spike", "volume_spike_magnitude"]
        )

        if not self.disable_progress:
            self.logger.info("  ‚úì –ê–Ω–æ–º–∞–ª—å–Ω—ã–µ –≤—Å–ø–ª–µ—Å–∫–∏ –æ–±—ä–µ–º–∞: —Å–æ–∑–¥–∞–Ω–æ 3 –ø—Ä–∏–∑–Ω–∞–∫–∞")

        # 3. –£—Ä–æ–≤–Ω–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏/—Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è (15 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
        # –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–∏–Ω–∏–º—É–º—ã –∏ –º–∞–∫—Å–∏–º—É–º—ã
        for window in [20, 50, 100]:  # 5—á, 12.5—á, 25—á
            df[f"local_high_{window}"] = df["high"].rolling(window).max()
            df[f"local_low_{window}"] = df["low"].rolling(window).min()
            df[f"distance_from_high_{window}"] = (
                df["close"] - df[f"local_high_{window}"]
            ) / df["close"]
            df[f"distance_from_low_{window}"] = (
                df["close"] - df[f"local_low_{window}"]
            ) / df["close"]
            df[f"position_in_range_{window}"] = self.safe_divide(
                df["close"] - df[f"local_low_{window}"],
                df[f"local_high_{window}"] - df[f"local_low_{window}"],
                fill_value=0.5,  # –°–µ—Ä–µ–¥–∏–Ω–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
                max_value=1.0,  # –ü–æ–∑–∏—Ü–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç 0 –¥–æ 1
            )
            features_created.extend(
                [
                    f"local_high_{window}",
                    f"local_low_{window}",
                    f"distance_from_high_{window}",
                    f"distance_from_low_{window}",
                    f"position_in_range_{window}",
                ]
            )

        if not self.disable_progress:
            self.logger.info("  ‚úì –£—Ä–æ–≤–Ω–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏/—Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è: —Å–æ–∑–¥–∞–Ω–æ 15 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        # 4. –°–∂–∞—Ç–∏–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ (–ø—Ä–∏–∑–Ω–∞–∫ –±—É–¥—É—â–µ–≥–æ –ø—Ä–æ—Ä—ã–≤–∞) (2 –ø—Ä–∏–∑–Ω–∞–∫–∞)
        # Bollinger Bands —É–∂–µ –µ—Å—Ç—å, –¥–æ–±–∞–≤–∏–º Keltner Channels –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        atr_multiplier = 2.0
        ema20 = df["close"].ewm(span=20, adjust=False).mean()
        kc_upper = ema20 + atr_multiplier * df["atr"]
        kc_lower = ema20 - atr_multiplier * df["atr"]
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —à–∏—Ä–∏–Ω—ã –∫–∞–Ω–∞–ª–æ–≤
        kc_width = (kc_upper - kc_lower) / df["close"]

        df["volatility_squeeze"] = (df["bb_width"] < kc_width).astype(int)
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∂–∞—Ç–∏—è —Å—á–∏—Ç–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–∏–æ–¥–æ–≤ squeeze
        squeeze_group = (
            df["volatility_squeeze"] != df["volatility_squeeze"].shift()
        ).cumsum()
        df["volatility_squeeze_duration"] = (
            df["volatility_squeeze"].groupby(squeeze_group).cumsum()
        )
        df.loc[df["volatility_squeeze"] == 0, "volatility_squeeze_duration"] = 0
        features_created.extend(["volatility_squeeze", "volatility_squeeze_duration"])

        if not self.disable_progress:
            self.logger.info("  ‚úì –°–∂–∞—Ç–∏–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏: —Å–æ–∑–¥–∞–Ω–æ 2 –ø—Ä–∏–∑–Ω–∞–∫–∞")

        # 5. –î–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ RSI/MACD —Å —Ü–µ–Ω–æ–π (4 –ø—Ä–∏–∑–Ω–∞–∫–∞)
        # RSI –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è
        price_higher = (df["close"] > df["close"].shift(14)) & (
            df["close"].shift(14) > df["close"].shift(28)
        )
        rsi_lower = (df["rsi"] < df["rsi"].shift(14)) & (
            df["rsi"].shift(14) < df["rsi"].shift(28)
        )
        df["bearish_divergence_rsi"] = (price_higher & rsi_lower).astype(int)

        price_lower = (df["close"] < df["close"].shift(14)) & (
            df["close"].shift(14) < df["close"].shift(28)
        )
        rsi_higher = (df["rsi"] > df["rsi"].shift(14)) & (
            df["rsi"].shift(14) > df["rsi"].shift(28)
        )
        df["bullish_divergence_rsi"] = (price_lower & rsi_higher).astype(int)

        # MACD –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è
        macd_lower = (df["macd"] < df["macd"].shift(14)) & (
            df["macd"].shift(14) < df["macd"].shift(28)
        )
        df["bearish_divergence_macd"] = (price_higher & macd_lower).astype(int)

        macd_higher = (df["macd"] > df["macd"].shift(14)) & (
            df["macd"].shift(14) > df["macd"].shift(28)
        )
        df["bullish_divergence_macd"] = (price_lower & macd_higher).astype(int)
        features_created.extend(
            [
                "bearish_divergence_rsi",
                "bullish_divergence_rsi",
                "bearish_divergence_macd",
                "bullish_divergence_macd",
            ]
        )

        if not self.disable_progress:
            self.logger.info("  ‚úì –î–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ RSI/MACD: —Å–æ–∑–¥–∞–Ω–æ 4 –ø—Ä–∏–∑–Ω–∞–∫–∞")

        # 6. –ü–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è/—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (4 –ø—Ä–∏–∑–Ω–∞–∫–∞)
        # On-Balance Volume (OBV)
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º log-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –º–∞—Å—à—Ç–∞–±–∞
        obv_change = df["volume"] * ((df["close"] > df["close"].shift(1)) * 2 - 1)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ —Å log-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        obv_raw = obv_change.rolling(100).sum()  # 100 –ø–µ—Ä–∏–æ–¥–æ–≤ (25 —á–∞—Å–æ–≤)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º log-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –º–∞—Å—à—Ç–∞–±–∞
        df["obv"] = np.sign(obv_raw) * np.log1p(np.abs(obv_raw))

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º OBV –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å—Ä–µ–¥–Ω–µ–≥–æ –æ–±—ä–µ–º–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –∞–∫—Ç–∏–≤–∞–º–∏
        avg_volume = df["volume"].rolling(100).mean()
        df["obv_normalized"] = self.safe_divide(
            df["obv"],
            np.log1p(avg_volume),  # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä—É–µ–º –∏ —Å—Ä–µ–¥–Ω–∏–π –æ–±—ä–µ–º
            fill_value=0.0,
            max_value=20.0,
        )

        df["obv_ema"] = df["obv"].ewm(span=20, adjust=False).mean()
        df["obv_divergence"] = df["obv"] - df["obv_ema"]

        # Chaikin Money Flow
        mfm = ((df["close"] - df["low"]) - (df["high"] - df["close"])) / (
            df["high"] - df["low"] + 1e-10
        )
        mfv = mfm * df["volume"]
        df["cmf"] = mfv.rolling(20).sum() / df["volume"].rolling(20).sum()
        features_created.extend(
            ["obv", "obv_normalized", "obv_ema", "obv_divergence", "cmf"]
        )

        if not self.disable_progress:
            self.logger.info(
                "  ‚úì –ü–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è/—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è: —Å–æ–∑–¥–∞–Ω–æ 5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
            )

        # 7. Momentum –∏ —É—Å–∫–æ—Ä–µ–Ω–∏–µ (4 –ø—Ä–∏–∑–Ω–∞–∫–∞)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º groupby –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ä–∞—Å—á–µ—Ç–∞ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        df["momentum_1h"] = df.groupby("symbol")["close"].pct_change(4) * 100  # 1 —á–∞—Å
        df["momentum_4h"] = df.groupby("symbol")["close"].pct_change(16) * 100  # 4 —á–∞—Å–∞
        df["momentum_24h"] = (
            df.groupby("symbol")["close"].pct_change(96) * 100
        )  # 24 —á–∞—Å–∞

        # –£—Å–∫–æ—Ä–µ–Ω–∏–µ (–∏–∑–º–µ–Ω–µ–Ω–∏–µ momentum)
        df["momentum_acceleration"] = df.groupby("symbol")["momentum_1h"].transform(
            lambda x: x - x.shift(4)
        )
        features_created.extend(
            ["momentum_1h", "momentum_4h", "momentum_24h", "momentum_acceleration"]
        )

        if not self.disable_progress:
            self.logger.info("  ‚úì Momentum –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã: —Å–æ–∑–¥–∞–Ω–æ 4 –ø—Ä–∏–∑–Ω–∞–∫–∞")

        # 8. –ü–∞—Ç—Ç–µ—Ä–Ω "–ø—Ä—É–∂–∏–Ω–∞" - —Å–∏–ª—å–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –ø–µ—Ä–µ–¥ –¥–≤–∏–∂–µ–Ω–∏–µ–º (1 –ø—Ä–∏–∑–Ω–∞–∫)
        df["spring_pattern"] = (
            (df["volatility_squeeze"] == 1)
            & (df["volume_spike"] == 1)
            & (
                df["atr_pct"].rolling(20).mean()
                < df["atr_pct"].rolling(100).mean() * 0.7
            )
        ).astype(int)
        features_created.append("spring_pattern")

        if not self.disable_progress:
            self.logger.info("  ‚úì –ü–∞—Ç—Ç–µ—Ä–Ω '–ø—Ä—É–∂–∏–Ω–∞': —Å–æ–∑–¥–∞–Ω 1 –ø—Ä–∏–∑–Ω–∞–∫")

        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_created = len(features_created)
        if not self.disable_progress:
            self.logger.info(
                f"‚úÖ Rally detection features: –≤—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ {total_created} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
            )
            self.logger.info(f"   –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è: {features_created}")

        return df

    def _create_signal_quality_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤"""
        if not self.disable_progress:
            self.logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–≥–Ω–∞–ª–æ–≤...")
        initial_cols = len(df.columns)
        features_created = []

        # 1. –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–∏–≥–Ω–∞–ª—ã –æ—Ç —Ä–∞–∑–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        indicators_long = []
        indicators_short = []
        indicators_used = []

        # RSI
        if "rsi" in df.columns:
            indicators_long.append((df["rsi"] < 30).astype(int))
            indicators_short.append((df["rsi"] > 70).astype(int))
            indicators_used.append("RSI")

        # MACD
        if "macd_diff" in df.columns:
            indicators_long.append((df["macd_diff"] > 0).astype(int))
            indicators_short.append((df["macd_diff"] < 0).astype(int))
            indicators_used.append("MACD")

        # Bollinger Bands
        if "bb_position" in df.columns:
            indicators_long.append((df["bb_position"] < 0.2).astype(int))
            indicators_short.append((df["bb_position"] > 0.8).astype(int))
            indicators_used.append("Bollinger Bands")

        # Stochastic
        if "stoch_k" in df.columns:
            indicators_long.append((df["stoch_k"] < 20).astype(int))
            indicators_short.append((df["stoch_k"] > 80).astype(int))
            indicators_used.append("Stochastic")

        # ADX (—Å–∏–ª–∞ —Ç—Ä–µ–Ω–¥–∞)
        if "adx" in df.columns:
            strong_trend = (df["adx"] > 25).astype(int)
            indicators_long.append(strong_trend & (df["adx_pos"] > df["adx_neg"]))
            indicators_short.append(strong_trend & (df["adx_neg"] > df["adx_pos"]))
            indicators_used.append("ADX")

        # Moving averages
        if "close_sma_20_ratio" in df.columns and "close_sma_50_ratio" in df.columns:
            indicators_long.append(
                (df["close_sma_20_ratio"] > df["close_sma_50_ratio"]).astype(int)
            )
            indicators_short.append(
                (df["close_sma_20_ratio"] < df["close_sma_50_ratio"]).astype(int)
            )
            indicators_used.append("Moving Averages")

        # –°—á–∏—Ç–∞–µ–º —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
        if indicators_long:
            df["indicators_consensus_long"] = sum(indicators_long) / len(
                indicators_long
            )
            df["indicators_count_long"] = sum(indicators_long)
        else:
            df["indicators_consensus_long"] = 0
            df["indicators_count_long"] = 0

        if indicators_short:
            df["indicators_consensus_short"] = sum(indicators_short) / len(
                indicators_short
            )
            df["indicators_count_short"] = sum(indicators_short)
        else:
            df["indicators_consensus_short"] = 0
            df["indicators_count_short"] = 0

        features_created.extend(
            [
                "indicators_consensus_long",
                "indicators_count_long",
                "indicators_consensus_short",
                "indicators_count_short",
            ]
        )

        if not self.disable_progress:
            self.logger.info("  ‚úì –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤: —Å–æ–∑–¥–∞–Ω–æ 4 –ø—Ä–∏–∑–Ω–∞–∫–∞")
            self.logger.info(
                f"    –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã: {', '.join(indicators_used)}"
            )

        # 2. –°–∏–ª–∞ —Ç—Ä–µ–Ω–¥–∞ –Ω–∞ —Å—Ç–∞—Ä—à–∏—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞—Ö (4 –ø—Ä–∏–∑–Ω–∞–∫–∞)
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç—Ä–µ–Ω–¥—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ü–µ–Ω—ã –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        # –≠–º—É–ª–∏—Ä—É–µ–º 1-—á–∞—Å–æ–≤–æ–π —Ç–∞–π–º—Ñ—Ä–µ–π–º (4 —Å–≤–µ—á–∏ –ø–æ 15 –º–∏–Ω)
        ma_1h = df["close"].rolling(4).mean()
        ma_1h_prev = ma_1h.shift(4)
        df["trend_1h"] = (
            self.safe_divide(
                ma_1h - ma_1h_prev,
                ma_1h_prev,
                fill_value=0.0,
                max_value=0.1,  # –ú–∞–∫—Å–∏–º—É–º 10% –∏–∑–º–µ–Ω–µ–Ω–∏–µ
            )
            * 100
        )  # –í –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö

        df["trend_1h_strength"] = self.safe_divide(
            df["trend_1h"],
            df["atr_pct"].rolling(4).mean() * 100,  # ATR —É–∂–µ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
            fill_value=0.0,
            max_value=10.0,
        )

        # –≠–º—É–ª–∏—Ä—É–µ–º 4-—á–∞—Å–æ–≤–æ–π —Ç–∞–π–º—Ñ—Ä–µ–π–º (16 —Å–≤–µ—á–µ–π)
        ma_4h = df["close"].rolling(16).mean()
        ma_4h_prev = ma_4h.shift(16)
        df["trend_4h"] = (
            self.safe_divide(
                ma_4h - ma_4h_prev,
                ma_4h_prev,
                fill_value=0.0,
                max_value=0.2,  # –ú–∞–∫—Å–∏–º—É–º 20% –∏–∑–º–µ–Ω–µ–Ω–∏–µ
            )
            * 100
        )  # –í –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö

        df["trend_4h_strength"] = self.safe_divide(
            df["trend_4h"],
            df["atr_pct"].rolling(16).mean() * 100,  # ATR —É–∂–µ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
            fill_value=0.0,
            max_value=10.0,
        )

        features_created.extend(
            ["trend_1h", "trend_1h_strength", "trend_4h", "trend_4h_strength"]
        )

        if not self.disable_progress:
            self.logger.info("  ‚úì –°–∏–ª–∞ —Ç—Ä–µ–Ω–¥–∞ –Ω–∞ —Å—Ç–∞—Ä—à–∏—Ö –¢–§: —Å–æ–∑–¥–∞–Ω–æ 4 –ø—Ä–∏–∑–Ω–∞–∫–∞")

        # 3. –ü–æ–∑–∏—Ü–∏—è –≤ –¥–Ω–µ–≤–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ (7 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
        # –î–Ω–µ–≤–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω (96 —Å–≤–µ—á–µ–π = 24 —á–∞—Å–∞)
        df["daily_high"] = df["high"].rolling(96).max()
        df["daily_low"] = df["low"].rolling(96).min()
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: daily_range –∫–∞–∫ –ø—Ä–æ—Ü–µ–Ω—Ç –æ—Ç —Ü–µ–Ω—ã
        df["daily_range"] = self.safe_divide(
            df["daily_high"] - df["daily_low"],
            df["close"],
            fill_value=0.02,  # 2% –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            max_value=0.5,  # –ú–∞–∫—Å–∏–º—É–º 50% –æ—Ç —Ü–µ–Ω—ã
        )
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç –ø–æ–∑–∏—Ü–∏–∏ –≤ –¥–Ω–µ–≤–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ
        # daily_range —É–∂–µ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö, –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ —Ü–µ–Ω—ã
        daily_range_abs = df["daily_high"] - df["daily_low"]
        df["position_in_daily_range"] = self.safe_divide(
            df["close"] - df["daily_low"],
            daily_range_abs,
            fill_value=0.5,
            max_value=1.0,
        )

        # –ë–ª–∏–∑–æ—Å—Ç—å –∫ —ç–∫—Å—Ç—Ä–µ–º—É–º–∞–º
        df["near_daily_high"] = (df["position_in_daily_range"] > 0.9).astype(int)
        df["near_daily_low"] = (df["position_in_daily_range"] < 0.1).astype(int)
        features_created.extend(
            [
                "daily_high",
                "daily_low",
                "daily_range",
                "position_in_daily_range",
                "near_daily_high",
                "near_daily_low",
            ]
        )

        if not self.disable_progress:
            self.logger.info("  ‚úì –ü–æ–∑–∏—Ü–∏—è –≤ –¥–Ω–µ–≤–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ: —Å–æ–∑–¥–∞–Ω–æ 6 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        # 4. –ö–∞—á–µ—Å—Ç–≤–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ä—ã–Ω–∫–∞ (2 –ø—Ä–∏–∑–Ω–∞–∫–∞)
        # Higher highs –∏ higher lows –¥–ª—è uptrend
        hh = (df["high"] > df["high"].shift(4)) & (
            df["high"].shift(4) > df["high"].shift(8)
        )
        hl = (df["low"] > df["low"].shift(4)) & (
            df["low"].shift(4) > df["low"].shift(8)
        )
        df["uptrend_structure"] = (hh & hl).astype(int)

        # Lower highs –∏ lower lows –¥–ª—è downtrend
        lh = (df["high"] < df["high"].shift(4)) & (
            df["high"].shift(4) < df["high"].shift(8)
        )
        ll = (df["low"] < df["low"].shift(4)) & (
            df["low"].shift(4) < df["low"].shift(8)
        )
        df["downtrend_structure"] = (lh & ll).astype(int)
        features_created.extend(["uptrend_structure", "downtrend_structure"])

        if not self.disable_progress:
            self.logger.info("  ‚úì –ö–∞—á–µ—Å—Ç–≤–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ä—ã–Ω–∫–∞: —Å–æ–∑–¥–∞–Ω–æ 2 –ø—Ä–∏–∑–Ω–∞–∫–∞")

        # 5. –†–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π (1 –ø—Ä–∏–∑–Ω–∞–∫)
        # –ê–Ω–æ–º–∞–ª—å–Ω—ã–π –æ–±—ä–µ–º —á–∞—Å—Ç–æ —Å–≤—è–∑–∞–Ω —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
        df["news_risk"] = (df["volume_spike"] == 1).astype(int)
        features_created.append("news_risk")

        if not self.disable_progress:
            self.logger.info("  ‚úì –†–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π: —Å–æ–∑–¥–∞–Ω 1 –ø—Ä–∏–∑–Ω–∞–∫")

        # 6. –û—Ü–µ–Ω–∫–∞ –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç–∏ (3 –ø—Ä–∏–∑–Ω–∞–∫–∞)
        # –°—Ä–µ–¥–Ω–∏–π —Å–ø—Ä–µ–¥ –∏ –æ–±—ä–µ–º –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º high-low —Å–ø—Ä–µ–¥ –≤–º–µ—Å—Ç–æ bid-ask
        df["hl_spread"] = (df["high"] - df["low"]) / df["close"]

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–∞—Å—á–µ—Ç liquidity_score —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏
        hl_spread_mean = df["hl_spread"].rolling(4).mean()
        volume_mean = df["volume"].rolling(4).mean()

        # –ö–ª–∏–ø–ø–∏–Ω–≥ hl_spread –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –æ—á–µ–Ω—å –º–∞–ª—ã–µ —á–∏—Å–ª–∞
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å–ø—Ä–µ–¥ 0.01% (0.0001) –¥–ª—è —Å—Ç–µ–π–±–ª–∫–æ–∏–Ω–æ–≤
        hl_spread_clipped = np.clip(hl_spread_mean, 0.0001, 1.0)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º log-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –º–∞—Å—à—Ç–∞–±–∞
        # liquidity_score —Ç–µ–ø–µ—Ä—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –ø—Ä–∏–º–µ—Ä–Ω–æ [0, 20]
        df["liquidity_score"] = np.log1p(volume_mean / (hl_spread_clipped * 1000))

        # –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç–∏
        df["liquidity_rank"] = df.groupby("datetime")["liquidity_score"].rank(pct=True)
        features_created.extend(["hl_spread", "liquidity_score", "liquidity_rank"])

        if not self.disable_progress:
            self.logger.info("  ‚úì –û—Ü–µ–Ω–∫–∞ –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç–∏: —Å–æ–∑–¥–∞–Ω–æ 3 –ø—Ä–∏–∑–Ω–∞–∫–∞")

        # 6. Signal Strength - –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏–ª–∞ —Å–∏–≥–Ω–∞–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        # –ë–ï–ó –£–¢–ï–ß–ï–ö: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã

        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã signal_strength:
        # 1. –°–∏–ª–∞ —Ç—Ä–µ–Ω–¥–∞ (ADX)
        trend_strength = (
            df["adx"] / 100 if "adx" in df.columns else pd.Series(0.5, index=df.index)
        )

        # 2. Momentum (RSI –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç 50)
        momentum_strength = (
            np.abs(df["rsi"] - 50) / 50
            if "rsi" in df.columns
            else pd.Series(0.5, index=df.index)
        )

        # 3. –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è)
        if "volatility_20" in df.columns:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ, –ù–ï –±—É–¥—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ
            vol_mean_hist = df.groupby("symbol")["volatility_20"].transform(
                lambda x: x.rolling(100, min_periods=20).mean()
            )
            vol_strength = df["volatility_20"] / (vol_mean_hist + 1e-6)
            vol_strength = np.clip(vol_strength, 0, 2) / 2
        else:
            vol_strength = pd.Series(0.5, index=df.index)

        # 4. Volume (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π)
        if "volume" in df.columns:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ, –ù–ï –±—É–¥—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ
            vol_mean_hist = df.groupby("symbol")["volume"].transform(
                lambda x: x.rolling(100, min_periods=20).mean()
            )
            volume_strength = df["volume"] / (vol_mean_hist + 1e-6)
            volume_strength = np.clip(volume_strength, 0, 2) / 2
        else:
            volume_strength = pd.Series(0.5, index=df.index)

        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏–ª–∞ —Å–∏–≥–Ω–∞–ª–∞ (–ø—Ä–∏–∑–Ω–∞–∫, –Ω–µ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)
        df["signal_strength"] = (
            0.3 * trend_strength
            + 0.3 * momentum_strength
            + 0.2 * vol_strength
            + 0.2 * volume_strength
        )
        df["signal_strength"] = np.clip(df["signal_strength"], 0, 1)
        features_created.append("signal_strength")

        if not self.disable_progress:
            self.logger.info("  ‚úì Signal strength: —Å–æ–∑–¥–∞–Ω 1 –ø—Ä–∏–∑–Ω–∞–∫ (–±–µ–∑ —É—Ç–µ—á–µ–∫)")

        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_created = len(features_created)
        if not self.disable_progress:
            self.logger.info(
                f"‚úÖ Signal quality features: –≤—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ {total_created} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
            )

        return df

    def _create_futures_specific_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ —Ñ—å—é—á–µ—Ä—Å–∞–º–∏ —Å –ø–ª–µ—á–æ–º"""
        if not self.disable_progress:
            self.logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ñ—å—é—á–µ—Ä—Å–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏...")
        initial_cols = len(df.columns)
        features_created = []

        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –ø–ª–µ—á–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        # –ë–∞–∑–æ–≤–æ–µ –ø–ª–µ—á–æ 5x, –Ω–æ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ ATR
        base_leverage = 5

        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –ø–ª–µ—á–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        # –ß–µ–º –≤—ã—à–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å, —Ç–µ–º –º–µ–Ω—å—à–µ –ø–ª–µ—á–æ
        # ATR –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö —É–∂–µ –µ—Å—Ç—å –≤ df['atr_pct']
        volatility_factor = (
            df["atr_pct"].rolling(24).mean()
        )  # –°—Ä–µ–¥–Ω—è—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –∑–∞ 6 —á–∞—Å–æ–≤

        # –ü–ª–µ—á–æ –æ—Ç 3x –¥–æ 10x –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        # –ü—Ä–∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ 0.5% -> leverage = 10
        # –ü—Ä–∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ 2% -> leverage = 3
        dynamic_leverage = base_leverage * (0.01 / (volatility_factor + 0.001))
        dynamic_leverage = dynamic_leverage.clip(3, 10)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω

        # 1. –†–∞—Å—á–µ—Ç –ª–∏–∫–≤–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π —Ü–µ–Ω—ã
        # –î–ª—è LONG: Liq Price = Entry Price * (1 - 1/leverage + fees)
        # –î–ª—è SHORT: Liq Price = Entry Price * (1 + 1/leverage - fees)
        maintenance_margin = 0.5 / 100  # 0.5% –¥–ª—è Bybit

        df["long_liquidation_price"] = df["close"] * (
            1 - 1 / dynamic_leverage + maintenance_margin
        )
        df["short_liquidation_price"] = df["close"] * (
            1 + 1 / dynamic_leverage - maintenance_margin
        )

        # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –ª–∏–∫–≤–∏–¥–∞—Ü–∏–∏ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
        df["long_liquidation_distance_pct"] = (
            (df["close"] - df["long_liquidation_price"]) / df["close"]
        ) * 100
        df["short_liquidation_distance_pct"] = (
            (df["short_liquidation_price"] - df["close"]) / df["close"]
        ) * 100

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–µ–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –ø–ª–µ—á–æ
        df["current_leverage"] = dynamic_leverage
        features_created.extend(
            [
                "long_liquidation_price",
                "short_liquidation_price",
                "long_liquidation_distance_pct",
                "short_liquidation_distance_pct",
                "current_leverage",
            ]
        )

        if not self.disable_progress:
            self.logger.info("  ‚úì –õ–∏–∫–≤–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ü–µ–Ω—ã: —Å–æ–∑–¥–∞–Ω–æ 4 –ø—Ä–∏–∑–Ω–∞–∫–∞")

        # 2. –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–∞—Å–∞–Ω–∏—è –ª–∏–∫–≤–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π —Ü–µ–Ω—ã
        # –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–π –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
        max_drawdown_24h = df["low"].rolling(96).min() / df["close"].shift(96) - 1
        max_rally_24h = df["high"].rolling(96).max() / df["close"].shift(96) - 1

        # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–≤–∏–∂–µ–Ω–∏–π
        df["long_liquidation_risk"] = (
            (abs(max_drawdown_24h) > df["long_liquidation_distance_pct"] / 100)
            .rolling(96)
            .mean()
        )
        df["short_liquidation_risk"] = (
            (max_rally_24h > df["short_liquidation_distance_pct"] / 100)
            .rolling(96)
            .mean()
        )
        features_created.extend(["long_liquidation_risk", "short_liquidation_risk"])

        if not self.disable_progress:
            self.logger.info("  ‚úì –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ª–∏–∫–≤–∏–¥–∞—Ü–∏–∏: —Å–æ–∑–¥–∞–Ω–æ 2 –ø—Ä–∏–∑–Ω–∞–∫–∞")

        # 3. –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –ø–ª–µ—á–æ –¥–ª—è —Ç–µ–∫—É—â–µ–π –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        # –ü—Ä–∞–≤–∏–ª–æ: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –ø–ª–µ—á–æ = 20% / (–¥–Ω–µ–≤–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å)
        daily_volatility = df["returns"].rolling(96).std() * np.sqrt(
            96
        )  # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –¥–Ω–µ–≤–Ω–æ–π
        df["optimal_leverage"] = (0.2 / (daily_volatility + 0.01)).clip(
            1, 10
        )  # –û—Ç 1x –¥–æ 10x

        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–ª–µ—á–æ (–∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–µ)
        df["safe_leverage"] = (0.1 / (daily_volatility + 0.01)).clip(
            1, 5
        )  # –û—Ç 1x –¥–æ 5x
        features_created.extend(["optimal_leverage", "safe_leverage"])

        if not self.disable_progress:
            self.logger.info("  ‚úì –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –ø–ª–µ—á–æ: —Å–æ–∑–¥–∞–Ω–æ 2 –ø—Ä–∏–∑–Ω–∞–∫–∞")

        # 4. –†–∏—Å–∫ –∫–∞—Å–∫–∞–¥–Ω—ã—Ö –ª–∏–∫–≤–∏–¥–∞—Ü–∏–π
        # –ö–æ–≥–¥–∞ –º–Ω–æ–≥–æ –ø–æ–∑–∏—Ü–∏–π –º–æ–≥—É—Ç –±—ã—Ç—å –ª–∏–∫–≤–∏–¥–∏—Ä–æ–≤–∞–Ω—ã –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
        # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä: —Ä–µ–∑–∫–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è + –≤—ã—Å–æ–∫–∏–π –æ–±—ä–µ–º
        df["cascade_risk"] = (
            (df["volume_spike"] == 1)
            & (abs(df["returns"]) > df["returns"].rolling(96).std() * 2)
        ).astype(int)
        features_created.append("cascade_risk")

        if not self.disable_progress:
            self.logger.info("  ‚úì –†–∏—Å–∫ –∫–∞—Å–∫–∞–¥–Ω—ã—Ö –ª–∏–∫–≤–∏–¥–∞—Ü–∏–π: —Å–æ–∑–¥–∞–Ω 1 –ø—Ä–∏–∑–Ω–∞–∫")

        # 5. Funding rate –≤–ª–∏—è–Ω–∏–µ (–¥–ª—è —É–¥–µ—Ä–∂–∞–Ω–∏—è –ø–æ–∑–∏—Ü–∏–π)
        # –ü—Ä–∏–º–µ—Ä–Ω—ã–π funding rate (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∂–∞—Ç—å —Å –±–∏—Ä–∂–∏)
        # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π funding = –ª–æ–Ω–≥–∏ –ø–ª–∞—Ç—è—Ç —à–æ—Ä—Ç–∞–º
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É —Å–ø–æ—Ç –∏ —Ñ—å—é—á–µ—Ä—Å –∫–∞–∫ –ø—Ä–æ–∫—Å–∏
        df["funding_proxy"] = df["momentum_1h"] * 0.01  # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞

        # –°—Ç–æ–∏–º–æ—Å—Ç—å —É–¥–µ—Ä–∂–∞–Ω–∏—è –ø–æ–∑–∏—Ü–∏–∏ –Ω–∞ –¥–µ–Ω—å (3 funding –ø–µ—Ä–∏–æ–¥–∞)
        df["long_holding_cost_daily"] = df["funding_proxy"] * 3
        df["short_holding_cost_daily"] = -df["funding_proxy"] * 3
        features_created.extend(
            ["funding_proxy", "long_holding_cost_daily", "short_holding_cost_daily"]
        )

        if not self.disable_progress:
            self.logger.info("  ‚úì Funding rate –≤–ª–∏—è–Ω–∏–µ: —Å–æ–∑–¥–∞–Ω–æ 3 –ø—Ä–∏–∑–Ω–∞–∫–∞")

        # 6. –ú–µ—Ç—Ä–∏–∫–∏ —Ä–∏—Å–∫–∞ –¥–ª—è —Ä–∞–∑–º–µ—Ä–∞ –ø–æ–∑–∏—Ü–∏–∏
        # Value at Risk (VaR) - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø–æ—Ç–µ—Ä—è —Å 95% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
        returns_sorted = df["returns"].rolling(96).apply(lambda x: np.percentile(x, 5))
        df["var_95"] = abs(returns_sorted)

        # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π —Ä–∞–∑–º–µ—Ä –ø–æ–∑–∏—Ü–∏–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ VaR
        max_loss_per_trade = 2.0  # 2% –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø–æ—Ç–µ—Ä—è –∫–∞–∫ –≤ –∫–æ–Ω—Ñ–∏–≥–µ
        df["recommended_position_size"] = max_loss_per_trade / (
            df["var_95"] * dynamic_leverage
        )
        features_created.extend(["var_95", "recommended_position_size"])

        if not self.disable_progress:
            self.logger.info("  ‚úì –ú–µ—Ç—Ä–∏–∫–∏ —Ä–∏—Å–∫–∞ –¥–ª—è –ø–æ–∑–∏—Ü–∏–π: —Å–æ–∑–¥–∞–Ω–æ 2 –ø—Ä–∏–∑–Ω–∞–∫–∞")

        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_created = len(features_created)
        if not self.disable_progress:
            self.logger.info(
                f"‚úÖ Futures-specific features: –≤—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ {total_created} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
            )

        return df

    def _create_temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        df["hour"] = df["datetime"].dt.hour
        df["minute"] = df["datetime"].dt.minute

        # –¶–∏–∫–ª–∏—á–µ—Å–∫–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
        df["hour_sin"] = np.sin(2 * np.pi * df["hour"] / 24)
        df["hour_cos"] = np.cos(2 * np.pi * df["hour"] / 24)

        df["dayofweek"] = df["datetime"].dt.dayofweek
        df["is_weekend"] = (df["dayofweek"] >= 5).astype(int)

        df["dow_sin"] = np.sin(2 * np.pi * df["dayofweek"] / 7)
        df["dow_cos"] = np.cos(2 * np.pi * df["dayofweek"] / 7)

        df["day"] = df["datetime"].dt.day
        df["month"] = df["datetime"].dt.month

        df["month_sin"] = np.sin(2 * np.pi * df["month"] / 12)
        df["month_cos"] = np.cos(2 * np.pi * df["month"] / 12)

        # –¢–æ—Ä–≥–æ–≤—ã–µ —Å–µ—Å—Å–∏–∏
        df["asian_session"] = ((df["hour"] >= 0) & (df["hour"] < 8)).astype(int)
        df["european_session"] = ((df["hour"] >= 7) & (df["hour"] < 16)).astype(int)
        df["american_session"] = ((df["hour"] >= 13) & (df["hour"] < 22)).astype(int)

        # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–µ—Å—Å–∏–π
        df["session_overlap"] = (
            (df["asian_session"] + df["european_session"] + df["american_session"]) > 1
        ).astype(int)

        return df

    def _create_ml_optimized_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ ML-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è 2024-2025"""
        if not self.disable_progress:
            self.logger.info("–°–æ–∑–¥–∞–Ω–∏–µ ML-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        # 1. Hurst Exponent - –º–µ—Ä–∞ –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Ä—ã–Ω–∫–∞
        # >0.5 = —Ç—Ä–µ–Ω–¥, <0.5 = –≤–æ–∑–≤—Ä–∞—Ç –∫ —Å—Ä–µ–¥–Ω–µ–º—É, ~0.5 = —Å–ª—É—á–∞–π–Ω–æ–µ –±–ª—É–∂–¥–∞–Ω–∏–µ
        def hurst_exponent(ts, max_lag=20):
            """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç—ã –•–µ—Ä—Å—Ç–∞"""
            lags = range(2, min(max_lag, len(ts) // 2))
            tau = []

            for lag in lags:
                pp = np.array(ts[:-lag])
                pn = np.array(ts[lag:])
                diff = pn - pp
                tau.append(np.sqrt(np.nanmean(diff**2)))

            if len(tau) > 0 and all(t > 0 for t in tau):
                poly = np.polyfit(np.log(lags), np.log(tau), 1)
                return poly[0] * 2.0
            return 0.5

        # –ü—Ä–∏–º–µ–Ω—è–µ–º Hurst –¥–ª—è close —Å –æ–∫–Ω–æ–º 50
        df["hurst_exponent"] = (
            df["close"]
            .rolling(50)
            .apply(lambda x: hurst_exponent(x) if len(x) == 50 else 0.5)
        )

        # 2. Fractal Dimension - —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Ü–µ–Ω–æ–≤–æ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è
        # 1 = –ø—Ä—è–º–∞—è –ª–∏–Ω–∏—è, 2 = –∑–∞–ø–æ–ª–Ω—è–µ—Ç –ø–ª–æ—Å–∫–æ—Å—Ç—å
        def fractal_dimension(ts):
            """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–º –•–∏–≥—É—á–∏"""
            N = len(ts)
            if N < 10:
                return 1.5

            kmax = min(5, N // 2)
            L = []

            for k in range(1, kmax + 1):
                Lk = 0
                for m in range(k):
                    Lmk = 0
                    for i in range(1, int((N - m) / k)):
                        Lmk += abs(ts[m + i * k] - ts[m + (i - 1) * k])
                    if int((N - m) / k) > 0:
                        Lmk = Lmk * (N - 1) / (k * int((N - m) / k))
                    Lk += Lmk
                L.append(Lk / k)

            if len(L) > 0 and all(l > 0 for l in L):
                x = np.log(range(1, kmax + 1))
                y = np.log(L)
                poly = np.polyfit(x, y, 1)
                return poly[0]
            return 1.5

        df["fractal_dimension"] = (
            df["close"]
            .rolling(30)
            .apply(lambda x: fractal_dimension(x.values) if len(x) == 30 else 1.5)
        )

        # 3. Market Efficiency Ratio - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã
        # –í—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è = —Å–∏–ª—å–Ω—ã–π —Ç—Ä–µ–Ω–¥, –Ω–∏–∑–∫–∏–µ = –±–æ–∫–æ–≤–∏–∫
        df["efficiency_ratio"] = self.safe_divide(
            (df["close"] - df["close"].shift(20)).abs(),
            df["close"].diff().abs().rolling(20).sum(),
        )

        # 4. Trend Quality Index - –∫–∞—á–µ—Å—Ç–≤–æ —Ç—Ä–µ–Ω–¥–∞
        # –ö–æ–º–±–∏–Ω–∞—Ü–∏—è ADX, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        df["trend_quality"] = (
            df["adx"]
            / 100  # –°–∏–ª–∞ —Ç—Ä–µ–Ω–¥–∞
            * ((df["close"] > df["sma_50"]).astype(float) * 2 - 1)  # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
            * (
                1 - df["bb_width"] / df["bb_width"].rolling(50).max()
            )  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        )

        # 5. Regime Detection Features
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä—ã–Ω–æ—á–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ (—Ç—Ä–µ–Ω–¥/—Ñ–ª—ç—Ç/–≤—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å)
        returns = df["close"].pct_change()

        # Realized volatility
        df["realized_vol_5m"] = returns.rolling(20).std() * np.sqrt(20)
        df["realized_vol_15m"] = returns.rolling(60).std() * np.sqrt(60)
        df["realized_vol_1h"] = returns.rolling(240).std() * np.sqrt(240)

        # GARCH-–ø–æ–¥–æ–±–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
        df["garch_vol"] = returns.rolling(20).apply(
            lambda x: np.sqrt(0.94 * x.var() + 0.06 * x.iloc[-1] ** 2)
            if len(x) > 0
            else 0
        )

        # –†–µ–∂–∏–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        atr_q25 = df["atr"].rolling(1000).quantile(0.25)
        atr_q75 = df["atr"].rolling(1000).quantile(0.75)
        df["vol_regime"] = 0  # –ù–æ—Ä–º–∞–ª—å–Ω–∞—è
        df.loc[df["atr"] < atr_q25, "vol_regime"] = -1  # –ù–∏–∑–∫–∞—è
        df.loc[df["atr"] > atr_q75, "vol_regime"] = 1  # –í—ã—Å–æ–∫–∞—è

        # 6. Information-theoretic features
        # –≠–Ω—Ç—Ä–æ–ø–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π
        def shannon_entropy(series, bins=10):
            """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –®–µ–Ω–Ω–æ–Ω–∞"""
            if len(series) < bins:
                return 0
            counts, _ = np.histogram(series, bins=bins)
            probs = counts / counts.sum()
            probs = probs[probs > 0]
            return -np.sum(probs * np.log(probs))

        df["return_entropy"] = returns.rolling(100).apply(lambda x: shannon_entropy(x))

        # 7. Microstructure features
        # Amihud illiquidity
        df["amihud_illiquidity"] = (
            self.safe_divide(returns.abs(), df["turnover"]).rolling(20).mean()
        )

        # Kyle's lambda (price impact)
        df["kyle_lambda"] = self.safe_divide(
            returns.abs().rolling(20).mean(), df["volume"].rolling(20).mean()
        )

        # 8. Cross-sectional features (–µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ BTC)
        if "btc_returns" in df.columns:
            # Beta –∫ BTC
            df["btc_beta"] = (
                returns.rolling(100).cov(df["btc_returns"])
                / df["btc_returns"].rolling(100).var()
            )

            # –ò–¥–∏–æ—Å–∏–Ω–∫—Ä–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            df["idio_vol"] = (
                (returns - df["btc_beta"] * df["btc_returns"]).rolling(50).std()
            )

        # 9. Autocorrelation features
        # –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ª–∞–≥–∞—Ö
        df["returns_ac_1"] = returns.rolling(50).apply(
            lambda x: x.autocorr(lag=1) if len(x) > 1 else 0
        )
        df["returns_ac_5"] = returns.rolling(50).apply(
            lambda x: x.autocorr(lag=5) if len(x) > 5 else 0
        )
        df["returns_ac_10"] = returns.rolling(50).apply(
            lambda x: x.autocorr(lag=10) if len(x) > 10 else 0
        )

        # 10. Jump detection
        # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø—Ä—ã–∂–∫–æ–≤ –≤ —Ü–µ–Ω–µ
        df["price_jump"] = (returns.abs() > returns.rolling(100).std() * 3).astype(int)

        df["jump_intensity"] = df["price_jump"].rolling(50).mean()

        # 11. Order flow imbalance persistence
        if "order_flow_imbalance" in df.columns:
            df["ofi_persistence"] = (
                df["order_flow_imbalance"]
                .rolling(20)
                .apply(lambda x: x.autocorr(lag=1) if len(x) > 1 else 0)
            )

        # 12. Volume-synchronized probability of informed trading (VPIN)
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
        df["vpin"] = self.safe_divide(
            (df["volume"] * ((df["close"] > df["open"]).astype(float) - 0.5))
            .rolling(50)
            .sum()
            .abs(),
            df["volume"].rolling(50).sum(),
        )

        # 13. Liquidity-adjusted returns
        df["liquidity_adj_returns"] = returns * (
            1 - df["amihud_illiquidity"] / df["amihud_illiquidity"].rolling(100).max()
        )

        # 14. Tail risk measures
        # Conditional Value at Risk (CVaR)
        df["cvar_5pct"] = returns.rolling(100).apply(
            lambda x: (
                x[x <= x.quantile(0.05)].mean()
                if len(x[x <= x.quantile(0.05)]) > 0
                else x.quantile(0.05)
            )
        )

        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        ml_features = [
            "hurst_exponent",
            "fractal_dimension",
            "efficiency_ratio",
            "trend_quality",
            "realized_vol_5m",
            "realized_vol_15m",
            "realized_vol_1h",
            "garch_vol",
            "vol_regime",
            "return_entropy",
            "amihud_illiquidity",
            "kyle_lambda",
            "returns_ac_1",
            "returns_ac_5",
            "returns_ac_10",
            "price_jump",
            "jump_intensity",
            "vpin",
            "liquidity_adj_returns",
            "cvar_5pct",
        ]

        # –î–æ–±–∞–≤–ª—è–µ–º —É—Å–ª–æ–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏ —Å–æ–∑–¥–∞–Ω—ã
        if "btc_beta" in df.columns:
            ml_features.extend(["btc_beta", "idio_vol"])
        if "ofi_persistence" in df.columns:
            ml_features.append("ofi_persistence")

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
        for feature in ml_features:
            if feature in df.columns:
                df[feature] = df[feature].fillna(method="ffill").fillna(0)

        return df

    def _handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        if not self.disable_progress:
            self.logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        info_cols = [
            "id",
            "symbol",
            "timestamp",
            "datetime",
            "open",
            "high",
            "low",
            "close",
            "volume",
            "turnover",
        ]

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Å–∏–º–≤–æ–ª–∞–º –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        processed_dfs = []

        for symbol in df["symbol"].unique():
            symbol_data = df[df["symbol"] == symbol].copy()

            # –î–ª—è –∫–∞–∂–¥–æ–π –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–∏–º–µ–Ω—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –º–µ—Ç–æ–¥ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è
            for col in symbol_data.columns:
                if col in info_cols:
                    continue

                if symbol_data[col].isna().any():
                    # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö (Categorical dtype)
                    if hasattr(symbol_data[col], "cat"):
                        # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–ª–∏ 'FLAT'/'HOLD'
                        if "direction" in col:
                            symbol_data[col] = symbol_data[col].fillna("FLAT")
                        else:
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥—É (–Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)
                            mode = symbol_data[col].mode()
                            if len(mode) > 0:
                                symbol_data[col] = symbol_data[col].fillna(mode.iloc[0])
                    # –î–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º forward fill
                    elif any(
                        indicator in col
                        for indicator in ["sma", "ema", "rsi", "macd", "bb_", "adx"]
                    ):
                        symbol_data[col] = symbol_data[col].ffill()
                    # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º 0
                    else:
                        symbol_data[col] = symbol_data[col].fillna(0)

            # –£–¥–∞–ª—è–µ–º –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –º–æ–≥—É—Ç –±—ã—Ç—å NaN –∏–∑-–∑–∞ —Ä–∞—Å—á–µ—Ç–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            # –ù–∞—Ö–æ–¥–∏–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø–µ—Ä–∏–æ–¥ —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            max_period = 50  # SMA50 —Ç—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º—É–º 50 –ø–µ—Ä–∏–æ–¥–æ–≤
            symbol_data = symbol_data.iloc[max_period:].copy()

            processed_dfs.append(symbol_data)

        result_df = pd.concat(processed_dfs, ignore_index=True)

        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        nan_count = result_df.isna().sum().sum()
        if nan_count > 0:
            if not self.disable_progress:
                self.logger.warning(
                    f"–û—Å—Ç–∞–ª–∏—Å—å {nan_count} NaN –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏"
                )
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è NaN
            for col in result_df.columns:
                if result_df[col].isna().any():
                    # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
                    if hasattr(result_df[col], "cat"):
                        if "direction" in col:
                            result_df[col] = result_df[col].fillna("FLAT")
                        else:
                            mode = result_df[col].mode()
                            if len(mode) > 0:
                                result_df[col] = result_df[col].fillna(mode.iloc[0])
                    # –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
                    elif pd.api.types.is_numeric_dtype(result_df[col]):
                        result_df[col] = result_df[col].fillna(0)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        numeric_cols = result_df.select_dtypes(include=[np.number]).columns
        inf_count = np.isinf(result_df[numeric_cols]).sum().sum()
        if inf_count > 0:
            if not self.disable_progress:
                self.logger.warning(
                    f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã {inf_count} –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π, –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ –∫–æ–Ω–µ—á–Ω—ã–µ"
                )
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ó–∞–º–µ–Ω—è–µ–º –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç–∏ –Ω–∞ 99-–π –ø–µ—Ä—Å–µ–Ω—Ç–∏–ª—å –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–ª–æ–Ω–∫–∏
            for col in numeric_cols:
                if np.isinf(result_df[col]).any():
                    # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä—Å–µ–Ω—Ç–∏–ª–∏ –Ω–∞ –∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö
                    finite_vals = result_df[col][np.isfinite(result_df[col])]
                    if len(finite_vals) > 0:
                        p99 = finite_vals.quantile(0.99)
                        p1 = finite_vals.quantile(0.01)
                        result_df[col] = result_df[col].replace(
                            [np.inf, -np.inf], [p99, p1]
                        )
                    else:
                        result_df[col] = result_df[col].replace(
                            [np.inf, -np.inf], [0, 0]
                        )

        if not self.disable_progress:
            self.logger.info(
                f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {len(result_df)} –∑–∞–ø–∏—Å–µ–π"
            )
        return result_df

    def _create_cross_asset_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ö—Ä–æ—Å—Å-–∞–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        if not self.disable_progress:
            self.logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–æ—Å—Å-–∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        # BTC –∫–∞–∫ –±–∞–∑–æ–≤—ã–π –∞–∫—Ç–∏–≤
        btc_data = df[df["symbol"] == "BTCUSDT"][
            ["datetime", "close", "returns"]
        ].copy()
        if len(btc_data) > 0:
            btc_data.rename(
                columns={"close": "btc_close", "returns": "btc_returns"}, inplace=True
            )

            df = df.merge(btc_data, on="datetime", how="left")

            # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å BTC
            for symbol in df["symbol"].unique():
                if symbol != "BTCUSDT":
                    mask = df["symbol"] == symbol
                    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º min_periods –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
                    df.loc[mask, "btc_correlation"] = (
                        df.loc[mask, "returns"]
                        .rolling(window=96, min_periods=50)
                        .corr(df.loc[mask, "btc_returns"])
                    )

            df.loc[df["symbol"] == "BTCUSDT", "btc_correlation"] = 1.0

            # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è —Å–∏–ª–∞ –∫ BTC
            df["relative_strength_btc"] = df["close"] / df["btc_close"]
            df["rs_btc_ma"] = df.groupby("symbol")["relative_strength_btc"].transform(
                lambda x: x.rolling(20, min_periods=10).mean()
            )

            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –∑–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è BTC-—Å–≤—è–∑–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            df["btc_close"] = (
                df["btc_close"].fillna(method="ffill").fillna(method="bfill")
            )
            df["btc_returns"] = df["btc_returns"].fillna(0.0)
            df["btc_correlation"] = df["btc_correlation"].fillna(
                0.5
            )  # –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
            df["relative_strength_btc"] = df["relative_strength_btc"].fillna(1.0)
            df["rs_btc_ma"] = df["rs_btc_ma"].fillna(1.0)
        else:
            # –ó–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –µ—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö BTC
            df["btc_close"] = 0
            df["btc_returns"] = 0
            df["btc_correlation"] = 0
            df["relative_strength_btc"] = 0
            df["rs_btc_ma"] = 0

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ–∫—Ç–æ—Ä–∞
        defi_tokens = ["AAVEUSDT", "UNIUSDT", "CAKEUSDT", "DYDXUSDT"]
        layer1_tokens = ["ETHUSDT", "SOLUSDT", "AVAXUSDT", "DOTUSDT", "NEARUSDT"]
        meme_tokens = [
            "DOGEUSDT",
            "FARTCOINUSDT",
            "MELANIAUSDT",
            "TRUMPUSDT",
            "POPCATUSDT",
            "PNUTUSDT",
            "ZEREBROUSDT",
            "WIFUSDT",
        ]

        df["sector"] = "other"
        df.loc[df["symbol"].isin(defi_tokens), "sector"] = "defi"
        df.loc[df["symbol"].isin(layer1_tokens), "sector"] = "layer1"
        df.loc[df["symbol"].isin(meme_tokens), "sector"] = "meme"
        df.loc[df["symbol"] == "BTCUSDT", "sector"] = "btc"

        # –°–µ–∫—Ç–æ—Ä–Ω—ã–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
        df["sector_returns"] = df.groupby(["datetime", "sector"])["returns"].transform(
            "mean"
        )

        # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –∫ —Å–µ–∫—Ç–æ—Ä—É
        df["relative_to_sector"] = df["returns"] - df["sector_returns"]

        # –†–∞–Ω–∫ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
        df["returns_rank"] = df.groupby("datetime")["returns"].rank(pct=True)

        # 24-—á–∞—Å–æ–≤–æ–π –º–æ–º–µ–Ω—Ç—É–º - —É–∂–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω –≤ rally_detection_features
        # –ó–¥–µ—Å—å —Ç–æ–ª—å–∫–æ –∑–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è –µ—Å–ª–∏ –µ—Å—Ç—å
        if "momentum_24h" in df.columns and df["momentum_24h"].isna().any():
            df["momentum_24h"] = df["momentum_24h"].fillna(0)
        df["is_momentum_leader"] = (
            df.groupby("datetime")["momentum_24h"].rank(ascending=False) <= 5
        ).astype(int)

        return df

    def _create_target_variables(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –ë–ï–ó –£–¢–ï–ß–ï–ö –î–ê–ù–ù–´–• - –≤–µ—Ä—Å–∏—è 4.0"""
        if not self.disable_progress:
            self.logger.info("üéØ –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö v4.0 (–±–µ–∑ —É—Ç–µ—á–µ–∫)...")

        # –ü–µ—Ä–∏–æ–¥—ã –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –±—É–¥—É—â–∏—Ö –≤–æ–∑–≤—Ä–∞—Ç–æ–≤ (–≤ —Å–≤–µ—á–∞—Ö –ø–æ 15 –º–∏–Ω—É—Ç)
        return_periods = {
            "15m": 1,  # 15 –º–∏–Ω—É—Ç
            "1h": 4,  # 1 —á–∞—Å
            "4h": 16,  # 4 —á–∞—Å–∞
            "12h": 48,  # 12 —á–∞—Å–æ–≤
        }

        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        # –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–´ –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–∏–≥–Ω–∞–ª–æ–≤
        direction_thresholds = {
            "15m": 0.0015,  # 0.15% - —É–º–µ–Ω—å—à–∞–µ—Ç —à—É–º –æ—Ç –º–µ–ª–∫–∏—Ö –¥–≤–∏–∂–µ–Ω–∏–π
            "1h": 0.003,  # 0.3% - —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ –∫–æ–ª–µ–±–∞–Ω–∏—è
            "4h": 0.007,  # 0.7% - —Ñ–æ–∫—É—Å –Ω–∞ –∑–Ω–∞—á–∏–º—ã—Ö –¥–≤–∏–∂–µ–Ω–∏—è—Ö
            "12h": 0.01,  # 1% - –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã
        }

        # –£—Ä–æ–≤–Ω–∏ –ø—Ä–∏–±—ã–ª–∏ –¥–ª—è –±–∏–Ω–∞—Ä–Ω—ã—Ö —Ü–µ–ª–µ–≤—ã—Ö
        profit_levels = {
            "1pct_4h": (0.01, 16),  # 1% –∑–∞ 4 —á–∞—Å–∞
            "2pct_4h": (0.02, 16),  # 2% –∑–∞ 4 —á–∞—Å–∞
            "3pct_12h": (0.03, 48),  # 3% –∑–∞ 12 —á–∞—Å–æ–≤
            "5pct_12h": (0.05, 48),  # 5% –∑–∞ 12 —á–∞—Å–æ–≤
        }

        # Commission and costs
        commission_rate = 0.0006  # 0.06%
        slippage = 0.0005  # 0.05%

        # A. –ë–∞–∑–æ–≤—ã–µ –≤–æ–∑–≤—Ä–∞—Ç—ã (4)
        for period_name, n_candles in return_periods.items():
            df[f"future_return_{period_name}"] = df.groupby("symbol")[
                "close"
            ].transform(lambda x: x.shift(-n_candles) / x - 1)

        # B. –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è (4)
        for period_name in return_periods.keys():
            future_return = df[f"future_return_{period_name}"]
            threshold = direction_thresholds[period_name]

            df[f"direction_{period_name}"] = pd.cut(
                future_return,
                bins=[-np.inf, -threshold, threshold, np.inf],
                labels=["DOWN", "FLAT", "UP"],
            )

        # C. –î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π –ø—Ä–∏–±—ã–ª–∏ LONG (4) - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ shift –¥–ª—è –±—É–¥—É—â–∏—Ö —Ü–µ–Ω
        for level_name, (profit_threshold, n_candles) in profit_levels.items():
            # –î–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∏–≥–Ω–µ—Ç –ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ –Ω—É–∂–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è
            max_future_returns = pd.DataFrame()
            for i in range(1, n_candles + 1):
                future_high = df.groupby("symbol")["high"].transform(
                    lambda x: x.shift(-i)
                )
                future_return = future_high / df["close"] - 1
                max_future_returns[f"return_{i}"] = future_return

            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π return –∑–∞ –ø–µ—Ä–∏–æ–¥
            max_return = max_future_returns.max(axis=1)
            df[f"long_will_reach_{level_name}"] = (
                max_return >= profit_threshold
            ).astype(int)

        # D. –î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π –ø—Ä–∏–±—ã–ª–∏ SHORT (4)
        for level_name, (profit_threshold, n_candles) in profit_levels.items():
            # –î–ª—è SHORT: –ø—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é —Ü–µ–Ω—É
            min_future_returns = pd.DataFrame()
            for i in range(1, n_candles + 1):
                future_low = df.groupby("symbol")["low"].transform(
                    lambda x: x.shift(-i)
                )
                future_return = df["close"] / future_low - 1  # –î–ª—è SHORT –∏–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º
                min_future_returns[f"return_{i}"] = future_return

            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π return –¥–ª—è SHORT –∑–∞ –ø–µ—Ä–∏–æ–¥
            max_return = min_future_returns.max(axis=1)
            df[f"short_will_reach_{level_name}"] = (
                max_return >= profit_threshold
            ).astype(int)

        # E. –†–∏—Å–∫-–º–µ—Ç—Ä–∏–∫–∏ (4)
        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞ –∑–∞ –ø–µ—Ä–∏–æ–¥ (–¥–ª—è LONG)
        for period_name, n_candles in [("1h", 4), ("4h", 16)]:
            min_prices = pd.DataFrame()
            for i in range(1, n_candles + 1):
                future_low = df.groupby("symbol")["low"].transform(
                    lambda x: x.shift(-i)
                )
                min_prices[f"low_{i}"] = future_low

            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ –∑–∞ –ø–µ—Ä–∏–æ–¥
            min_price = min_prices.min(axis=1)
            df[f"max_drawdown_{period_name}"] = (df["close"] / min_price - 1).fillna(0)

        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç –∑–∞ –ø–µ—Ä–∏–æ–¥ (–¥–ª—è SHORT)
        for period_name, n_candles in [("1h", 4), ("4h", 16)]:
            max_prices = pd.DataFrame()
            for i in range(1, n_candles + 1):
                future_high = df.groupby("symbol")["high"].transform(
                    lambda x: x.shift(-i)
                )
                max_prices[f"high_{i}"] = future_high

            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ –∑–∞ –ø–µ—Ä–∏–æ–¥
            max_price = max_prices.max(axis=1)
            df[f"max_rally_{period_name}"] = (max_price / df["close"] - 1).fillna(0)

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–±–∏—Ä–∞–µ–º —Ç–æ—Ä–≥–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã —Å —É—Ç–µ—á–∫–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö
        # best_action, risk_reward_ratio –∏ optimal_hold_time –±—É–¥—É—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è
        # –≤ trading/signal_generator.py –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏

        # –ü–ï–†–ï–ù–ï–°–ï–ù–û –í –ü–†–ò–ó–ù–ê–ö–ò: signal_strength —Ç–µ–ø–µ—Ä—å feature, –Ω–µ target
        # –≠—Ç–æ –æ—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –±–µ–∑ —É—Ç–µ—á–µ–∫

        # –£–î–ê–õ–ï–ù–û: risk_reward_ratio –∏ optimal_hold_time —Å–æ–¥–µ—Ä–∂–∞–ª–∏ —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        # –≠—Ç–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –±—É–¥—É—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –≤ trading/signal_generator.py
        # –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏, –∞ –Ω–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –±—É–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö

        # –£–î–ê–õ–ï–ù–û: best_action –∏ –≤—Å–µ legacy –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (best_direction, reached, hit)
        # –í –≤–µ—Ä—Å–∏–∏ 4.0 –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ 20 —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –±–µ–∑ —É—Ç–µ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö
        # –í—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —É–∂–µ —Å–æ–∑–¥–∞–Ω—ã –≤—ã—à–µ

        # –§–∏–∫—Ç–∏–≤–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        df["long_tp1_time"] = 16  # 4 —á–∞—Å–∞
        df["long_tp2_time"] = 16
        df["long_tp3_time"] = 48  # 12 —á–∞—Å–æ–≤
        df["long_sl_time"] = 100
        df["short_tp1_time"] = 16
        df["short_tp2_time"] = 16
        df["short_tp3_time"] = 48
        df["short_sl_time"] = 100

        # Expected value –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        df["long_expected_value"] = (
            df["future_return_4h"] * df["long_will_reach_2pct_4h"] * 2.0
        )
        df["short_expected_value"] = (
            -df["future_return_4h"] * df["short_will_reach_2pct_4h"] * 2.0
        )

        # Optimal entry —Ñ–∏–∫—Ç–∏–≤–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        df["long_optimal_entry_time"] = 1
        df["long_optimal_entry_price"] = df["close"]
        df["long_optimal_entry_improvement"] = 0
        df["short_optimal_entry_time"] = 1
        df["short_optimal_entry_price"] = df["close"]
        df["short_optimal_entry_improvement"] = 0

        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if not self.disable_progress:
            self.logger.info("  ‚úÖ –°–æ–∑–¥–∞–Ω–æ 20 —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –±–µ–∑ —É—Ç–µ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö")
            self.logger.info("  üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π:")
            for period in ["15m", "1h", "4h", "12h"]:
                if f"direction_{period}" in df.columns:
                    dist = df[f"direction_{period}"].value_counts(normalize=True) * 100
                    self.logger.info(
                        f"     {period}: UP={dist.get('UP', 0):.1f}%, DOWN={dist.get('DOWN', 0):.1f}%, FLAT={dist.get('FLAT', 0):.1f}%"
                    )

        return df

    def _normalize_features(self, df: pd.DataFrame, fit: bool = True) -> pd.DataFrame:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–µ–∂–∏–º–∞ fit/transform

        Args:
            df: –¥–∞–Ω–Ω—ã–µ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            fit: –µ—Å–ª–∏ True - –æ–±—É—á–∞–µ—Ç scaler, –µ—Å–ª–∏ False - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π
        """
        if fit:
            if not self.disable_progress:
                self.logger.info("üìä –û–±—É—á–µ–Ω–∏–µ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏...")
        else:
            if not self.disable_progress:
                self.logger.info("üìä –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏...")

        # –°—Ç–æ–ª–±—Ü—ã –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        exclude_cols = [
            "id",
            "symbol",
            "timestamp",
            "datetime",
            "sector",
            "open",
            "high",
            "low",
            "close",
            "volume",
            "turnover",
        ]

        # –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        target_cols = [
            col
            for col in df.columns
            if any(
                pattern in col
                for pattern in [
                    "target_",
                    "future_",
                    "optimal_",
                    "_reached",
                    "_tp",
                    "_sl",
                    "expected_value",
                    "best_direction",
                    "signal_strength",
                ]
            )
        ]
        exclude_cols.extend(target_cols)

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        time_cols = [
            "hour",
            "minute",
            "dayofweek",
            "day",
            "month",
            "is_weekend",
            "asian_session",
            "european_session",
            "american_session",
            "session_overlap",
        ]
        exclude_cols.extend(time_cols)

        # –ü—Ä–∏–∑–Ω–∞–∫–∏-—Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã –ø–æ —Å–≤–æ–µ–π –ø—Ä–∏—Ä–æ–¥–µ
        ratio_cols = [
            "close_vwap_ratio",
            "close_open_ratio",
            "high_low_ratio",
            "close_position",
            "bb_position",
            "position_in_range_20",
            "position_in_range_50",
            "position_in_range_100",
        ]
        exclude_cols.extend(ratio_cols)

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º–∏ –ù–ï –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        technical_indicators = [
            "rsi",
            "stoch_k",
            "stoch_d",
            "adx",
            "adx_pos",
            "adx_neg",
            "rsi_oversold",
            "rsi_overbought",
            "toxicity",
            "psar_trend",
            "cci",
            "williams_r",
            "roc",
            "momentum",
            "kama",
            "trix",
            "ppo",
            "macd",
            "macd_signal",
            "macd_diff",
        ]
        exclude_cols.extend(technical_indicators)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        feature_cols = [col for col in numeric_cols if col not in exclude_cols]

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        if not self.disable_progress:
            self.logger.debug(
                f"–ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ ({len(feature_cols)}): {feature_cols[:10]}..."
            )
            excluded_technical = [
                col
                for col in [
                    "toxicity",
                    "bb_position",
                    "close_position",
                    "psar_trend",
                    "rsi_oversold",
                    "rsi_overbought",
                ]
                if col in numeric_cols
            ]
            self.logger.debug(
                f"–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –≤ –∏—Å–∫–ª—é—á–µ–Ω–∏—è—Ö: {excluded_technical}"
            )

        if not feature_cols:
            self.logger.warning("‚ö†Ô∏è –ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏!")
            return df

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        for symbol in df["symbol"].unique():
            symbol_mask = df["symbol"] == symbol

            if symbol_mask.sum() > 0:
                if fit:
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π scaler –¥–ª—è —Å–∏–º–≤–æ–ª–∞
                    if symbol not in self.scalers:
                        self.scalers[symbol] = RobustScaler()

                    # –û–±—É—á–∞–µ–º scaler
                    symbol_data = df.loc[symbol_mask, feature_cols]
                    valid_data = symbol_data.dropna()

                    if len(valid_data) > 0:
                        self.scalers[symbol].fit(valid_data)
                        if not self.disable_progress:
                            self.logger.debug(
                                f"‚úÖ Scaler –æ–±—É—á–µ–Ω –¥–ª—è {symbol} –Ω–∞ {len(valid_data)} –∑–∞–ø–∏—Å—è—Ö"
                            )

                # –ü—Ä–∏–º–µ–Ω—è–µ–º scaler (–µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
                if symbol in self.scalers:
                    valid_mask = symbol_mask & df[feature_cols].notna().all(axis=1)
                    if valid_mask.sum() > 0:
                        df.loc[valid_mask, feature_cols] = self.scalers[
                            symbol
                        ].transform(df.loc[valid_mask, feature_cols])
                else:
                    if not self.disable_progress:
                        self.logger.warning(f"‚ö†Ô∏è Scaler –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è {symbol}")

        return df

    def _normalize_walk_forward(
        self, df: pd.DataFrame, train_end_date: str
    ) -> pd.DataFrame:
        """Walk-forward –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±–µ–∑ data leakage"""
        if not self.disable_progress:
            self.logger.info(f"Walk-forward –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ {train_end_date}...")

        # –°—Ç–æ–ª–±—Ü—ã –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        exclude_cols = [
            "id",
            "symbol",
            "timestamp",
            "datetime",
            "sector",
            "open",
            "high",
            "low",
            "close",
            "volume",
            "turnover",
        ]

        # –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        target_cols = [
            col
            for col in df.columns
            if col.startswith(("target_", "future_", "optimal_"))
        ]
        exclude_cols.extend(target_cols)

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        time_cols = [
            "hour",
            "minute",
            "dayofweek",
            "day",
            "month",
            "is_weekend",
            "asian_session",
            "european_session",
            "american_session",
            "session_overlap",
        ]
        exclude_cols.extend(time_cols)

        # –ü—Ä–∏–∑–Ω–∞–∫–∏-—Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã –ø–æ —Å–≤–æ–µ–π –ø—Ä–∏—Ä–æ–¥–µ
        ratio_cols = [
            "close_vwap_ratio",
            "close_open_ratio",
            "high_low_ratio",
            "close_position",
            "bb_position",
            "position_in_range_20",
            "position_in_range_50",
            "position_in_range_100",
        ]
        exclude_cols.extend(ratio_cols)

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º–∏ –ù–ï –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        technical_indicators = [
            "rsi",
            "stoch_k",
            "stoch_d",
            "adx",
            "adx_pos",
            "adx_neg",
            "rsi_oversold",
            "rsi_overbought",
            "toxicity",
            "psar_trend",
            "cci",
            "williams_r",
            "roc",
            "momentum",
            "kama",
            "trix",
            "ppo",
            "macd",
            "macd_signal",
            "macd_diff",
        ]
        exclude_cols.extend(technical_indicators)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        feature_cols = [col for col in df.columns if col not in exclude_cols]

        # –ú–∞—Å–∫–∞ –¥–ª—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        train_mask = df["datetime"] <= pd.to_datetime(train_end_date)

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        for symbol in df["symbol"].unique():
            symbol_mask = df["symbol"] == symbol
            train_symbol_mask = symbol_mask & train_mask

            if train_symbol_mask.sum() > 0:
                if symbol not in self.scalers:
                    self.scalers[symbol] = StandardScaler()

                # –û–±—É—á–∞–µ–º scaler —Ç–æ–ª—å–∫–æ –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö
                train_data = df.loc[train_symbol_mask, feature_cols].dropna()
                if len(train_data) > 0:
                    self.scalers[symbol].fit(train_data)

                    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ –≤—Å–µ–º –¥–∞–Ω–Ω—ã–º —Å–∏–º–≤–æ–ª–∞
                    valid_mask = symbol_mask & df[feature_cols].notna().all(axis=1)
                    if valid_mask.sum() > 0:
                        df.loc[valid_mask, feature_cols] = self.scalers[
                            symbol
                        ].transform(df.loc[valid_mask, feature_cols])

        return df

    def _log_feature_statistics(self, df: pd.DataFrame):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º"""
        if not self.disable_progress:
            feature_counts = {
                "basic": len(
                    [
                        col
                        for col in df.columns
                        if col
                        in [
                            "returns",
                            "high_low_ratio",
                            "close_open_ratio",
                            "volume_ratio",
                        ]
                    ]
                ),
                "technical": len(
                    [
                        col
                        for col in df.columns
                        if any(
                            ind in col
                            for ind in ["sma", "ema", "rsi", "macd", "bb", "atr"]
                        )
                    ]
                ),
                "microstructure": len(
                    [
                        col
                        for col in df.columns
                        if any(
                            ms in col
                            for ms in ["spread", "imbalance", "toxicity", "illiquidity"]
                        )
                    ]
                ),
                "temporal": len(
                    [
                        col
                        for col in df.columns
                        if any(t in col for t in ["hour", "day", "month", "session"])
                    ]
                ),
                "cross_asset": len(
                    [
                        col
                        for col in df.columns
                        if any(
                            ca in col for ca in ["btc_", "sector", "rank", "momentum"]
                        )
                    ]
                ),
            }

            self.logger.info(f"üìä –°–æ–∑–¥–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º: {feature_counts}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            missing_counts = df.isnull().sum()
            if missing_counts.sum() > 0:
                self.logger.warning(
                    f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ {missing_counts[missing_counts > 0].shape[0]} –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö"
                )

    def get_feature_names(self, include_targets: bool = False) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        return []

    def save_scalers(self, path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∫–µ–π–ª–µ—Ä–æ–≤ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ"""
        import pickle

        with open(path, "wb") as f:
            pickle.dump(self.scalers, f)

        if not self.disable_progress:
            self.logger.info(f"–°–∫–µ–π–ª–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {path}")

    def load_scalers(self, path: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Å–∫–µ–π–ª–µ—Ä–æ–≤"""
        import pickle

        with open(path, "rb") as f:
            self.scalers = pickle.load(f)

        if not self.disable_progress:
            self.logger.info(f"–°–∫–µ–π–ª–µ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ {path}")

    def create_features_with_train_split(
        self, df: pd.DataFrame, train_ratio: float = 0.6, val_ratio: float = 0.2
    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ë–ï–ó DATA LEAKAGE

        Args:
            df: –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            train_ratio: –¥–æ–ª—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            val_ratio: –¥–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

        Returns:
            Tuple[train_data, val_data, test_data] - –ø—Ä–∞–≤–∏–ª—å–Ω–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        """
        if not self.disable_progress:
            self.logger.start_stage(
                "feature_engineering_no_leakage", symbols=df["symbol"].nunique()
            )

        # 1. –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–±–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏)
        if not self.disable_progress:
            self.logger.info("1/5 - –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        featured_dfs = []

        symbols = df["symbol"].unique()
        if not self.disable_progress:
            self.logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(symbols)} —Å–∏–º–≤–æ–ª–æ–≤...")

        # –í –º–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–Ω–æ–º —Ä–µ–∂–∏–º–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä—ã –Ω–µ –Ω—É–∂–Ω—ã
        disable_progress = hasattr(self, "disable_progress") and self.disable_progress

        if disable_progress:
            symbols_iterator = symbols
        else:
            symbols_iterator = tqdm(symbols, desc="–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤", unit="—Å–∏–º–≤–æ–ª")

        for symbol in symbols_iterator:
            symbol_data = df[df["symbol"] == symbol].copy()
            symbol_data = symbol_data.sort_values("datetime")

            symbol_data = self._create_basic_features(symbol_data)
            symbol_data = self._create_technical_indicators(symbol_data)
            symbol_data = self._create_microstructure_features(symbol_data)
            symbol_data = self._create_rally_detection_features(symbol_data)
            symbol_data = self._create_signal_quality_features(symbol_data)
            symbol_data = self._create_futures_specific_features(symbol_data)
            symbol_data = self._create_ml_optimized_features(symbol_data)
            symbol_data = self._create_temporal_features(symbol_data)
            symbol_data = self._create_target_variables(symbol_data)

            featured_dfs.append(symbol_data)

        if not self.disable_progress:
            self.logger.info("2/5 - –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∫—Ä–æ—Å—Å-–∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        result_df = pd.concat(featured_dfs, ignore_index=True)
        result_df = self._create_cross_asset_features(result_df)

        if not self.disable_progress:
            self.logger.info("3/5 - –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...")
        result_df = self._handle_missing_values(result_df)

        # 2. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ü–û –í–†–ï–ú–ï–ù–ò (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è data leakage)
        if not self.disable_progress:
            self.logger.info("4/5 - –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
        train_data_list = []
        val_data_list = []
        test_data_list = []

        for symbol in result_df["symbol"].unique():
            symbol_data = result_df[result_df["symbol"] == symbol].sort_values(
                "datetime"
            )
            n = len(symbol_data)

            train_end = int(n * train_ratio)
            val_end = int(n * (train_ratio + val_ratio))

            train_data_list.append(symbol_data.iloc[:train_end])
            val_data_list.append(symbol_data.iloc[train_end:val_end])
            test_data_list.append(symbol_data.iloc[val_end:])

        train_data = pd.concat(train_data_list, ignore_index=True)
        val_data = pd.concat(val_data_list, ignore_index=True)
        test_data = pd.concat(test_data_list, ignore_index=True)

        # 3. –ü–†–ê–í–ò–õ–¨–ù–ê–Ø –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ë–ï–ó DATA LEAKAGE
        if not self.disable_progress:
            self.logger.info("5/5 - –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±–µ–∑ data leakage...")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        exclude_cols = [
            "id",
            "symbol",
            "timestamp",
            "datetime",
            "sector",
            "open",
            "high",
            "low",
            "close",
            "volume",
            "turnover",
        ]

        # –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        target_cols = [
            col
            for col in train_data.columns
            if col.startswith(("target_", "future_", "optimal_"))
        ]
        exclude_cols.extend(target_cols)

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (—É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã)
        time_cols = [
            "hour",
            "minute",
            "dayofweek",
            "day",
            "month",
            "is_weekend",
            "asian_session",
            "european_session",
            "american_session",
            "session_overlap",
        ]
        exclude_cols.extend(time_cols)

        # –ü—Ä–∏–∑–Ω–∞–∫–∏-—Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã –ø–æ —Å–≤–æ–µ–π –ø—Ä–∏—Ä–æ–¥–µ
        ratio_cols = [
            "close_vwap_ratio",
            "close_open_ratio",
            "high_low_ratio",
            "close_position",
            "bb_position",
            "position_in_range_20",
            "position_in_range_50",
            "position_in_range_100",
        ]
        exclude_cols.extend(ratio_cols)

        feature_cols = [col for col in train_data.columns if col not in exclude_cols]

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        unique_symbols = train_data["symbol"].unique()
        # –í –º–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–Ω–æ–º —Ä–µ–∂–∏–º–µ –æ—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä—ã
        if disable_progress:
            norm_iterator = unique_symbols
        else:
            norm_iterator = tqdm(unique_symbols, desc="–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è", unit="—Å–∏–º–≤–æ–ª")

        for symbol in norm_iterator:
            # –ú–∞—Å–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
            train_mask = train_data["symbol"] == symbol
            val_mask = val_data["symbol"] == symbol
            test_mask = test_data["symbol"] == symbol

            if train_mask.sum() == 0:
                continue

            # –û–±—É—á–∞–µ–º scaler –¢–û–õ–¨–ö–û –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö
            if symbol not in self.scalers:
                self.scalers[symbol] = RobustScaler()

            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ train –¥–∞–Ω–Ω—ã–µ
            train_symbol_data = train_data.loc[train_mask, feature_cols].dropna()

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–æ –≤—Å–µ–º —Ü–∏–∫–ª–µ
            numeric_feature_cols = []

            if len(train_symbol_data) > 0:
                # –û—á–∏—Å—Ç–∫–∞ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ train –¥–∞–Ω–Ω—ã—Ö
                train_cleaned = train_symbol_data.copy()

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
                for col in feature_cols:
                    if col in train_cleaned.columns and pd.api.types.is_numeric_dtype(
                        train_cleaned[col]
                    ):
                        numeric_feature_cols.append(col)
                    else:
                        if not self.disable_progress:
                            self.logger.warning(
                                f"–ö–æ–ª–æ–Ω–∫–∞ '{col}' –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —á–∏—Å–ª–æ–≤–æ–π –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º"
                            )

                for col in numeric_feature_cols:
                    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –ø–µ—Ä–µ–¥ –∫–≤–∞–Ω—Ç–∏–ª—è–º–∏
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø –Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –µ—Å—Ç—å —Å—Ç—Ä–æ–∫–∏
                    train_cleaned[col] = pd.to_numeric(
                        train_cleaned[col], errors="coerce"
                    )

                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–æ–ª—å–∫–æ NaN –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                    if train_cleaned[col].notna().sum() == 0:
                        if not self.disable_progress:
                            self.logger.warning(
                                f"–ö–æ–ª–æ–Ω–∫–∞ '{col}' —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ NaN –∑–Ω–∞—á–µ–Ω–∏—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º"
                            )
                        continue

                    # –ö–ª–∏–ø–ø–∏–Ω–≥ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                    q01 = train_cleaned[col].quantile(0.01)
                    q99 = train_cleaned[col].quantile(0.99)
                    train_cleaned[col] = train_cleaned[col].clip(lower=q01, upper=q99)

                    # –ó–∞–º–µ–Ω–∞ inf –Ω–∞ –∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    train_cleaned[col] = train_cleaned[col].replace(
                        [np.inf, -np.inf], [q99, q01]
                    )
                    train_cleaned[col] = train_cleaned[col].fillna(
                        train_cleaned[col].median()
                    )

                # –û–±—É—á–∞–µ–º scaler –Ω–∞ –æ—á–∏—â–µ–Ω–Ω—ã—Ö train –¥–∞–Ω–Ω—ã—Ö —Ç–æ–ª—å–∫–æ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –∫–æ–ª–æ–Ω–∫–∞–º
                self.scalers[symbol].fit(train_cleaned[numeric_feature_cols])

                # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ –≤—Å–µ–º –¥–∞–Ω–Ω—ã–º —Å–∏–º–≤–æ–ª–∞
                # Train
                train_valid_mask = train_mask & train_data[
                    numeric_feature_cols
                ].notna().all(axis=1)
                if train_valid_mask.sum() > 0:
                    train_to_scale = train_data.loc[
                        train_valid_mask, numeric_feature_cols
                    ].copy()
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—É –∂–µ –æ—á–∏—Å—Ç–∫—É
                    for col in numeric_feature_cols:
                        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø
                        train_to_scale[col] = pd.to_numeric(
                            train_to_scale[col], errors="coerce"
                        )

                        if train_to_scale[col].notna().sum() == 0:
                            continue

                        q01 = (
                            train_cleaned[col].quantile(0.01)
                            if col in train_cleaned.columns
                            else train_to_scale[col].quantile(0.01)
                        )
                        q99 = (
                            train_cleaned[col].quantile(0.99)
                            if col in train_cleaned.columns
                            else train_to_scale[col].quantile(0.99)
                        )
                        train_to_scale[col] = train_to_scale[col].clip(
                            lower=q01, upper=q99
                        )
                        train_to_scale[col] = train_to_scale[col].replace(
                            [np.inf, -np.inf], [q99, q01]
                        )
                        train_to_scale[col] = train_to_scale[col].fillna(
                            train_to_scale[col].median()
                        )

                    train_data.loc[train_valid_mask, numeric_feature_cols] = (
                        self.scalers[symbol].transform(train_to_scale)
                    )

                # Val
                val_valid_mask = val_mask & val_data[numeric_feature_cols].notna().all(
                    axis=1
                )
                if val_valid_mask.sum() > 0:
                    val_to_scale = val_data.loc[
                        val_valid_mask, numeric_feature_cols
                    ].copy()
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—É –∂–µ –æ—á–∏—Å—Ç–∫—É –∏—Å–ø–æ–ª—å–∑—É—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–∑ train
                    for col in numeric_feature_cols:
                        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø
                        val_to_scale[col] = pd.to_numeric(
                            val_to_scale[col], errors="coerce"
                        )

                        if val_to_scale[col].notna().sum() == 0:
                            continue

                        q01 = (
                            train_cleaned[col].quantile(0.01)
                            if col in train_cleaned.columns
                            else val_to_scale[col].quantile(0.01)
                        )
                        q99 = (
                            train_cleaned[col].quantile(0.99)
                            if col in train_cleaned.columns
                            else val_to_scale[col].quantile(0.99)
                        )
                        val_to_scale[col] = val_to_scale[col].clip(lower=q01, upper=q99)
                        val_to_scale[col] = val_to_scale[col].replace(
                            [np.inf, -np.inf], [q99, q01]
                        )
                        val_to_scale[col] = val_to_scale[col].fillna(
                            val_to_scale[col].median()
                        )

                    val_data.loc[val_valid_mask, numeric_feature_cols] = self.scalers[
                        symbol
                    ].transform(val_to_scale)

                # Test
                test_valid_mask = test_mask & test_data[
                    numeric_feature_cols
                ].notna().all(axis=1)
                if test_valid_mask.sum() > 0:
                    test_to_scale = test_data.loc[
                        test_valid_mask, numeric_feature_cols
                    ].copy()
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—É –∂–µ –æ—á–∏—Å—Ç–∫—É –∏—Å–ø–æ–ª—å–∑—É—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–∑ train
                    for col in numeric_feature_cols:
                        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø
                        test_to_scale[col] = pd.to_numeric(
                            test_to_scale[col], errors="coerce"
                        )

                        if test_to_scale[col].notna().sum() == 0:
                            continue

                        q01 = (
                            train_cleaned[col].quantile(0.01)
                            if col in train_cleaned.columns
                            else test_to_scale[col].quantile(0.01)
                        )
                        q99 = (
                            train_cleaned[col].quantile(0.99)
                            if col in train_cleaned.columns
                            else test_to_scale[col].quantile(0.99)
                        )
                        test_to_scale[col] = test_to_scale[col].clip(
                            lower=q01, upper=q99
                        )
                        test_to_scale[col] = test_to_scale[col].replace(
                            [np.inf, -np.inf], [q99, q01]
                        )
                        test_to_scale[col] = test_to_scale[col].fillna(
                            test_to_scale[col].median()
                        )

                    test_data.loc[test_valid_mask, numeric_feature_cols] = self.scalers[
                        symbol
                    ].transform(test_to_scale)

        # –ö–†–ò–¢–ò–ß–ù–û: –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ future –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        # NaN –ø–æ—è–≤–ª—è—é—Ç—Å—è –≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö N —Å—Ç—Ä–æ–∫–∞—Ö –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –∏–∑-–∑–∞ shift(-N)
        future_cols = [col for col in train_data.columns if col.startswith("future_")]
        if future_cols:
            if not self.disable_progress:
                self.logger.info("üßë –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å NaN –≤ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö...")

            # –ü–æ–¥—Å—á–µ—Ç –¥–æ —É–¥–∞–ª–µ–Ω–∏—è
            train_before = len(train_data)
            val_before = len(val_data)
            test_before = len(test_data)

            # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ –ª—é–±–æ–π –∏–∑ future –∫–æ–ª–æ–Ω–æ–∫
            train_data = train_data.dropna(subset=future_cols)
            val_data = val_data.dropna(subset=future_cols)
            test_data = test_data.dropna(subset=future_cols)

            if not self.disable_progress:
                self.logger.info(
                    f"  –£–¥–∞–ª–µ–Ω–æ —Å—Ç—Ä–æ–∫: Train={train_before - len(train_data)}, "
                    f"Val={val_before - len(val_data)}, Test={test_before - len(test_data)}"
                )

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è NaN
        nan_check = {
            "train": train_data.isna().sum().sum(),
            "val": val_data.isna().sum().sum(),
            "test": test_data.isna().sum().sum(),
        }

        for split, nan_count in nan_check.items():
            if nan_count > 0:
                if not self.disable_progress:
                    self.logger.warning(f"‚ö†Ô∏è  –û—Å—Ç–∞–ª–æ—Å—å {nan_count} NaN –≤ {split} –¥–∞–Ω–Ω—ã—Ö")

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if not self.disable_progress:
            self.logger.info("‚úÖ –†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ data leakage:")
            self.logger.info(f"   - Train: {len(train_data)} –∑–∞–ø–∏—Å–µ–π")
            self.logger.info(f"   - Val: {len(val_data)} –∑–∞–ø–∏—Å–µ–π")
            self.logger.info(f"   - Test: {len(test_data)} –∑–∞–ø–∏—Å–µ–π")
            self.logger.info(f"   - –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_cols)}")

            self.logger.end_stage(
                "feature_engineering_no_leakage",
                train_size=len(train_data),
                val_size=len(val_data),
                test_size=len(test_data),
            )

        return train_data, val_data, test_data

    def _add_enhanced_features(
        self, df: pd.DataFrame, all_symbols_data: Dict[str, pd.DataFrame]
    ) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è direction prediction

        Args:
            df: DataFrame —Å –±–∞–∑–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            all_symbols_data: —Å–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è cross-asset features

        Returns:
            DataFrame —Å enhanced features
        """
        try:
            from data.enhanced_features import EnhancedFeatureEngineer
        except ImportError:
            self.logger.warning(
                "‚ö†Ô∏è –ú–æ–¥—É–ª—å enhanced_features –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º enhanced features"
            )
            return df

        self.logger.info("üöÄ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ enhanced features –¥–ª—è direction prediction...")

        enhanced_engineer = EnhancedFeatureEngineer()
        enhanced_dfs = []

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Å–∏–º–≤–æ–ª
        for symbol in tqdm(
            df["symbol"].unique(),
            desc="Enhanced features",
            disable=self.disable_progress,
        ):
            symbol_data = df[df["symbol"] == symbol].copy()

            # –ü—Ä–∏–º–µ–Ω—è–µ–º enhanced features
            enhanced_data = enhanced_engineer.create_enhanced_features(
                symbol_data, all_symbols_data if len(all_symbols_data) > 1 else None
            )

            enhanced_dfs.append(enhanced_data)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        result_df = pd.concat(enhanced_dfs, ignore_index=True)

        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        original_cols = set(df.columns)
        new_cols = set(result_df.columns) - original_cols

        if new_cols:
            self.logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(new_cols)} enhanced features")

            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            categories = {
                "market_regime": [
                    col for col in new_cols if "regime" in col or "wyckoff" in col
                ],
                "microstructure": [
                    col
                    for col in new_cols
                    if any(x in col for x in ["ofi", "tick", "imbalance"])
                ],
                "cross_asset": [
                    col
                    for col in new_cols
                    if any(x in col for x in ["btc_", "sector_", "beta_"])
                ],
                "sentiment": [
                    col
                    for col in new_cols
                    if any(x in col for x in ["fear_greed", "panic", "euphoria"])
                ],
            }

            for category, cols in categories.items():
                if cols:
                    self.logger.info(f"  - {category}: {len(cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        return result_df
