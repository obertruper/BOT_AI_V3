"""
–ò–Ω–∂–µ–Ω–µ—Ä–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö - BOT_AI_V3 –≤–µ—Ä—Å–∏—è
–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –∏–∑ LLM TRANSFORM –ø—Ä–æ–µ–∫—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:
- RobustScaler –≤–º–µ—Å—Ç–æ StandardScaler
- Walk-forward validation
- Crypto-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
- –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –¥–µ–ª–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ NaN/Inf
"""

# –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
import warnings
from typing import Dict, Optional, Tuple

import numpy as np
import pandas as pd
import ta
from sklearn.preprocessing import RobustScaler
from tqdm import tqdm

warnings.filterwarnings("ignore")

# BOT_AI_V3 imports
from core.logger import setup_logger


class FeatureEngineer:
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è - BOT_AI_V3 –≤–µ—Ä—Å–∏—è"""

    def __init__(self, config: Dict):
        self.config = config
        self.logger = setup_logger(__name__)
        self.feature_config = config.get("features", {})
        self.scalers = {}
        self.process_position = (
            None  # –ü–æ–∑–∏—Ü–∏—è –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–≤ –ø—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ
        )
        self.disable_progress = False  # –§–ª–∞–≥ –¥–ª—è –æ—Ç–∫–ª—é—á–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–≤

    @staticmethod
    def safe_divide(
        numerator: pd.Series,
        denominator: pd.Series,
        fill_value=0.0,
        max_value=1000.0,
        min_denominator=1e-8,
    ) -> pd.Series:
        """–ò–°–ü–†–ê–í–õ–ï–ù–û: –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –¥–µ–ª–µ–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –º–∞–ª—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∑–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å
        safe_denominator = denominator.copy()

        # –ó–∞–º–µ–Ω—è–µ–º –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        mask_small = safe_denominator.abs() < min_denominator
        safe_denominator[mask_small] = min_denominator

        # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–ª–µ–Ω–∏–µ
        result = numerator / safe_denominator

        # –ö–ª–∏–ø–ø–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        result = result.clip(lower=-max_value, upper=max_value)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ inf –∏ nan
        result = result.replace([np.inf, -np.inf], [fill_value, fill_value])
        result = result.fillna(fill_value)

        return result

    def calculate_vwap(self, df: pd.DataFrame) -> pd.Series:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç VWAP —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏"""
        # –ë–∞–∑–æ–≤—ã–π —Ä–∞—Å—á–µ—Ç VWAP
        vwap = self.safe_divide(df["turnover"], df["volume"], fill_value=df["close"])

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: VWAP –Ω–µ –¥–æ–ª–∂–µ–Ω —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –æ—Ç close
        # –ï—Å–ª–∏ VWAP —Å–ª–∏—à–∫–æ–º –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç close (–±–æ–ª–µ–µ —á–µ–º –≤ 2 —Ä–∞–∑–∞), –∏—Å–ø–æ–ª—å–∑—É–µ–º close
        mask_invalid = (vwap < df["close"] * 0.5) | (vwap > df["close"] * 2.0)
        vwap[mask_invalid] = df["close"][mask_invalid]

        return vwap

    def create_features(
        self,
        df: pd.DataFrame,
        train_end_date: Optional[str] = None,
        use_enhanced_features: bool = False,
    ) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π

        Args:
            df: DataFrame —Å raw –¥–∞–Ω–Ω—ã–º–∏
            train_end_date: –¥–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –¥–ª—è walk-forward –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            use_enhanced_features: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è direction prediction
        """
        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏: timestamp ‚Üí datetime
        if "timestamp" in df.columns and "datetime" not in df.columns:
            df = df.copy()
            df["datetime"] = df["timestamp"]
            self.logger.debug(
                "–ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ timestamp ‚Üí datetime –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"
            )

        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏: –¥–æ–±–∞–≤–ª—è–µ–º turnover –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
        if "turnover" not in df.columns:
            df = df.copy() if df is df else df
            # turnover = volume * price (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)
            df["turnover"] = df["volume"] * df["close"]
            self.logger.debug("–°–æ–∑–¥–∞–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ turnover = volume * close")

        if not self.disable_progress:
            self.logger.info(
                f"üîß –ù–∞—á–∏–Ω–∞–µ–º feature engineering –¥–ª—è {df['symbol'].nunique()} —Å–∏–º–≤–æ–ª–æ–≤"
            )

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        self._validate_data(df)

        featured_dfs = []
        all_symbols_data = {}  # –î–ª—è enhanced features

        # –ü–µ—Ä–≤—ã–π –ø—Ä–æ—Ö–æ–¥ - –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        for symbol in df["symbol"].unique():
            symbol_data = df[df["symbol"] == symbol].copy()
            symbol_data = symbol_data.sort_values("datetime")

            symbol_data = self._create_basic_features(symbol_data)
            symbol_data = self._create_technical_indicators(symbol_data)
            symbol_data = self._create_microstructure_features(symbol_data)
            symbol_data = self._create_rally_detection_features(symbol_data)
            symbol_data = self._create_signal_quality_features(symbol_data)
            symbol_data = self._create_futures_specific_features(symbol_data)
            symbol_data = self._create_ml_optimized_features(symbol_data)
            symbol_data = self._create_temporal_features(symbol_data)
            symbol_data = self._create_target_variables(symbol_data)

            featured_dfs.append(symbol_data)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è enhanced features
            if use_enhanced_features:
                all_symbols_data[symbol] = symbol_data.copy()

        result_df = pd.concat(featured_dfs, ignore_index=True)

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: cross-asset features –Ω—É–∂–Ω—ã –≤—Å–µ —Å–∏–º–≤–æ–ª—ã, –Ω–æ –µ—Å–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ –æ–¥–Ω–æ–º—É - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        # –ï—Å–ª–∏ –≤ df –±–æ–ª—å—à–µ –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ - —Å–æ–∑–¥–∞–µ–º cross-asset features
        if df["symbol"].nunique() > 1:
            result_df = self._create_cross_asset_features(result_df)

        # –î–æ–±–∞–≤–ª—è–µ–º enhanced features –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–æ
        if use_enhanced_features:
            result_df = self._add_enhanced_features(result_df, all_symbols_data)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ NaN –∑–Ω–∞—á–µ–Ω–∏–π
        result_df = self._handle_missing_values(result_df)

        # Walk-forward –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ –¥–∞—Ç–∞ (–∏–Ω–∞—á–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç –≤ prepare_trading_data.py)
        if train_end_date:
            result_df = self._normalize_walk_forward(result_df, train_end_date)

        self._log_feature_statistics(result_df)

        if not self.disable_progress:
            self.logger.info(
                f"‚úÖ Feature engineering –∑–∞–≤–µ—Ä—à–µ–Ω. –°–æ–∑–¥–∞–Ω–æ {len(result_df.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {len(result_df)} –∑–∞–ø–∏—Å–µ–π"
            )

        return result_df

    def _validate_data(self, df: pd.DataFrame):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö"""
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–∏–ø—ã
        numeric_columns = ["open", "high", "low", "close", "volume", "turnover"]
        for col in numeric_columns:
            if col in df.columns:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø, –∑–∞–º–µ–Ω—è—è –æ—à–∏–±–∫–∏ –Ω–∞ NaN
                df[col] = pd.to_numeric(df[col], errors="coerce")
                # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                df[col] = df[col].ffill().bfill()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        if df.isnull().any().any():
            if not self.disable_progress:
                self.logger.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∞–Ω–Ω—ã—Ö")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ —Ü–µ–Ω—ã
        price_changes = df.groupby("symbol")["close"].pct_change()
        extreme_moves = abs(price_changes) > 0.15  # >15% –∑–∞ 15 –º–∏–Ω—É—Ç

        if extreme_moves.sum() > 0:
            if not self.disable_progress:
                self.logger.warning(
                    f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {extreme_moves.sum()} —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π —Ü–µ–Ω—ã"
                )

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≥—ç–ø–æ–≤ (—Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞–∑—Ä—ã–≤—ã > 2 —á–∞—Å–æ–≤)
        for symbol in df["symbol"].unique():
            symbol_data = df[df["symbol"] == symbol]
            time_diff = symbol_data["datetime"].diff()
            expected_diff = pd.Timedelta("15 minutes")
            # –°—á–∏—Ç–∞–µ–º –±–æ–ª—å—à–∏–º–∏ —Ç–æ–ª—å–∫–æ —Ä–∞–∑—Ä—ã–≤—ã –±–æ–ª—å—à–µ 2 —á–∞—Å–æ–≤ (8 –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤)
            large_gaps = time_diff > expected_diff * 8

            if large_gaps.sum() > 0:
                if not self.disable_progress:
                    self.logger.warning(
                        f"–°–∏–º–≤–æ–ª {symbol}: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ {large_gaps.sum()} –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞–∑—Ä—ã–≤–æ–≤ (> 2 —á–∞—Å–æ–≤)"
                    )

    def _create_basic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ OHLCV –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ look-ahead bias"""
        df["returns"] = np.log(df["close"] / df["close"].shift(1))

        # –î–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –∑–∞ —Ä–∞–∑–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã
        for period in [5, 10, 20]:
            df[f"returns_{period}"] = np.log(df["close"] / df["close"].shift(period))

        # –¶–µ–Ω–æ–≤—ã–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è
        df["high_low_ratio"] = df["high"] / df["low"]
        df["close_open_ratio"] = df["close"] / df["open"]

        # –ü–æ–∑–∏—Ü–∏—è –∑–∞–∫—Ä—ã—Ç–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ
        df["close_position"] = (df["close"] - df["low"]) / (
            df["high"] - df["low"] + 1e-10
        )

        # –û–±—ä–µ–º–Ω—ã–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–æ–ª—å–∫–æ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        df["volume_ratio"] = self.safe_divide(
            df["volume"],
            df["volume"].rolling(20, min_periods=20).mean(),
            fill_value=1.0,
        )
        df["turnover_ratio"] = self.safe_divide(
            df["turnover"],
            df["turnover"].rolling(20, min_periods=20).mean(),
            fill_value=1.0,
        )

        # VWAP —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ä–∞—Å—á–µ—Ç–æ–º
        df["vwap"] = self.calculate_vwap(df)

        # –ë–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π —Ä–∞—Å—á–µ—Ç close_vwap_ratio
        # –ù–æ—Ä–º–∞–ª—å–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ close/vwap –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–∫–æ–ª–æ 1.0
        # VWAP —É–∂–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω –≤ calculate_vwap()

        # –ü—Ä–æ—Å—Ç–æ–π –∏ –Ω–∞–¥–µ–∂–Ω—ã–π —Ä–∞—Å—á–µ—Ç
        df["close_vwap_ratio"] = df["close"] / df["vwap"]

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç (¬±30%)
        # –ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã –º–æ–≥—É—Ç –æ—Ç–∫–ª–æ–Ω—è—Ç—å—Å—è –æ—Ç VWAP –Ω–∞ 20-50% –≤ –ø–µ—Ä–∏–æ–¥—ã –≤—ã—Å–æ–∫–æ–π –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        df["close_vwap_ratio"] = df["close_vwap_ratio"].clip(lower=0.7, upper=1.3)

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –æ—Ç VWAP
        df["vwap_extreme_deviation"] = (
            (df["close_vwap_ratio"] < 0.85) | (df["close_vwap_ratio"] > 1.15)
        ).astype(int)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–Ω–æ–º–∞–ª–∏–∏
        # –ï—Å–ª–∏ ratio –≤—Å–µ –µ—â–µ –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ —Ä–∞–∑—É–º–Ω—ã–µ –ø—Ä–µ–¥–µ–ª—ã, –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ 1.0
        mask_invalid = (df["close_vwap_ratio"] < 0.95) | (df["close_vwap_ratio"] > 1.05)
        if mask_invalid.sum() > 0:
            self.logger.debug(
                f"–ó–∞–º–µ–Ω–µ–Ω–æ {mask_invalid.sum()} –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö close_vwap_ratio –Ω–∞ 1.0"
            )
            df.loc[mask_invalid, "close_vwap_ratio"] = 1.0

        return df

    def _create_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã"""
        tech_config = self.feature_config.get("technical", [])

        # SMA
        sma_config = next((c for c in tech_config if c.get("name") == "sma"), {})
        if sma_config:
            for period in sma_config.get("periods", [20, 50]):
                df[f"sma_{period}"] = ta.trend.sma_indicator(df["close"], period)
                df[f"close_sma_{period}_ratio"] = df["close"] / df[f"sma_{period}"]

        # EMA
        ema_config = next((c for c in tech_config if c.get("name") == "ema"), {})
        if ema_config:
            for period in ema_config.get("periods", [12, 26]):
                df[f"ema_{period}"] = ta.trend.ema_indicator(df["close"], period)
                df[f"close_ema_{period}_ratio"] = df["close"] / df[f"ema_{period}"]

        # RSI
        rsi_config = next((c for c in tech_config if c.get("name") == "rsi"), {})
        if rsi_config:
            df["rsi"] = ta.momentum.RSIIndicator(
                df["close"], window=rsi_config.get("period", 14)
            ).rsi()

            df["rsi_oversold"] = (df["rsi"] < 30).astype(int)
            df["rsi_overbought"] = (df["rsi"] > 70).astype(int)

        # MACD
        macd_config = next((c for c in tech_config if c.get("name") == "macd"), {})
        if macd_config:
            macd = ta.trend.MACD(
                df["close"],
                window_slow=macd_config.get("slow", 26),
                window_fast=macd_config.get("fast", 12),
                window_sign=macd_config.get("signal", 9),
            )
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º MACD –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ü–µ–Ω—ã –¥–ª—è —Å—Ä–∞–≤–Ω–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –∞–∫—Ç–∏–≤–∞–º–∏
            # MACD –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–º –¥–ª—è –¥–æ—Ä–æ–≥–∏—Ö –∞–∫—Ç–∏–≤–æ–≤
            df["macd"] = macd.macd() / df["close"] * 100  # –í –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö –æ—Ç —Ü–µ–Ω—ã
            df["macd_signal"] = macd.macd_signal() / df["close"] * 100
            df["macd_diff"] = macd.macd_diff() / df["close"] * 100

        # Bollinger Bands
        bb_config = next(
            (c for c in tech_config if c.get("name") == "bollinger_bands"), {}
        )
        if bb_config:
            bb = ta.volatility.BollingerBands(
                df["close"],
                window=bb_config.get("period", 20),
                window_dev=bb_config.get("std_dev", 2),
            )
            df["bb_high"] = bb.bollinger_hband()
            df["bb_low"] = bb.bollinger_lband()
            df["bb_middle"] = bb.bollinger_mavg()
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: bb_width –∫–∞–∫ –ø—Ä–æ—Ü–µ–Ω—Ç –æ—Ç —Ü–µ–Ω—ã
            df["bb_width"] = self.safe_divide(
                df["bb_high"] - df["bb_low"],
                df["close"],
                fill_value=0.02,  # 2% –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                max_value=0.5,  # –ú–∞–∫—Å–∏–º—É–º 50% –æ—Ç —Ü–µ–Ω—ã
            )

            # –ò–°–ü–†–ê–í–õ–ï–ù–û: bb_position —Ç–µ–ø–µ—Ä—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–±—Å–æ–ª—é—Ç–Ω–æ–π —à–∏—Ä–∏–Ω—ã
            # bb_position –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —Ü–µ–Ω–∞ –≤–Ω—É—Ç—Ä–∏ –∫–∞–Ω–∞–ª–∞ Bollinger
            bb_range = df["bb_high"] - df["bb_low"]
            df["bb_position"] = self.safe_divide(
                df["close"] - df["bb_low"],
                bb_range,
                fill_value=0.5,
                max_value=2.0,  # –ü–æ–∑–≤–æ–ª—è–µ–º –≤—ã—Ö–æ–¥—ã –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ—Ä—ã–≤–æ–≤
            )

            # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –ø—Ä–æ—Ä—ã–≤–æ–≤ –ü–ï–†–ï–î –∫–ª–∏–ø–ø–∏–Ω–≥–æ–º
            df["bb_breakout_upper"] = (df["bb_position"] > 1).astype(int)
            df["bb_breakout_lower"] = (df["bb_position"] < 0).astype(int)
            df["bb_breakout_strength"] = (
                np.abs(df["bb_position"] - 0.5) * 2
            )  # –°–∏–ª–∞ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –æ—Ç —Ü–µ–Ω—Ç—Ä–∞

            # –¢–µ–ø–µ—Ä—å –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            df["bb_position"] = df["bb_position"].clip(0, 1)

        # ATR
        atr_config = next((c for c in tech_config if c.get("name") == "atr"), {})
        if atr_config:
            df["atr"] = ta.volatility.AverageTrueRange(
                df["high"], df["low"], df["close"], window=atr_config.get("period", 14)
            ).average_true_range()

            # ATR –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö –æ—Ç —Ü–µ–Ω—ã —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            df["atr_pct"] = self.safe_divide(
                df["atr"],
                df["close"],
                fill_value=0.01,  # 1% –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                max_value=0.2,  # –ú–∞–∫—Å–∏–º—É–º 20% –æ—Ç —Ü–µ–Ω—ã
            )

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è crypto
        self._add_crypto_specific_indicators(df)

        return df

    def _add_crypto_specific_indicators(self, df: pd.DataFrame):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∫—Ä–∏–ø—Ç–æ—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã"""

        # Stochastic
        stoch = ta.momentum.StochasticOscillator(
            df["high"], df["low"], df["close"], window=14, smooth_window=3
        )
        df["stoch_k"] = stoch.stoch()
        df["stoch_d"] = stoch.stoch_signal()

        # ADX
        adx = ta.trend.ADXIndicator(df["high"], df["low"], df["close"])
        df["adx"] = adx.adx()
        df["adx_pos"] = adx.adx_pos()
        df["adx_neg"] = adx.adx_neg()

        # Parabolic SAR
        psar = ta.trend.PSARIndicator(df["high"], df["low"], df["close"])
        df["psar"] = psar.psar()
        # –í–º–µ—Å—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö psar_up –∏ psar_down, —Å–æ–∑–¥–∞–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        df["psar_trend"] = (df["close"] > df["psar"]).astype(float)

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ PSAR –ø–æ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        # –î–µ–ª–µ–Ω–∏–µ –Ω–∞ ATR –¥–µ–ª–∞–µ—Ç –º–µ—Ç—Ä–∏–∫—É —Å—Ä–∞–≤–Ω–∏–º–æ–π –º–µ–∂–¥—É –∞–∫—Ç–∏–≤–∞–º–∏
        df["psar_distance"] = (df["close"] - df["psar"]) / df["close"]
        if "atr" in df.columns:
            df["psar_distance_normalized"] = (df["close"] - df["psar"]) / (
                df["atr"] + 1e-10
            )
        else:
            df["psar_distance_normalized"] = df["psar_distance"]

        # Volume Weighted Moving Average (VWMA)
        df["vwma_20"] = (df["close"] * df["volume"]).rolling(20).sum() / df[
            "volume"
        ].rolling(20).sum()
        df["close_vwma_ratio"] = df["close"] / df["vwma_20"]

        # Money Flow Index (MFI) - –æ–±—ä–µ–º–Ω—ã–π –æ—Å—Ü–∏–ª–ª—è—Ç–æ—Ä
        try:
            mfi = ta.volume.MFIIndicator(
                high=df["high"],
                low=df["low"],
                close=df["close"],
                volume=df["volume"],
                window=14,
            )
            df["mfi"] = mfi.money_flow_index()
            df["mfi_overbought"] = (df["mfi"] > 80).astype(int)
            df["mfi_oversold"] = (df["mfi"] < 20).astype(int)
        except:
            pass

        # Commodity Channel Index (CCI)
        try:
            cci = ta.trend.CCIIndicator(
                high=df["high"], low=df["low"], close=df["close"], window=20
            )
            df["cci"] = cci.cci()
            df["cci_overbought"] = (df["cci"] > 100).astype(int)
            df["cci_oversold"] = (df["cci"] < -100).astype(int)
        except:
            pass

        # Williams %R
        try:
            williams = ta.momentum.WilliamsRIndicator(
                high=df["high"], low=df["low"], close=df["close"], lbp=14
            )
            df["williams_r"] = williams.williams_r()
        except:
            pass

    def _create_microstructure_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–∏–∑–Ω–∞–∫–∏ –º–∏–∫—Ä–æ—Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ä—ã–Ω–∫–∞"""
        # –°–ø—Ä–µ–¥ high-low
        df["hl_spread"] = self.safe_divide(
            df["high"] - df["low"], df["close"], fill_value=0.0
        )
        df["hl_spread_ma"] = df["hl_spread"].rolling(20).mean()

        # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏ –æ–±—ä–µ–º
        df["price_direction"] = np.sign(df["close"] - df["open"])
        df["directed_volume"] = df["volume"] * df["price_direction"]
        df["volume_imbalance"] = (
            df["directed_volume"].rolling(10).sum() / df["volume"].rolling(10).sum()
        )

        # –¶–µ–Ω–æ–≤–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ - —É–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º dollar volume –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏
        df["dollar_volume"] = df["volume"] * df["close"]
        # –ò–°–ü–†–ê–í–õ–ï–ù–û v3: –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º price_impact –¥–ª—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç
        # –≥–¥–µ dollar_volume –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç $10K –¥–æ $100M+
        # log10($10K) ‚âà 4, log10($1M) ‚âà 6, log10($100M) ‚âà 8
        # –£–º–Ω–æ–∂–∞–µ–º –Ω–∞ 100 –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∑–Ω–∞—á–∏–º—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π price_impact
        df["price_impact"] = self.safe_divide(
            df["returns"].abs() * 100,  # –£–º–Ω–æ–∂–∞–µ–º –Ω–∞ 100 –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞
            np.log10(df["dollar_volume"] + 100),  # log10 –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞
            fill_value=0.0,
            max_value=0.1,  # –õ–∏–º–∏—Ç –¥–ª—è –Ω–æ–≤–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞
        )

        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ —Å –ª–æ–≥–∞—Ä–∏—Ñ–º–æ–º –æ–±—ä–µ–º–∞
        df["price_impact_log"] = self.safe_divide(
            df["returns"].abs(),
            np.log(df["volume"] + 10),  # –£–≤–µ–ª–∏—á–µ–Ω —Å–¥–≤–∏–≥ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            fill_value=0.0,
            max_value=0.01,
        )

        return df

    def _create_rally_detection_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ä–∞–ª–ª–∏ –∏ —Ä–∞–∑–≤–æ—Ä–æ—Ç–æ–≤"""
        # –ú–æ–º–µ–Ω—Ç—É–º –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–∞—Ö
        df["momentum_24h"] = (
            df["close"] / df["close"].shift(96) - 1
        )  # 96 —Å–≤–µ—á–µ–π = 24 —á–∞—Å–∞
        df["momentum_4h"] = (
            df["close"] / df["close"].shift(16) - 1
        )  # 16 —Å–≤–µ—á–µ–π = 4 —á–∞—Å–∞
        df["momentum_1h"] = df["close"] / df["close"].shift(4) - 1  # 4 —Å–≤–µ—á–∏ = 1 —á–∞—Å

        # –î–µ—Ç–µ–∫—Ü–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Ä–∞–∑–≤–æ—Ä–æ—Ç–∞
        df["higher_highs"] = (
            (df["high"] > df["high"].shift(1))
            & (df["high"].shift(1) > df["high"].shift(2))
        ).astype(int)

        df["lower_lows"] = (
            (df["low"] < df["low"].shift(1)) & (df["low"].shift(1) < df["low"].shift(2))
        ).astype(int)

        # –°–∫–æ—Ä–æ—Å—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è –æ–±—ä–µ–º–∞
        df["volume_acceleration"] = df["volume"] / df["volume"].shift(1) - 1
        df["volume_trend"] = (
            df["volume"].rolling(5).mean() > df["volume"].rolling(20).mean()
        ).astype(int)

        return df

    def _create_signal_quality_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–∏–∑–Ω–∞–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–æ—Ä–≥–æ–≤–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞"""
        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–∞—Ö
        df["volatility_5m"] = df["returns"].rolling(20).std()
        df["volatility_1h"] = df["returns"].rolling(240).std()
        df["volatility_ratio"] = df["volatility_5m"] / (df["volatility_1h"] + 1e-10)

        # –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        df["trend_consistency"] = (
            df["returns"].rolling(10).apply(lambda x: (x > 0).sum() / len(x))
        )

        # –†–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ –∫–ª—é—á–µ–≤—ã—Ö —É—Ä–æ–≤–Ω–µ–π
        df["distance_to_high_20"] = (df["high"].rolling(20).max() - df["close"]) / df[
            "close"
        ]
        df["distance_to_low_20"] = (df["close"] - df["low"].rolling(20).min()) / df[
            "close"
        ]

        # –ü–æ–∑–∏—Ü–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ
        range_20 = df["high"].rolling(20).max() - df["low"].rolling(20).min()
        df["position_in_range_20"] = self.safe_divide(
            df["close"] - df["low"].rolling(20).min(), range_20, fill_value=0.5
        )

        return df

    def _create_futures_specific_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è —Ñ—å—é—á–µ—Ä—Å–æ–≤"""
        # Funding rate –ø—Ä–æ–∫—Å–∏ (–Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–º–∏–∏ —Å–ø–æ—Ç/—Ñ—å—é—á–µ—Ä—Å)
        # –î–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ä–µ–º –∫–∞–∫ –ø—Ä–æ–∫—Å–∏ –¥–ª—è –∏–Ω—Ç–µ—Ä–µ—Å–∞
        df["funding_rate_proxy"] = self.safe_divide(
            df["volume"] - df["volume"].rolling(96).mean(),
            df["volume"].rolling(96).mean(),
            fill_value=0.0,
        )

        # Open Interest –ø—Ä–æ–∫—Å–∏
        df["oi_proxy"] = df["volume"].rolling(24).sum()
        df["oi_change"] = df["oi_proxy"].pct_change()

        # –õ–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å –º–µ—Ç—Ä–∏–∫–∏
        df["liquidity_score"] = np.log(df["volume"] + 1) / np.log(
            df["hl_spread"] + 1e-6
        )

        return df

    def _create_ml_optimized_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ ML-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è 2024-2025"""
        if not self.disable_progress:
            self.logger.info("–°–æ–∑–¥–∞–Ω–∏–µ ML-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        # 1. Hurst Exponent - –º–µ—Ä–∞ –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Ä—ã–Ω–∫–∞
        # >0.5 = —Ç—Ä–µ–Ω–¥, <0.5 = –≤–æ–∑–≤—Ä–∞—Ç –∫ —Å—Ä–µ–¥–Ω–µ–º—É, ~0.5 = —Å–ª—É—á–∞–π–Ω–æ–µ –±–ª—É–∂–¥–∞–Ω–∏–µ
        def hurst_exponent(ts, max_lag=20):
            """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç—ã –•–µ—Ä—Å—Ç–∞"""
            lags = range(2, min(max_lag, len(ts) // 2))
            tau = []

            for lag in lags:
                pp = np.array(ts[:-lag])
                pn = np.array(ts[lag:])
                diff = pn - pp
                tau.append(np.sqrt(np.nanmean(diff**2)))

            if len(tau) > 0 and all(t > 0 for t in tau):
                poly = np.polyfit(np.log(lags), np.log(tau), 1)
                return poly[0] * 2.0
            return 0.5

        # –ü—Ä–∏–º–µ–Ω—è–µ–º Hurst –¥–ª—è close —Å –æ–∫–Ω–æ–º 50
        df["hurst_exponent"] = (
            df["close"]
            .rolling(50)
            .apply(lambda x: hurst_exponent(x) if len(x) == 50 else 0.5)
        )

        # 2. Market Efficiency Ratio - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã
        # –í—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è = —Å–∏–ª—å–Ω—ã–π —Ç—Ä–µ–Ω–¥, –Ω–∏–∑–∫–∏–µ = –±–æ–∫–æ–≤–∏–∫
        df["efficiency_ratio"] = self.safe_divide(
            (df["close"] - df["close"].shift(20)).abs(),
            df["close"].diff().abs().rolling(20).sum(),
        )

        # 3. Regime Detection Features
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä—ã–Ω–æ—á–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ (—Ç—Ä–µ–Ω–¥/—Ñ–ª—ç—Ç/–≤—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å)
        returns = df["close"].pct_change()

        # Realized volatility
        df["realized_vol_5m"] = returns.rolling(20).std() * np.sqrt(20)
        df["realized_vol_15m"] = returns.rolling(60).std() * np.sqrt(60)
        df["realized_vol_1h"] = returns.rolling(240).std() * np.sqrt(240)

        # GARCH-–ø–æ–¥–æ–±–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
        df["garch_vol"] = returns.rolling(20).apply(
            lambda x: np.sqrt(0.94 * x.var() + 0.06 * x.iloc[-1] ** 2)
            if len(x) > 0
            else 0
        )

        # –†–µ–∂–∏–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        if "atr" in df.columns:
            atr_q25 = df["atr"].rolling(1000).quantile(0.25)
            atr_q75 = df["atr"].rolling(1000).quantile(0.75)
            df["vol_regime"] = 0  # –ù–æ—Ä–º–∞–ª—å–Ω–∞—è
            df.loc[df["atr"] < atr_q25, "vol_regime"] = -1  # –ù–∏–∑–∫–∞—è
            df.loc[df["atr"] > atr_q75, "vol_regime"] = 1  # –í—ã—Å–æ–∫–∞—è

        # 4. Information-theoretic features
        # –≠–Ω—Ç—Ä–æ–ø–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π
        def shannon_entropy(series, bins=10):
            """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –®–µ–Ω–Ω–æ–Ω–∞"""
            if len(series) < bins:
                return 0
            counts, _ = np.histogram(series, bins=bins)
            probs = counts / counts.sum()
            probs = probs[probs > 0]
            return -np.sum(probs * np.log(probs))

        df["return_entropy"] = returns.rolling(100).apply(lambda x: shannon_entropy(x))

        # 5. Autocorrelation features
        # –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ª–∞–≥–∞—Ö
        df["returns_ac_1"] = returns.rolling(50).apply(
            lambda x: x.autocorr(lag=1) if len(x) > 1 else 0
        )
        df["returns_ac_5"] = returns.rolling(50).apply(
            lambda x: x.autocorr(lag=5) if len(x) > 5 else 0
        )
        df["returns_ac_10"] = returns.rolling(50).apply(
            lambda x: x.autocorr(lag=10) if len(x) > 10 else 0
        )

        # 6. Jump detection
        # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø—Ä—ã–∂–∫–æ–≤ –≤ —Ü–µ–Ω–µ
        df["price_jump"] = (returns.abs() > returns.rolling(100).std() * 3).astype(int)

        df["jump_intensity"] = df["price_jump"].rolling(50).mean()

        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        ml_features = [
            "hurst_exponent",
            "efficiency_ratio",
            "realized_vol_5m",
            "realized_vol_15m",
            "realized_vol_1h",
            "garch_vol",
            "return_entropy",
            "returns_ac_1",
            "returns_ac_5",
            "returns_ac_10",
            "price_jump",
            "jump_intensity",
        ]

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
        for feature in ml_features:
            if feature in df.columns:
                df[feature] = df[feature].fillna(method="ffill").fillna(0)

        return df

    def _create_temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        df["hour"] = df["datetime"].dt.hour
        df["minute"] = df["datetime"].dt.minute

        # –¶–∏–∫–ª–∏—á–µ—Å–∫–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
        df["hour_sin"] = np.sin(2 * np.pi * df["hour"] / 24)
        df["hour_cos"] = np.cos(2 * np.pi * df["hour"] / 24)

        df["dayofweek"] = df["datetime"].dt.dayofweek
        df["is_weekend"] = (df["dayofweek"] >= 5).astype(int)

        df["dow_sin"] = np.sin(2 * np.pi * df["dayofweek"] / 7)
        df["dow_cos"] = np.cos(2 * np.pi * df["dayofweek"] / 7)

        df["day"] = df["datetime"].dt.day
        df["month"] = df["datetime"].dt.month

        df["month_sin"] = np.sin(2 * np.pi * df["month"] / 12)
        df["month_cos"] = np.cos(2 * np.pi * df["month"] / 12)

        # –¢–æ—Ä–≥–æ–≤—ã–µ —Å–µ—Å—Å–∏–∏
        df["asian_session"] = ((df["hour"] >= 0) & (df["hour"] < 8)).astype(int)
        df["european_session"] = ((df["hour"] >= 7) & (df["hour"] < 16)).astype(int)
        df["american_session"] = ((df["hour"] >= 13) & (df["hour"] < 22)).astype(int)

        # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–µ—Å—Å–∏–π
        df["session_overlap"] = (
            (df["asian_session"] + df["european_session"] + df["american_session"]) > 1
        ).astype(int)

        return df

    def _create_cross_asset_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ö—Ä–æ—Å—Å-–∞–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        if not self.disable_progress:
            self.logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–æ—Å—Å-–∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        # BTC –∫–∞–∫ –±–∞–∑–æ–≤—ã–π –∞–∫—Ç–∏–≤
        btc_data = df[df["symbol"] == "BTCUSDT"][
            ["datetime", "close", "returns"]
        ].copy()
        if len(btc_data) > 0:
            btc_data.rename(
                columns={"close": "btc_close", "returns": "btc_returns"}, inplace=True
            )

            df = df.merge(btc_data, on="datetime", how="left")

            # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å BTC
            for symbol in df["symbol"].unique():
                if symbol != "BTCUSDT":
                    mask = df["symbol"] == symbol
                    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º min_periods –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
                    df.loc[mask, "btc_correlation"] = (
                        df.loc[mask, "returns"]
                        .rolling(window=96, min_periods=50)
                        .corr(df.loc[mask, "btc_returns"])
                    )

            df.loc[df["symbol"] == "BTCUSDT", "btc_correlation"] = 1.0

            # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è —Å–∏–ª–∞ –∫ BTC
            df["relative_strength_btc"] = df["close"] / df["btc_close"]
            df["rs_btc_ma"] = df.groupby("symbol")["relative_strength_btc"].transform(
                lambda x: x.rolling(20, min_periods=10).mean()
            )

            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –∑–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è BTC-—Å–≤—è–∑–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            df["btc_close"] = (
                df["btc_close"].fillna(method="ffill").fillna(method="bfill")
            )
            df["btc_returns"] = df["btc_returns"].fillna(0.0)
            df["btc_correlation"] = df["btc_correlation"].fillna(
                0.5
            )  # –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
            df["relative_strength_btc"] = df["relative_strength_btc"].fillna(1.0)
            df["rs_btc_ma"] = df["rs_btc_ma"].fillna(1.0)
        else:
            # –ó–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –µ—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö BTC
            df["btc_close"] = 0
            df["btc_returns"] = 0
            df["btc_correlation"] = 0
            df["relative_strength_btc"] = 0
            df["rs_btc_ma"] = 0

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ–∫—Ç–æ—Ä–∞
        defi_tokens = ["AAVEUSDT", "UNIUSDT", "CAKEUSDT", "DYDXUSDT"]
        layer1_tokens = ["ETHUSDT", "SOLUSDT", "AVAXUSDT", "DOTUSDT", "NEARUSDT"]
        meme_tokens = [
            "DOGEUSDT",
            "FARTCOINUSDT",
            "MELANIAUSDT",
            "TRUMPUSDT",
            "POPCATUSDT",
            "PNUTUSDT",
            "ZEREBROUSDT",
            "WIFUSDT",
        ]

        df["sector"] = "other"
        df.loc[df["symbol"].isin(defi_tokens), "sector"] = "defi"
        df.loc[df["symbol"].isin(layer1_tokens), "sector"] = "layer1"
        df.loc[df["symbol"].isin(meme_tokens), "sector"] = "meme"
        df.loc[df["symbol"] == "BTCUSDT", "sector"] = "btc"

        # –°–µ–∫—Ç–æ—Ä–Ω—ã–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
        df["sector_returns"] = df.groupby(["datetime", "sector"])["returns"].transform(
            "mean"
        )

        # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –∫ —Å–µ–∫—Ç–æ—Ä—É
        df["relative_to_sector"] = df["returns"] - df["sector_returns"]

        # –†–∞–Ω–∫ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
        df["returns_rank"] = df.groupby("datetime")["returns"].rank(pct=True)

        return df

    def _create_target_variables(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –ë–ï–ó –£–¢–ï–ß–ï–ö –î–ê–ù–ù–´–• - –≤–µ—Ä—Å–∏—è 4.0"""
        if not self.disable_progress:
            self.logger.info("üéØ –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö v4.0 (–±–µ–∑ —É—Ç–µ—á–µ–∫)...")

        # –ü–µ—Ä–∏–æ–¥—ã –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –±—É–¥—É—â–∏—Ö –≤–æ–∑–≤—Ä–∞—Ç–æ–≤ (–≤ —Å–≤–µ—á–∞—Ö –ø–æ 15 –º–∏–Ω—É—Ç)
        return_periods = {
            "15m": 1,  # 15 –º–∏–Ω—É—Ç
            "1h": 4,  # 1 —á–∞—Å
            "4h": 16,  # 4 —á–∞—Å–∞
            "12h": 48,  # 12 —á–∞—Å–æ–≤
        }

        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        # –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–´ –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–∏–≥–Ω–∞–ª–æ–≤
        direction_thresholds = {
            "15m": 0.0015,  # 0.15% - —É–º–µ–Ω—å—à–∞–µ—Ç —à—É–º –æ—Ç –º–µ–ª–∫–∏—Ö –¥–≤–∏–∂–µ–Ω–∏–π
            "1h": 0.003,  # 0.3% - —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ –∫–æ–ª–µ–±–∞–Ω–∏—è
            "4h": 0.007,  # 0.7% - —Ñ–æ–∫—É—Å –Ω–∞ –∑–Ω–∞—á–∏–º—ã—Ö –¥–≤–∏–∂–µ–Ω–∏—è—Ö
            "12h": 0.01,  # 1% - –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã
        }

        # –£—Ä–æ–≤–Ω–∏ –ø—Ä–∏–±—ã–ª–∏ –¥–ª—è –±–∏–Ω–∞—Ä–Ω—ã—Ö —Ü–µ–ª–µ–≤—ã—Ö
        profit_levels = {
            "1pct_4h": (0.01, 16),  # 1% –∑–∞ 4 —á–∞—Å–∞
            "2pct_4h": (0.02, 16),  # 2% –∑–∞ 4 —á–∞—Å–∞
            "3pct_12h": (0.03, 48),  # 3% –∑–∞ 12 —á–∞—Å–æ–≤
            "5pct_12h": (0.05, 48),  # 5% –∑–∞ 12 —á–∞—Å–æ–≤
        }

        # A. –ë–∞–∑–æ–≤—ã–µ –≤–æ–∑–≤—Ä–∞—Ç—ã (4)
        for period_name, n_candles in return_periods.items():
            df[f"future_return_{period_name}"] = df.groupby("symbol")[
                "close"
            ].transform(lambda x: x.shift(-n_candles) / x - 1)

        # B. –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è (4)
        for period_name in return_periods.keys():
            future_return = df[f"future_return_{period_name}"]
            threshold = direction_thresholds[period_name]

            df[f"direction_{period_name}"] = pd.cut(
                future_return,
                bins=[-np.inf, -threshold, threshold, np.inf],
                labels=["DOWN", "FLAT", "UP"],
            )

        # C. –î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π –ø—Ä–∏–±—ã–ª–∏ LONG (4) - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ shift –¥–ª—è –±—É–¥—É—â–∏—Ö —Ü–µ–Ω
        for level_name, (profit_threshold, n_candles) in profit_levels.items():
            # –î–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∏–≥–Ω–µ—Ç –ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ –Ω—É–∂–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è
            max_future_returns = pd.DataFrame()
            for i in range(1, n_candles + 1):
                future_high = df.groupby("symbol")["high"].transform(
                    lambda x: x.shift(-i)
                )
                future_return = future_high / df["close"] - 1
                max_future_returns[f"return_{i}"] = future_return

            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π return –∑–∞ –ø–µ—Ä–∏–æ–¥
            max_return = max_future_returns.max(axis=1)
            df[f"long_will_reach_{level_name}"] = (
                max_return >= profit_threshold
            ).astype(int)

        # D. –î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π –ø—Ä–∏–±—ã–ª–∏ SHORT (4)
        for level_name, (profit_threshold, n_candles) in profit_levels.items():
            # –î–ª—è SHORT: –ø—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é —Ü–µ–Ω—É
            min_future_returns = pd.DataFrame()
            for i in range(1, n_candles + 1):
                future_low = df.groupby("symbol")["low"].transform(
                    lambda x: x.shift(-i)
                )
                future_return = df["close"] / future_low - 1  # –î–ª—è SHORT –∏–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º
                min_future_returns[f"return_{i}"] = future_return

            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π return –¥–ª—è SHORT –∑–∞ –ø–µ—Ä–∏–æ–¥
            max_return = min_future_returns.max(axis=1)
            df[f"short_will_reach_{level_name}"] = (
                max_return >= profit_threshold
            ).astype(int)

        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if not self.disable_progress:
            self.logger.info("  ‚úÖ –°–æ–∑–¥–∞–Ω–æ 20 —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –±–µ–∑ —É—Ç–µ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö")
            for period in ["15m", "1h", "4h", "12h"]:
                if f"direction_{period}" in df.columns:
                    dist = df[f"direction_{period}"].value_counts(normalize=True) * 100
                    self.logger.info(
                        f"     {period}: UP={dist.get('UP', 0):.1f}%, DOWN={dist.get('DOWN', 0):.1f}%, FLAT={dist.get('FLAT', 0):.1f}%"
                    )

        return df

    def _normalize_walk_forward(
        self, df: pd.DataFrame, train_end_date: str
    ) -> pd.DataFrame:
        """Walk-forward –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RobustScaler"""
        if not self.disable_progress:
            self.logger.info(
                f"üìä Walk-forward –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å –≥—Ä–∞–Ω–∏—Ü–µ–π: {train_end_date}"
            )

        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        train_mask = df["datetime"] <= pd.to_datetime(train_end_date)
        test_mask = ~train_mask

        # –°—Ç–æ–ª–±—Ü—ã –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        exclude_cols = [
            "id",
            "symbol",
            "timestamp",
            "datetime",
            "sector",
            "open",
            "high",
            "low",
            "close",
            "volume",
            "turnover",
        ]

        # –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        target_cols = [
            col
            for col in df.columns
            if col.startswith(
                ("future_", "direction_", "long_will_reach_", "short_will_reach_")
            )
        ]
        exclude_cols.extend(target_cols)

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (—É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã)
        time_cols = [
            "hour",
            "minute",
            "dayofweek",
            "day",
            "month",
            "is_weekend",
            "asian_session",
            "european_session",
            "american_session",
            "session_overlap",
        ]
        exclude_cols.extend(time_cols)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        feature_cols = [col for col in numeric_cols if col not in exclude_cols]

        if not self.disable_progress:
            self.logger.info(
                f"–ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º {len(feature_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {df['symbol'].nunique()} —Å–∏–º–≤–æ–ª–æ–≤"
            )

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        for symbol in df["symbol"].unique():
            symbol_mask = df["symbol"] == symbol
            train_symbol_mask = train_mask & symbol_mask
            test_symbol_mask = test_mask & symbol_mask

            if train_symbol_mask.sum() == 0:
                continue

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RobustScaler –¥–ª—è —Å–∏–º–≤–æ–ª–∞
            if symbol not in self.scalers:
                self.scalers[symbol] = RobustScaler()

            # –ü–æ–ª—É—á–∞–µ–º train –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è scaler
            train_data = df.loc[train_symbol_mask, feature_cols].dropna()

            if len(train_data) > 0:
                # –û–±—É—á–∞–µ–º scaler —Ç–æ–ª—å–∫–æ –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö
                self.scalers[symbol].fit(train_data)

                # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ train –¥–∞–Ω–Ω—ã–º
                if train_symbol_mask.sum() > 0:
                    train_to_scale = df.loc[train_symbol_mask, feature_cols].fillna(0)
                    df.loc[train_symbol_mask, feature_cols] = self.scalers[
                        symbol
                    ].transform(train_to_scale)

                # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ test –¥–∞–Ω–Ω—ã–º
                if test_symbol_mask.sum() > 0:
                    test_to_scale = df.loc[test_symbol_mask, feature_cols].fillna(0)
                    df.loc[test_symbol_mask, feature_cols] = self.scalers[
                        symbol
                    ].transform(test_to_scale)

        if not self.disable_progress:
            self.logger.info("‚úÖ Walk-forward –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

        return df

    def _handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        if not self.disable_progress:
            self.logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        info_cols = [
            "id",
            "symbol",
            "timestamp",
            "datetime",
            "open",
            "high",
            "low",
            "close",
            "volume",
            "turnover",
        ]

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Å–∏–º–≤–æ–ª–∞–º –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        processed_dfs = []

        for symbol in df["symbol"].unique():
            symbol_data = df[df["symbol"] == symbol].copy()

            # –î–ª—è –∫–∞–∂–¥–æ–π –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–∏–º–µ–Ω—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –º–µ—Ç–æ–¥ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è
            for col in symbol_data.columns:
                if col in info_cols:
                    continue

                if symbol_data[col].isna().any():
                    # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö (Categorical dtype)
                    if hasattr(symbol_data[col], "cat"):
                        # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–ª–∏ 'FLAT'
                        if "direction" in col:
                            symbol_data[col] = symbol_data[col].fillna("FLAT")
                        else:
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥—É (–Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)
                            mode = symbol_data[col].mode()
                            if len(mode) > 0:
                                symbol_data[col] = symbol_data[col].fillna(mode.iloc[0])
                    # –î–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º forward fill
                    elif any(
                        indicator in col
                        for indicator in ["sma", "ema", "rsi", "macd", "bb_", "adx"]
                    ):
                        symbol_data[col] = symbol_data[col].ffill()
                    # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º 0
                    else:
                        symbol_data[col] = symbol_data[col].fillna(0)

            # –£–¥–∞–ª—è–µ–º –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –º–æ–≥—É—Ç –±—ã—Ç—å NaN –∏–∑-–∑–∞ —Ä–∞—Å—á–µ—Ç–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            # –ù–∞—Ö–æ–¥–∏–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø–µ—Ä–∏–æ–¥ —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            max_period = 50  # SMA50 —Ç—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º—É–º 50 –ø–µ—Ä–∏–æ–¥–æ–≤
            symbol_data = symbol_data.iloc[max_period:].copy()

            processed_dfs.append(symbol_data)

        result_df = pd.concat(processed_dfs, ignore_index=True)

        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        nan_count = result_df.isna().sum().sum()
        if nan_count > 0:
            if not self.disable_progress:
                self.logger.warning(
                    f"–û—Å—Ç–∞–ª–∏—Å—å {nan_count} NaN –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏"
                )
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è NaN
            for col in result_df.columns:
                if result_df[col].isna().any():
                    # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
                    if hasattr(result_df[col], "cat"):
                        if "direction" in col:
                            result_df[col] = result_df[col].fillna("FLAT")
                        else:
                            mode = result_df[col].mode()
                            if len(mode) > 0:
                                result_df[col] = result_df[col].fillna(mode.iloc[0])
                    # –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
                    elif pd.api.types.is_numeric_dtype(result_df[col]):
                        result_df[col] = result_df[col].fillna(0)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        numeric_cols = result_df.select_dtypes(include=[np.number]).columns
        inf_count = np.isinf(result_df[numeric_cols]).sum().sum()
        if inf_count > 0:
            if not self.disable_progress:
                self.logger.warning(
                    f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã {inf_count} –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π, –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ –∫–æ–Ω–µ—á–Ω—ã–µ"
                )
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ó–∞–º–µ–Ω—è–µ–º –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç–∏ –Ω–∞ 99-–π –ø–µ—Ä—Å–µ–Ω—Ç–∏–ª—å –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–ª–æ–Ω–∫–∏
            for col in numeric_cols:
                if np.isinf(result_df[col]).any():
                    # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä—Å–µ–Ω—Ç–∏–ª–∏ –Ω–∞ –∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö
                    finite_vals = result_df[col][np.isfinite(result_df[col])]
                    if len(finite_vals) > 0:
                        p99 = finite_vals.quantile(0.99)
                        p1 = finite_vals.quantile(0.01)
                        result_df[col] = result_df[col].replace(
                            [np.inf, -np.inf], [p99, p1]
                        )
                    else:
                        result_df[col] = result_df[col].replace(
                            [np.inf, -np.inf], [0, 0]
                        )

        if not self.disable_progress:
            self.logger.info(
                f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {len(result_df)} –∑–∞–ø–∏—Å–µ–π"
            )
        return result_df

    def _log_feature_statistics(self, df: pd.DataFrame):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º"""
        if self.disable_progress:
            return

        total_features = len(df.columns)
        numeric_features = len(df.select_dtypes(include=[np.number]).columns)
        categorical_features = len(
            df.select_dtypes(include=["object", "category"]).columns
        )

        self.logger.info("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        self.logger.info(f"   - –í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {total_features}")
        self.logger.info(f"   - –ß–∏—Å–ª–æ–≤—ã—Ö: {numeric_features}")
        self.logger.info(f"   - –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö: {categorical_features}")
        self.logger.info(f"   - –ó–∞–ø–∏—Å–µ–π: {len(df)}")
        self.logger.info(f"   - –°–∏–º–≤–æ–ª–æ–≤: {df['symbol'].nunique()}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN –∏ inf
        nan_count = df.isna().sum().sum()
        inf_count = np.isinf(df.select_dtypes(include=[np.number])).sum().sum()

        if nan_count > 0:
            self.logger.warning(f"   ‚ö†Ô∏è NaN –∑–Ω–∞—á–µ–Ω–∏–π: {nan_count}")
        if inf_count > 0:
            self.logger.warning(f"   ‚ö†Ô∏è Inf –∑–Ω–∞—á–µ–Ω–∏–π: {inf_count}")

    def _add_enhanced_features(
        self, df: pd.DataFrame, all_symbols_data: Dict[str, pd.DataFrame]
    ) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è direction prediction

        Args:
            df: DataFrame —Å –±–∞–∑–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            all_symbols_data: —Å–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è cross-asset features

        Returns:
            DataFrame —Å enhanced features
        """
        self.logger.info("üöÄ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ enhanced features –¥–ª—è direction prediction...")

        # –ë–∞–∑–æ–≤—ã–µ enhanced features –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        enhanced_dfs = []

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Å–∏–º–≤–æ–ª
        for symbol in df["symbol"].unique():
            symbol_data = df[df["symbol"] == symbol].copy()

            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è direction prediction
            # 1. –£–ª—É—á—à–µ–Ω–Ω—ã–µ momentum –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            symbol_data["momentum_strength"] = (
                symbol_data["momentum_4h"].abs() + symbol_data["momentum_1h"].abs()
            ) / 2

            # 2. Volatility clustering
            symbol_data["vol_cluster"] = (
                symbol_data["volatility_5m"]
                > symbol_data["volatility_5m"].rolling(20).mean()
            ).astype(int)

            # 3. Price position in multiple timeframes
            for period in [10, 20, 50]:
                high_period = symbol_data["high"].rolling(period).max()
                low_period = symbol_data["low"].rolling(period).min()
                symbol_data[f"price_position_{period}"] = (
                    symbol_data["close"] - low_period
                ) / (high_period - low_period + 1e-10)

            enhanced_dfs.append(symbol_data)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        result_df = pd.concat(enhanced_dfs, ignore_index=True)

        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        original_cols = set(df.columns)
        new_cols = set(result_df.columns) - original_cols

        if new_cols:
            self.logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(new_cols)} enhanced features")

        return result_df

    def prepare_trading_data_without_leakage(
        self,
        df: pd.DataFrame,
        train_ratio: float = 0.7,
        val_ratio: float = 0.15,
        disable_progress: bool = False,
    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ –ë–ï–ó DATA LEAKAGE
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö

        Args:
            df: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            train_ratio: –¥–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            val_ratio: –¥–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            disable_progress: –æ—Ç–∫–ª—é—á–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä—ã

        Returns:
            Tuple[train_data, val_data, test_data]
        """

        self.disable_progress = disable_progress

        if not self.disable_progress:
            self.logger.info("üßπ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ data leakage...")
            self.logger.info("1/5 - –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤...")

        # 1. –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤
        # –ü—Ä–∏–∑–Ω–∞–∫–∏ —É–∂–µ —Å–æ–∑–¥–∞–Ω—ã –≤ df, —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º
        featured_dfs = []
        for symbol in df["symbol"].unique():
            symbol_data = df[df["symbol"] == symbol].copy()
            featured_dfs.append(symbol_data)

        if not self.disable_progress:
            self.logger.info("2/5 - –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∫—Ä–æ—Å—Å-–∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        result_df = pd.concat(featured_dfs, ignore_index=True)
        result_df = self._create_cross_asset_features(result_df)

        if not self.disable_progress:
            self.logger.info("3/5 - –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...")
        result_df = self._handle_missing_values(result_df)

        # 2. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ü–û –í–†–ï–ú–ï–ù–ò (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è data leakage)
        if not self.disable_progress:
            self.logger.info("4/5 - –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
        train_data_list = []
        val_data_list = []
        test_data_list = []

        for symbol in result_df["symbol"].unique():
            symbol_data = result_df[result_df["symbol"] == symbol].sort_values(
                "datetime"
            )
            n = len(symbol_data)

            train_end = int(n * train_ratio)
            val_end = int(n * (train_ratio + val_ratio))

            train_data_list.append(symbol_data.iloc[:train_end])
            val_data_list.append(symbol_data.iloc[train_end:val_end])
            test_data_list.append(symbol_data.iloc[val_end:])

        train_data = pd.concat(train_data_list, ignore_index=True)
        val_data = pd.concat(val_data_list, ignore_index=True)
        test_data = pd.concat(test_data_list, ignore_index=True)

        # 3. –ü–†–ê–í–ò–õ–¨–ù–ê–Ø –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ë–ï–ó DATA LEAKAGE
        if not self.disable_progress:
            self.logger.info("5/5 - –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±–µ–∑ data leakage...")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        exclude_cols = [
            "id",
            "symbol",
            "timestamp",
            "datetime",
            "sector",
            "open",
            "high",
            "low",
            "close",
            "volume",
            "turnover",
        ]

        # –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        target_cols = [
            col
            for col in train_data.columns
            if col.startswith(("target_", "future_", "optimal_"))
        ]
        exclude_cols.extend(target_cols)

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (—É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã)
        time_cols = [
            "hour",
            "minute",
            "dayofweek",
            "day",
            "month",
            "is_weekend",
            "asian_session",
            "european_session",
            "american_session",
            "session_overlap",
        ]
        exclude_cols.extend(time_cols)

        # –ü—Ä–∏–∑–Ω–∞–∫–∏-—Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã –ø–æ —Å–≤–æ–µ–π –ø—Ä–∏—Ä–æ–¥–µ
        ratio_cols = [
            "close_vwap_ratio",
            "close_open_ratio",
            "high_low_ratio",
            "close_position",
            "bb_position",
            "position_in_range_20",
            "position_in_range_50",
            "position_in_range_100",
        ]
        exclude_cols.extend(ratio_cols)

        feature_cols = [col for col in train_data.columns if col not in exclude_cols]

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        unique_symbols = train_data["symbol"].unique()
        # –í –º–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–Ω–æ–º —Ä–µ–∂–∏–º–µ –æ—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä—ã
        if disable_progress:
            norm_iterator = unique_symbols
        else:
            norm_iterator = tqdm(unique_symbols, desc="–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è", unit="—Å–∏–º–≤–æ–ª")

        for symbol in norm_iterator:
            # –ú–∞—Å–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
            train_mask = train_data["symbol"] == symbol
            val_mask = val_data["symbol"] == symbol
            test_mask = test_data["symbol"] == symbol

            if train_mask.sum() == 0:
                continue

            # –û–±—É—á–∞–µ–º RobustScaler –¢–û–õ–¨–ö–û –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö
            if symbol not in self.scalers:
                self.scalers[symbol] = RobustScaler()

            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ train –¥–∞–Ω–Ω—ã–µ
            train_symbol_data = train_data.loc[train_mask, feature_cols].dropna()

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–æ –≤—Å–µ–º —Ü–∏–∫–ª–µ
            numeric_feature_cols = []

            if len(train_symbol_data) > 0:
                # –û—á–∏—Å—Ç–∫–∞ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ train –¥–∞–Ω–Ω—ã—Ö
                train_cleaned = train_symbol_data.copy()

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
                for col in feature_cols:
                    if col in train_cleaned.columns and pd.api.types.is_numeric_dtype(
                        train_cleaned[col]
                    ):
                        numeric_feature_cols.append(col)
                    else:
                        if not self.disable_progress:
                            self.logger.warning(
                                f"–ö–æ–ª–æ–Ω–∫–∞ '{col}' –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —á–∏—Å–ª–æ–≤–æ–π –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º"
                            )

                for col in numeric_feature_cols:
                    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –ø–µ—Ä–µ–¥ –∫–≤–∞–Ω—Ç–∏–ª—è–º–∏
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø –Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –µ—Å—Ç—å —Å—Ç—Ä–æ–∫–∏
                    train_cleaned[col] = pd.to_numeric(
                        train_cleaned[col], errors="coerce"
                    )

                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–æ–ª—å–∫–æ NaN –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                    if train_cleaned[col].notna().sum() == 0:
                        if not self.disable_progress:
                            self.logger.warning(
                                f"–ö–æ–ª–æ–Ω–∫–∞ '{col}' —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ NaN –∑–Ω–∞—á–µ–Ω–∏—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º"
                            )
                        continue

                    # –ö–ª–∏–ø–ø–∏–Ω–≥ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                    q01 = train_cleaned[col].quantile(0.01)
                    q99 = train_cleaned[col].quantile(0.99)
                    train_cleaned[col] = train_cleaned[col].clip(lower=q01, upper=q99)

                    # –ó–∞–º–µ–Ω–∞ inf –Ω–∞ –∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    train_cleaned[col] = train_cleaned[col].replace(
                        [np.inf, -np.inf], [q99, q01]
                    )
                    train_cleaned[col] = train_cleaned[col].fillna(
                        train_cleaned[col].median()
                    )

                # –û–±—É—á–∞–µ–º RobustScaler –Ω–∞ –æ—á–∏—â–µ–Ω–Ω—ã—Ö train –¥–∞–Ω–Ω—ã—Ö —Ç–æ–ª—å–∫–æ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –∫–æ–ª–æ–Ω–∫–∞–º
                self.scalers[symbol].fit(train_cleaned[numeric_feature_cols])

                # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ –≤—Å–µ–º –¥–∞–Ω–Ω—ã–º —Å–∏–º–≤–æ–ª–∞
                # Train
                train_valid_mask = train_mask & train_data[
                    numeric_feature_cols
                ].notna().all(axis=1)
                if train_valid_mask.sum() > 0:
                    train_to_scale = train_data.loc[
                        train_valid_mask, numeric_feature_cols
                    ].copy()
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—É –∂–µ –æ—á–∏—Å—Ç–∫—É
                    for col in numeric_feature_cols:
                        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø
                        train_to_scale[col] = pd.to_numeric(
                            train_to_scale[col], errors="coerce"
                        )

                        if train_to_scale[col].notna().sum() == 0:
                            continue

                        q01 = (
                            train_cleaned[col].quantile(0.01)
                            if col in train_cleaned.columns
                            else train_to_scale[col].quantile(0.01)
                        )
                        q99 = (
                            train_cleaned[col].quantile(0.99)
                            if col in train_cleaned.columns
                            else train_to_scale[col].quantile(0.99)
                        )
                        train_to_scale[col] = train_to_scale[col].clip(
                            lower=q01, upper=q99
                        )
                        train_to_scale[col] = train_to_scale[col].replace(
                            [np.inf, -np.inf], [q99, q01]
                        )
                        train_to_scale[col] = train_to_scale[col].fillna(
                            train_to_scale[col].median()
                        )

                    train_data.loc[train_valid_mask, numeric_feature_cols] = (
                        self.scalers[symbol].transform(train_to_scale)
                    )

                # Val
                val_valid_mask = val_mask & val_data[numeric_feature_cols].notna().all(
                    axis=1
                )
                if val_valid_mask.sum() > 0:
                    val_to_scale = val_data.loc[
                        val_valid_mask, numeric_feature_cols
                    ].copy()
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—É –∂–µ –æ—á–∏—Å—Ç–∫—É –∏—Å–ø–æ–ª—å–∑—É—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–∑ train
                    for col in numeric_feature_cols:
                        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø
                        val_to_scale[col] = pd.to_numeric(
                            val_to_scale[col], errors="coerce"
                        )

                        if val_to_scale[col].notna().sum() == 0:
                            continue

                        q01 = (
                            train_cleaned[col].quantile(0.01)
                            if col in train_cleaned.columns
                            else val_to_scale[col].quantile(0.01)
                        )
                        q99 = (
                            train_cleaned[col].quantile(0.99)
                            if col in train_cleaned.columns
                            else val_to_scale[col].quantile(0.99)
                        )
                        val_to_scale[col] = val_to_scale[col].clip(lower=q01, upper=q99)
                        val_to_scale[col] = val_to_scale[col].replace(
                            [np.inf, -np.inf], [q99, q01]
                        )
                        val_to_scale[col] = val_to_scale[col].fillna(
                            val_to_scale[col].median()
                        )

                    val_data.loc[val_valid_mask, numeric_feature_cols] = self.scalers[
                        symbol
                    ].transform(val_to_scale)

                # Test
                test_valid_mask = test_mask & test_data[
                    numeric_feature_cols
                ].notna().all(axis=1)
                if test_valid_mask.sum() > 0:
                    test_to_scale = test_data.loc[
                        test_valid_mask, numeric_feature_cols
                    ].copy()
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—É –∂–µ –æ—á–∏—Å—Ç–∫—É –∏—Å–ø–æ–ª—å–∑—É—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–∑ train
                    for col in numeric_feature_cols:
                        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø
                        test_to_scale[col] = pd.to_numeric(
                            test_to_scale[col], errors="coerce"
                        )

                        if test_to_scale[col].notna().sum() == 0:
                            continue

                        q01 = (
                            train_cleaned[col].quantile(0.01)
                            if col in train_cleaned.columns
                            else test_to_scale[col].quantile(0.01)
                        )
                        q99 = (
                            train_cleaned[col].quantile(0.99)
                            if col in train_cleaned.columns
                            else test_to_scale[col].quantile(0.99)
                        )
                        test_to_scale[col] = test_to_scale[col].clip(
                            lower=q01, upper=q99
                        )
                        test_to_scale[col] = test_to_scale[col].replace(
                            [np.inf, -np.inf], [q99, q01]
                        )
                        test_to_scale[col] = test_to_scale[col].fillna(
                            test_to_scale[col].median()
                        )

                    test_data.loc[test_valid_mask, numeric_feature_cols] = self.scalers[
                        symbol
                    ].transform(test_to_scale)

        # –ö–†–ò–¢–ò–ß–ù–û: –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ future –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        # NaN –ø–æ—è–≤–ª—è—é—Ç—Å—è –≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö N —Å—Ç—Ä–æ–∫–∞—Ö –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –∏–∑-–∑–∞ shift(-N)
        future_cols = [col for col in train_data.columns if col.startswith("future_")]
        if future_cols:
            if not self.disable_progress:
                self.logger.info("üßë –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å NaN –≤ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö...")

            # –ü–æ–¥—Å—á–µ—Ç –¥–æ —É–¥–∞–ª–µ–Ω–∏—è
            train_before = len(train_data)
            val_before = len(val_data)
            test_before = len(test_data)

            # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ –ª—é–±–æ–π –∏–∑ future –∫–æ–ª–æ–Ω–æ–∫
            train_data = train_data.dropna(subset=future_cols)
            val_data = val_data.dropna(subset=future_cols)
            test_data = test_data.dropna(subset=future_cols)

            if not self.disable_progress:
                self.logger.info(
                    f"  –£–¥–∞–ª–µ–Ω–æ —Å—Ç—Ä–æ–∫: Train={train_before - len(train_data)}, "
                    f"Val={val_before - len(val_data)}, Test={test_before - len(test_data)}"
                )

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if not self.disable_progress:
            self.logger.info("‚úÖ –†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ data leakage:")
            self.logger.info(f"   - Train: {len(train_data)} –∑–∞–ø–∏—Å–µ–π")
            self.logger.info(f"   - Val: {len(val_data)} –∑–∞–ø–∏—Å–µ–π")
            self.logger.info(f"   - Test: {len(test_data)} –∑–∞–ø–∏—Å–µ–π")
            self.logger.info(f"   - –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_cols)}")

        return train_data, val_data, test_data
