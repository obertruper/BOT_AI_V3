#!/usr/bin/env python3
"""
Signal Quality Analyzer –¥–ª—è ML —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Å–∏–≥–Ω–∞–ª–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
"""

from dataclasses import dataclass
from enum import Enum

import numpy as np

from core.logger import setup_logger

logger = setup_logger(__name__)


class FilterStrategy(Enum):
    """–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å–∏–≥–Ω–∞–ª–æ–≤"""

    CONSERVATIVE = "conservative"
    MODERATE = "moderate"
    AGGRESSIVE = "aggressive"


@dataclass
class QualityMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–≥–Ω–∞–ª–∞"""

    quality_score: float
    agreement_score: float
    confidence_score: float
    return_score: float
    risk_score: float


@dataclass
class FilterResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å–∏–≥–Ω–∞–ª–∞"""

    passed: bool
    signal_type: str
    strategy_used: FilterStrategy
    quality_metrics: QualityMetrics
    rejection_reasons: list[str]


class SignalQualityAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ ML —Å–∏–≥–Ω–∞–ª–æ–≤
    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    """

    def __init__(self, config: dict = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞

        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ ml_config.yaml
        """
        self.config = config or {}

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        filter_config = self.config.get("signal_filtering", {})

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∞–∫—Ç–∏–≤–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
        strategy_name = filter_config.get("strategy", "moderate")
        self.active_strategy = FilterStrategy(strategy_name)

        # –í–µ—Å–∞ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ (–∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–ª–∏ defaults)
        timeframe_weights_list = filter_config.get("timeframe_weights", [0.25, 0.25, 0.35, 0.15])
        self.timeframe_weights = np.array(timeframe_weights_list)

        # –ò–Ω–¥–µ–∫—Å –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ (–æ–±—ã—á–Ω–æ 4h = –∏–Ω–¥–µ–∫—Å 2)
        self.main_timeframe_index = filter_config.get("main_timeframe_index", 2)

        # –í–µ—Å–∞ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        quality_weights = filter_config.get("quality_weights", {})
        self.quality_weights = {
            "agreement": quality_weights.get("agreement", 0.35),
            "confidence": quality_weights.get("confidence", 0.30),
            "return_expectation": quality_weights.get("return_expectation", 0.20),
            "risk_adjustment": quality_weights.get("risk_adjustment", 0.15),
        }

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ç—Ä–∞—Ç–µ–≥–∏–π - –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–û –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏
        self.strategy_params = {
            FilterStrategy.CONSERVATIVE: filter_config.get(
                "conservative",
                {
                    "min_timeframe_agreement": 2,  # –°–Ω–∏–∂–µ–Ω–æ —Å 3
                    "required_confidence_per_timeframe": 0.50,  # –°–Ω–∏–∂–µ–Ω–æ —Å 0.65
                    "main_timeframe_required_confidence": 0.55,  # –°–Ω–∏–∂–µ–Ω–æ —Å 0.70
                    "min_expected_return_pct": 0.002,  # –°–Ω–∏–∂–µ–Ω–æ —Å 0.008
                    "min_signal_strength": 0.45,  # –°–Ω–∏–∂–µ–Ω–æ —Å 0.7
                    "max_risk_level": "HIGH",  # –ò–∑–º–µ–Ω–µ–Ω–æ —Å MEDIUM
                    "min_quality_score": 0.40,  # –°–Ω–∏–∂–µ–Ω–æ —Å 0.75
                },
            ),
            FilterStrategy.MODERATE: filter_config.get(
                "moderate",
                {
                    "min_timeframe_agreement": 3,  # –û–ü–¢–ò–ú–ê–õ–¨–ù–û: 3 –∏–∑ 4 —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ (–º–µ–Ω—å—à–µ –ª–æ–∂–Ω—ã—Ö)
                    "required_confidence_per_timeframe": 0.38,  # –†–ï–ê–õ–ò–°–¢–ò–ß–ù–û: —á—É—Ç—å –≤—ã—à–µ —Å—Ä–µ–¥–Ω–µ–≥–æ
                    "main_timeframe_required_confidence": 0.40,  # –†–ï–ê–õ–ò–°–¢–ò–ß–ù–û: 4h –≤–∞–∂–µ–Ω
                    "alternative_main_plus_one": True,
                    "alternative_confidence_threshold": 0.42,  # –†–ï–ê–õ–ò–°–¢–ò–ß–ù–û: –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥
                    "min_expected_return_pct": 0.0010,  # –†–ï–ê–õ–ò–°–¢–ò–ß–ù–û: 0.10% –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –∫—Ä–∏–ø—Ç–æ
                    "min_signal_strength": 0.40,  # –£–ú–ï–†–ï–ù–ù–û: —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –æ—á–µ–Ω—å —Å–ª–∞–±—ã–µ
                    "max_risk_level": "MEDIUM",  # –£–ú–ï–†–ï–ù–ù–û: –¥–æ–ø—É—Å–∫–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫
                    "min_quality_score": 0.45,  # –£–ú–ï–†–ï–ù–ù–û: –±–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —á–∞—Å—Ç–æ—Ç—ã
                },
            ),
            FilterStrategy.AGGRESSIVE: filter_config.get(
                "aggressive",
                {
                    "min_timeframe_agreement": 1,
                    "required_confidence_per_timeframe": 0.40,  # –ü–æ–≤—ã—à–µ–Ω–æ —Å 0.30
                    "main_timeframe_required_confidence": 0.42,  # –ü–æ–≤—ã—à–µ–Ω–æ —Å 0.35
                    "min_expected_return_pct": 0.0008,  # –ü–æ–≤—ã—à–µ–Ω–æ —Å 0.0005 - –º–∏–Ω–∏–º—É–º 0.08%
                    "min_signal_strength": 0.32,  # –ü–æ–≤—ã—à–µ–Ω–æ —Å 0.25
                    "max_risk_level": "HIGH",
                    "min_quality_score": 0.35,  # –ü–æ–≤—ã—à–µ–Ω–æ —Å 0.25
                },
            ),
        }

        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (legacy –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        self.quality_thresholds = {"excellent": 0.75, "good": 0.60, "moderate": 0.45, "poor": 0.30}

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        self.stats = {
            "total_analyzed": 0,
            "high_quality": 0,
            "medium_quality": 0,
            "low_quality": 0,
            "rejected": 0,
        }

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º
        self.strategy_stats = {
            strategy: {
                "analyzed": 0,
                "passed": 0,
                "rejected": 0,
                "avg_quality": 0.0,
                "rejection_reasons": {},
            }
            for strategy in FilterStrategy
        }

        logger.info(
            f"SignalQualityAnalyzer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π: {self.active_strategy.value}"
        )

    def analyze_signal_quality(
        self,
        directions: np.ndarray,
        direction_probs: list[np.ndarray],
        future_returns: np.ndarray,
        risk_metrics: np.ndarray,
        weighted_direction: float,
    ) -> FilterResult:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Å–∏–≥–Ω–∞–ª–∞ —Å —É—á–µ—Ç–æ–º –∞–∫—Ç–∏–≤–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.

        Args:
            directions: –ú–∞—Å—Å–∏–≤ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π [15m, 1h, 4h, 12h]
            direction_probs: –°–ø–∏—Å–æ–∫ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
            future_returns: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
            risk_metrics: –ú–µ—Ç—Ä–∏–∫–∏ —Ä–∏—Å–∫–∞
            weighted_direction: –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ

        Returns:
            FilterResult —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –∞–Ω–∞–ª–∏–∑–∞
        """
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.strategy_stats[self.active_strategy]["analyzed"] += 1
        self.stats["total_analyzed"] += 1

        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–∫—Ç–∏–≤–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        params = self.strategy_params[self.active_strategy]

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        quality_metrics = self._calculate_quality_metrics(
            directions, direction_probs, future_returns, risk_metrics
        )

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–π —Å–∏–≥–Ω–∞–ª
        signal_type = self._determine_signal_type(directions, weighted_direction)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–µ—Ä–∏–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        rejection_reasons = []
        passed = self._check_filtering_criteria(
            directions,
            direction_probs,
            future_returns,
            risk_metrics,
            quality_metrics,
            params,
            rejection_reasons,
        )

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        if passed:
            self.strategy_stats[self.active_strategy]["passed"] += 1
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
            stats = self.strategy_stats[self.active_strategy]
            stats["avg_quality"] = (
                (stats["avg_quality"] * (stats["passed"] - 1)) + quality_metrics.quality_score
            ) / stats["passed"]
        else:
            self.strategy_stats[self.active_strategy]["rejected"] += 1
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–∏—á–∏–Ω—ã –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
            for reason in rejection_reasons:
                reason_stats = self.strategy_stats[self.active_strategy]["rejection_reasons"]
                reason_stats[reason] = reason_stats.get(reason, 0) + 1

        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = FilterResult(
            passed=passed,
            signal_type=signal_type,
            strategy_used=self.active_strategy,
            quality_metrics=quality_metrics,
            rejection_reasons=rejection_reasons,
        )

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        status = "‚úÖ –ü–†–ò–ù–Ø–¢" if passed else "‚ùå –û–¢–ö–õ–û–ù–ï–ù"
        logger.info(
            f"{status} —Å–∏–≥–Ω–∞–ª {signal_type} (—Å—Ç—Ä–∞—Ç–µ–≥–∏—è: {self.active_strategy.value}, –∫–∞—á–µ—Å—Ç–≤–æ: {quality_metrics.quality_score:.3f})"
        )

        if not passed and rejection_reasons:
            logger.debug(f"–ü—Ä–∏—á–∏–Ω—ã –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è: {'; '.join(rejection_reasons)}")

        return result

    def _calculate_quality_metrics(
        self,
        directions: np.ndarray,
        direction_probs: list[np.ndarray],
        future_returns: np.ndarray,
        risk_metrics: np.ndarray,
    ) -> QualityMetrics:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–≥–Ω–∞–ª–∞"""

        # 1. Agreement Score - —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
        agreement_score = self._calculate_agreement_score_weighted(directions)

        # 2. Confidence Score - —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
        confidence_score = self._calculate_confidence_score_enhanced(direction_probs)

        # 3. Return Score - –æ–∂–∏–¥–∞–µ–º–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å
        return_score = self._calculate_return_score_enhanced(future_returns)

        # 4. Risk Score - –æ—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–∞
        risk_score = self._calculate_risk_score(risk_metrics)

        # 5. Overall Quality Score
        quality_score = (
            self.quality_weights["agreement"] * agreement_score
            + self.quality_weights["confidence"] * confidence_score
            + self.quality_weights["return_expectation"] * return_score
            + self.quality_weights["risk_adjustment"] * (1.0 - risk_score)  # –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–∏—Å–∫
        )

        return QualityMetrics(
            quality_score=quality_score,
            agreement_score=agreement_score,
            confidence_score=confidence_score,
            return_score=return_score,
            risk_score=risk_score,
        )

    def _calculate_agreement_score_weighted(self, directions: np.ndarray) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤–∑–≤–µ—à–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤"""
        # –ù–∞—Ö–æ–¥–∏–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â–µ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
        unique, counts = np.unique(directions, return_counts=True)
        dominant_direction = unique[np.argmax(counts)]

        # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É —Å–æ–≥–ª–∞—Å–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
        agreement_mask = (directions == dominant_direction).astype(float)

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π score
        weighted_score = np.sum(agreement_mask * self.timeframe_weights)

        return float(weighted_score)

    def _calculate_confidence_score_enhanced(self, direction_probs: list[np.ndarray]) -> float:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ —Å —É—á–µ—Ç–æ–º —ç–Ω—Ç—Ä–æ–ø–∏–∏"""
        confidences = np.array([np.max(probs) for probs in direction_probs])

        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        weighted_confidence = np.sum(confidences * self.timeframe_weights)

        # –≠–Ω—Ç—Ä–æ–ø–∏–π–Ω–∞—è —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∞—è
        entropies = []
        for probs in direction_probs:
            probs_safe = np.clip(probs, 1e-10, 1.0)
            entropy = -np.sum(probs_safe * np.log(probs_safe))
            entropies.append(entropy)

        avg_entropy = np.mean(entropies)
        max_entropy = np.log(3)  # –î–ª—è 3 –∫–ª–∞—Å—Å–æ–≤
        entropy_score = 1.0 - (avg_entropy / max_entropy)

        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π score
        final_score = 0.7 * weighted_confidence + 0.3 * entropy_score

        return float(final_score)

    def _calculate_return_score_enhanced(self, future_returns: np.ndarray) -> float:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç score –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏"""
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ–∂–∏–¥–∞–µ–º–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å
        weighted_return = np.sum(np.abs(future_returns) * self.timeframe_weights)

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ö–æ—Ä–æ—à–µ–≥–æ —Å–∏–≥–Ω–∞–ª–∞ (1.5%)
        normalized_score = min(weighted_return / 0.015, 1.0)

        # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –∑–Ω–∞–∫–æ–≤
        non_zero_returns = future_returns[np.abs(future_returns) > 0.001]
        if len(non_zero_returns) > 0:
            sign_consistency = len(np.unique(np.sign(non_zero_returns))) == 1
            consistency_bonus = 0.1 if sign_consistency else 0
        else:
            consistency_bonus = 0

        final_score = min(normalized_score + consistency_bonus, 1.0)

        return float(final_score)

    def _calculate_risk_score(self, risk_metrics: np.ndarray) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É —Ä–∏—Å–∫–∞ (0 = –Ω–∏–∑–∫–∏–π —Ä–∏—Å–∫, 1 = –≤—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫)"""
        if len(risk_metrics) == 0:
            return 0.5

        # –°—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫ —Å —É—á–µ—Ç–æ–º –≤–µ—Å–æ–≤
        if len(risk_metrics) >= len(self.timeframe_weights):
            weighted_risk = np.sum(
                risk_metrics[: len(self.timeframe_weights)] * self.timeframe_weights
            )
        else:
            weighted_risk = np.mean(risk_metrics)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [0, 1]
        risk_score = np.clip(weighted_risk, 0.0, 1.0)

        return float(risk_score)

    def _determine_signal_type(self, directions: np.ndarray, weighted_direction: float) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Å–∏–≥–Ω–∞–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π"""
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–∂–¥–æ–≥–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        unique, counts = np.unique(directions, return_counts=True)
        direction_counts = dict(zip(unique, counts, strict=False))

        # –ú–∞–ø–ø–∏–Ω–≥ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        direction_map = {0: "LONG", 1: "SHORT", 2: "NEUTRAL"}

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â–µ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
        dominant_idx = np.argmax(counts)
        dominant_direction = unique[dominant_idx]
        dominant_count = counts[dominant_idx]

        # –ï—Å–ª–∏ –µ—Å—Ç—å —è–≤–Ω–æ–µ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ
        if dominant_count >= 3:
            return direction_map[dominant_direction]

        # –ï—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω–æ–≥–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
        if weighted_direction < 0.8:  # –ë–ª–∏–∂–µ –∫ LONG (0)
            return "LONG"
        elif weighted_direction > 1.2:  # –ë–ª–∏–∂–µ –∫ SHORT (1)
            return "SHORT"
        else:  # –ú–µ–∂–¥—É LONG –∏ SHORT –∏–ª–∏ –±–ª–∏–∑–∫–æ –∫ NEUTRAL (2)
            return "NEUTRAL"

    def _check_filtering_criteria(
        self,
        directions: np.ndarray,
        direction_probs: list[np.ndarray],
        future_returns: np.ndarray,
        risk_metrics: np.ndarray,
        quality_metrics: QualityMetrics,
        params: dict,
        rejection_reasons: list[str],
    ) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤—Å–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è –∞–∫—Ç–∏–≤–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""

        passed = True

        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
        unique, counts = np.unique(directions, return_counts=True)
        max_agreement = np.max(counts)

        if max_agreement < params["min_timeframe_agreement"]:
            rejection_reasons.append(
                f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –¢–§: {max_agreement} < {params['min_timeframe_agreement']}"
            )
            passed = False

        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        confidences = np.array([np.max(probs) for probs in direction_probs])

        # –û–±—â–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        min_conf_threshold = params["required_confidence_per_timeframe"]
        low_confidence_count = np.sum(confidences < min_conf_threshold)

        # –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–û: –†–∞–∑—Ä–µ—à–∞–µ–º –±–æ–ª—å—à–µ –¢–§ —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        max_low_confidence_allowed = 3  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 2 –¥–æ 3
        if low_confidence_count > max_low_confidence_allowed:
            rejection_reasons.append(
                f"–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –¢–§ —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é: {low_confidence_count} > {max_low_confidence_allowed}"
            )
            passed = False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
        main_tf_confidence = confidences[self.main_timeframe_index]
        main_tf_threshold = params["main_timeframe_required_confidence"]

        if main_tf_confidence < main_tf_threshold:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –∫—Ä–∏—Ç–µ—Ä–∏–π –¥–ª—è moderate —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
            if self.active_strategy == FilterStrategy.MODERATE and params.get(
                "alternative_main_plus_one", False
            ):

                alt_threshold = params.get("alternative_confidence_threshold", 0.75)
                high_conf_count = np.sum(confidences >= alt_threshold)

                if high_conf_count < 2:
                    rejection_reasons.append(
                        f"–û—Å–Ω–æ–≤–Ω–æ–π –¢–§: {main_tf_confidence:.3f} < {main_tf_threshold:.3f}, –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞"
                    )
                    passed = False
            else:
                rejection_reasons.append(
                    f"–û—Å–Ω–æ–≤–Ω–æ–π –¢–§: {main_tf_confidence:.3f} < {main_tf_threshold:.3f}"
                )
                passed = False

        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∂–∏–¥–∞–µ–º–æ–π –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
        max_return = np.max(np.abs(future_returns))
        min_return_threshold = params["min_expected_return_pct"]

        if max_return < min_return_threshold:
            rejection_reasons.append(
                f"–ù–∏–∑–∫–∞—è –æ–∂–∏–¥–∞–µ–º–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {max_return:.4f} < {min_return_threshold:.4f}"
            )
            passed = False

        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—â–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        if quality_metrics.quality_score < params["min_quality_score"]:
            rejection_reasons.append(
                f"–ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {quality_metrics.quality_score:.3f} < {params['min_quality_score']:.3f}"
            )
            passed = False

        # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–ª—ã —Å–∏–≥–Ω–∞–ª–∞
        signal_strength = (
            0.6 * quality_metrics.quality_score + 0.4 * quality_metrics.confidence_score
        )
        if signal_strength < params["min_signal_strength"]:
            rejection_reasons.append(
                f"–°–ª–∞–±—ã–π —Å–∏–≥–Ω–∞–ª: {signal_strength:.3f} < {params['min_signal_strength']:.3f}"
            )
            passed = False

        # 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Ä–æ–≤–Ω—è —Ä–∏—Å–∫–∞
        risk_level = (
            "LOW"
            if quality_metrics.risk_score < 0.3
            else "MEDIUM" if quality_metrics.risk_score < 0.7 else "HIGH"
        )
        max_risk_level = params["max_risk_level"]

        risk_hierarchy = {"LOW": 0, "MEDIUM": 1, "HIGH": 2}
        if risk_hierarchy[risk_level] > risk_hierarchy[max_risk_level]:
            rejection_reasons.append(f"–í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫: {risk_level} > {max_risk_level}")
            passed = False

        return passed

    def switch_strategy(self, strategy: str) -> bool:
        """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç –∞–∫—Ç–∏–≤–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"""
        try:
            new_strategy = FilterStrategy(strategy)
            self.active_strategy = new_strategy
            logger.info(f"–°—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∞ –Ω–∞: {strategy}")
            return True
        except ValueError:
            logger.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: {strategy}")
            return False

    def get_strategy_statistics(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Å–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º"""
        stats = {}

        for strategy, data in self.strategy_stats.items():
            strategy_name = strategy.value
            analyzed = data["analyzed"]

            if analyzed > 0:
                stats[f"{strategy_name}_analyzed"] = analyzed
                stats[f"{strategy_name}_passed"] = data["passed"]
                stats[f"{strategy_name}_rejected"] = data["rejected"]
                stats[f"{strategy_name}_pass_rate"] = (data["passed"] / analyzed) * 100
                stats[f"{strategy_name}_avg_quality"] = data["avg_quality"]
                stats[f"{strategy_name}_top_rejection_reasons"] = dict(
                    sorted(data["rejection_reasons"].items(), key=lambda x: x[1], reverse=True)[:3]
                )
            else:
                stats[f"{strategy_name}_analyzed"] = 0

        stats["active_strategy"] = self.active_strategy.value
        return stats

    def analyze_signal(
        self,
        directions: np.ndarray,
        confidences: np.ndarray,
        returns: np.ndarray,
        direction_probs: list[np.ndarray],
    ) -> dict:
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–≥–Ω–∞–ª–∞

        Args:
            directions: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è [15m, 1h, 4h, 12h]
            confidences: –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
            returns: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
            direction_probs: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞

        Returns:
            Dict —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–µ–π
        """
        self.stats["total_analyzed"] += 1

        # 1. Agreement Score - —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
        agreement_score = self._calculate_agreement_score(directions)

        # 2. Confidence Score - —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
        confidence_score = self._calculate_confidence_score(confidences, direction_probs)

        # 3. Return Score - –æ–∂–∏–¥–∞–µ–º–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å
        return_score = self._calculate_return_score(returns)

        # 4. Directional Consistency - —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å returns
        consistency_score = self._calculate_consistency_score(directions, returns)

        # 5. Overall Quality Score
        quality_score = self._calculate_overall_quality(
            agreement_score, confidence_score, return_score, consistency_score
        )

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Å–∏–≥–Ω–∞–ª–∞
        quality_level = self._determine_quality_level(quality_score)

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self._update_statistics(quality_level)

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–æ —Å–∏–≥–Ω–∞–ª—É
        recommendation = self._generate_recommendation(
            quality_score, agreement_score, confidence_score, directions
        )

        result = {
            "quality_score": quality_score,
            "quality_level": quality_level,
            "agreement_score": agreement_score,
            "confidence_score": confidence_score,
            "return_score": return_score,
            "consistency_score": consistency_score,
            "recommendation": recommendation,
            "details": {
                "timeframe_agreement": self._get_timeframe_agreement_details(directions),
                "confidence_distribution": self._get_confidence_distribution(confidences),
                "expected_return": float(np.mean(returns)),
                "signal_strength": self._calculate_signal_strength(quality_score, confidence_score),
            },
        }

        logger.info(f"üìä Signal Quality Analysis: {quality_level} (score: {quality_score:.3f})")
        logger.debug(
            f"   Agreement: {agreement_score:.3f}, Confidence: {confidence_score:.3f}, "
            f"Return: {return_score:.3f}, Consistency: {consistency_score:.3f}"
        )

        return result

    def _calculate_agreement_score(self, directions: np.ndarray) -> float:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ —Å —É—á–µ—Ç–æ–º –≤–µ—Å–æ–≤

        Returns:
            Score –æ—Ç 0 –¥–æ 1, –≥–¥–µ 1 = –ø–æ–ª–Ω–æ–µ —Å–æ–≥–ª–∞—Å–∏–µ
        """
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É –∫–∞–∂–¥–æ–≥–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        unique, counts = np.unique(directions, return_counts=True)

        # –ù–∞—Ö–æ–¥–∏–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â–µ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
        dominant_direction = unique[np.argmax(counts)]
        dominant_count = np.max(counts)

        # –ë–∞–∑–æ–≤—ã–π score –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–æ–≥–ª–∞—Å–Ω—ã—Ö
        base_score = dominant_count / len(directions)

        # –£—á–∏—Ç—ã–≤–∞–µ–º –≤–µ—Å–∞ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
        weights = self.timeframe_weights

        # –í–∑–≤–µ—à–µ–Ω–Ω—ã–π score –¥–ª—è —Å–æ–≥–ª–∞—Å–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
        agreement_mask = (directions == dominant_direction).astype(float)
        weighted_score = np.sum(agreement_mask * weights)

        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π score
        final_score = 0.6 * base_score + 0.4 * weighted_score

        return float(final_score)

    def _calculate_confidence_score(
        self, confidences: np.ndarray, direction_probs: list[np.ndarray]
    ) -> float:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ–±—â—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
        """
        # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        avg_confidence = np.mean(confidences)

        # –≠–Ω—Ç—Ä–æ–ø–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π (–Ω–∏–∑–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è = –≤—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
        entropies = []
        for probs in direction_probs:
            # –ò–∑–±–µ–≥–∞–µ–º log(0)
            probs_safe = np.clip(probs, 1e-10, 1.0)
            entropy = -np.sum(probs_safe * np.log(probs_safe))
            entropies.append(entropy)

        avg_entropy = np.mean(entropies)
        max_entropy = np.log(3)  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è –¥–ª—è 3 –∫–ª–∞—Å—Å–æ–≤

        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è (0 = –≤—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å, 1 = –Ω–∏–∑–∫–∞—è)
        normalized_entropy = avg_entropy / max_entropy
        entropy_score = 1.0 - normalized_entropy

        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π score
        final_score = 0.7 * avg_confidence + 0.3 * entropy_score

        return float(final_score)

    def _calculate_return_score(self, returns: np.ndarray) -> float:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ–∂–∏–¥–∞–µ–º—É—é –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å
        """
        # –°—Ä–µ–¥–Ω–∏–π return —Å —É—á–µ—Ç–æ–º –≤–µ—Å–æ–≤ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
        weights = self.timeframe_weights
        weighted_return = np.sum(np.abs(returns) * weights)

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ 2% return = –æ—Ç–ª–∏—á–Ω—ã–π —Å–∏–≥–Ω–∞–ª)
        normalized_score = min(weighted_return / 0.02, 1.0)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –∑–Ω–∞–∫–æ–≤ returns
        sign_consistency = len(np.unique(np.sign(returns[returns != 0]))) == 1
        consistency_bonus = 0.1 if sign_consistency else 0

        final_score = min(normalized_score + consistency_bonus, 1.0)

        return float(final_score)

    def _calculate_consistency_score(self, directions: np.ndarray, returns: np.ndarray) -> float:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º–∏ returns
        """
        consistency_checks = []

        for i, (direction, ret) in enumerate(zip(directions, returns, strict=False)):
            # LONG (0) –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π return
            # SHORT (1) –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π return
            # NEUTRAL (2) –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å –º–∞–ª—ã–π return

            if direction == 0:  # LONG
                consistency_checks.append(ret > 0.001)
            elif direction == 1:  # SHORT
                consistency_checks.append(ret < -0.001)
            else:  # NEUTRAL
                consistency_checks.append(abs(ret) < 0.003)

        consistency_score = np.mean(consistency_checks)
        return float(consistency_score)

    def _calculate_overall_quality(
        self, agreement: float, confidence: float, returns: float, consistency: float
    ) -> float:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å–∏–≥–Ω–∞–ª–∞
        """
        # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        weights = {"agreement": 0.35, "confidence": 0.30, "returns": 0.20, "consistency": 0.15}

        quality_score = (
            weights["agreement"] * agreement
            + weights["confidence"] * confidence
            + weights["returns"] * returns
            + weights["consistency"] * consistency
        )

        return float(quality_score)

    def _determine_quality_level(self, quality_score: float) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–≥–Ω–∞–ª–∞
        """
        if quality_score >= self.quality_thresholds["excellent"]:
            return "EXCELLENT"
        elif quality_score >= self.quality_thresholds["good"]:
            return "GOOD"
        elif quality_score >= self.quality_thresholds["moderate"]:
            return "MODERATE"
        elif quality_score >= self.quality_thresholds["poor"]:
            return "POOR"
        else:
            return "REJECT"

    def _generate_recommendation(
        self,
        quality_score: float,
        agreement_score: float,
        confidence_score: float,
        directions: np.ndarray,
    ) -> dict:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –ø–æ —Ç–æ—Ä–≥–æ–≤–ª–µ
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â–µ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
        unique, counts = np.unique(directions, return_counts=True)
        dominant_idx = np.argmax(counts)
        dominant_direction = unique[dominant_idx]
        dominant_count = counts[dominant_idx]

        # –ú–∞–ø–ø–∏–Ω–≥ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        direction_map = {0: "LONG", 1: "SHORT", 2: "NEUTRAL"}
        signal_type = direction_map[dominant_direction]

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–∏–ª—É —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if quality_score >= 0.75 and dominant_count >= 3:
            action = "STRONG_SIGNAL"
            position_size = 1.0
        elif quality_score >= 0.60 and dominant_count >= 2:
            action = "MODERATE_SIGNAL"
            position_size = 0.7
        elif quality_score >= 0.45:
            action = "WEAK_SIGNAL"
            position_size = 0.4
        else:
            action = "NO_TRADE"
            position_size = 0.0

        return {
            "action": action,
            "signal_type": signal_type,
            "position_size_multiplier": position_size,
            "reasons": self._get_recommendation_reasons(
                quality_score, agreement_score, confidence_score, dominant_count
            ),
        }

    def _get_recommendation_reasons(
        self,
        quality_score: float,
        agreement_score: float,
        confidence_score: float,
        dominant_count: int,
    ) -> list[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–∏—á–∏–Ω –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        """
        reasons = []

        if quality_score >= 0.75:
            reasons.append("‚úÖ –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å–∏–≥–Ω–∞–ª–∞")
        elif quality_score < 0.45:
            reasons.append("‚ùå –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å–∏–≥–Ω–∞–ª–∞")

        if agreement_score >= 0.75:
            reasons.append("‚úÖ –í—ã—Å–æ–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤")
        elif agreement_score < 0.50:
            reasons.append("‚ö†Ô∏è –ù–∏–∑–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤")

        if confidence_score >= 0.70:
            reasons.append("‚úÖ –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏")
        elif confidence_score < 0.50:
            reasons.append("‚ö†Ô∏è –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏")

        if dominant_count >= 3:
            reasons.append(f"‚úÖ {dominant_count}/4 —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ —Å–æ–≥–ª–∞—Å–Ω—ã")
        elif dominant_count < 2:
            reasons.append("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —Å–æ–≥–ª–∞—Å–∏–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤")

        return reasons

    def _get_timeframe_agreement_details(self, directions: np.ndarray) -> dict:
        """
        –î–µ—Ç–∞–ª–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
        """
        timeframes = ["15m", "1h", "4h", "12h"]
        direction_map = {0: "LONG", 1: "SHORT", 2: "NEUTRAL"}

        details = {}
        for i, (tf, direction) in enumerate(zip(timeframes, directions, strict=False)):
            details[tf] = direction_map[direction]

        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        unique, counts = np.unique(directions, return_counts=True)
        for dir_idx, count in zip(unique, counts, strict=False):
            details[f"{direction_map[dir_idx]}_count"] = int(count)

        return details

    def _get_confidence_distribution(self, confidences: np.ndarray) -> dict:
        """
        –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º
        """
        timeframes = ["15m", "1h", "4h", "12h"]
        distribution = {}

        for tf, conf in zip(timeframes, confidences, strict=False):
            distribution[tf] = float(conf)

        distribution["average"] = float(np.mean(confidences))
        distribution["std"] = float(np.std(confidences))

        return distribution

    def _calculate_signal_strength(self, quality_score: float, confidence_score: float) -> float:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å–∏–ª—É —Ç–æ—Ä–≥–æ–≤–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞
        """
        # –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        strength = 0.6 * quality_score + 0.4 * confidence_score

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–∏–≥–º–æ–∏–¥—É –¥–ª—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è
        strength = 1 / (1 + np.exp(-10 * (strength - 0.5)))

        return float(strength)

    def _update_statistics(self, quality_level: str):
        """
        –û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        """
        if quality_level in ["EXCELLENT", "GOOD"]:
            self.stats["high_quality"] += 1
        elif quality_level == "MODERATE":
            self.stats["medium_quality"] += 1
        elif quality_level == "POOR":
            self.stats["low_quality"] += 1
        else:
            self.stats["rejected"] += 1

    def get_statistics(self) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        """
        total = self.stats["total_analyzed"]
        if total == 0:
            return self.stats

        stats_with_percentages = self.stats.copy()
        stats_with_percentages["high_quality_pct"] = (self.stats["high_quality"] / total) * 100
        stats_with_percentages["medium_quality_pct"] = (self.stats["medium_quality"] / total) * 100
        stats_with_percentages["low_quality_pct"] = (self.stats["low_quality"] / total) * 100
        stats_with_percentages["rejected_pct"] = (self.stats["rejected"] / total) * 100

        return stats_with_percentages

    def reset_statistics(self):
        """
        –°–±—Ä–æ—Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        """
        self.stats = {
            "total_analyzed": 0,
            "high_quality": 0,
            "medium_quality": 0,
            "low_quality": 0,
            "rejected": 0,
        }
        logger.info("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–±—Ä–æ—à–µ–Ω–∞")
