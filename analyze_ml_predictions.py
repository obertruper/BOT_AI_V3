#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –∞–Ω–∞–ª–∏–∑–∞ ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–æ—á–Ω–æ—Å—Ç–∏, –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –∏ –¥—Ä—É–≥–∏–µ –º–µ—Ç—Ä–∏–∫–∏
"""

import asyncio
from datetime import UTC, datetime, timedelta

import numpy as np
import pandas as pd

from core.logger import setup_logger
from database.connections.postgres import AsyncPGPool

logger = setup_logger("ml_predictions_analyzer")


class MLPredictionsAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏–∑ –ë–î"""

    def __init__(self):
        self.predictions_df = None

    async def load_predictions(self, days_back: int = 7, symbol: str | None = None) -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏–∑ –ë–î –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥

        Args:
            days_back: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥
            symbol: –§–∏–ª—å—Ç—Ä –ø–æ —Å–∏–º–≤–æ–ª—É (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        try:
            query = """
                SELECT *
                FROM ml_predictions
                WHERE datetime >= $1
            """
            params = [datetime.now(UTC) - timedelta(days=days_back)]

            if symbol:
                query += " AND symbol = $2"
                params.append(symbol)

            query += " ORDER BY datetime DESC"

            rows = await AsyncPGPool.fetch(query, *params)

            if not rows:
                logger.warning("–ù–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –≤ –ë–î –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥")
                return pd.DataFrame()

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DataFrame
            self.predictions_df = pd.DataFrame([dict(row) for row in rows])
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.predictions_df)} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")

            return self.predictions_df

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {e}")
            return pd.DataFrame()

    async def calculate_accuracy_metrics(self) -> dict:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
        if self.predictions_df is None or self.predictions_df.empty:
            return {}

        df = self.predictions_df
        metrics = {}

        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
        for timeframe in ["15m", "1h", "4h", "12h"]:
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–∞–ø–∏—Å–∏ –≥–¥–µ –µ—Å—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            mask = df[f"actual_direction_{timeframe}"].notna()
            valid_df = df[mask]

            if valid_df.empty:
                continue

            # –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
            correct = (
                valid_df[f"direction_{timeframe}"] == valid_df[f"actual_direction_{timeframe}"]
            ).sum()
            total = len(valid_df)
            accuracy = correct / total if total > 0 else 0

            # –°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
            if f"actual_return_{timeframe}" in valid_df.columns:
                return_errors = (
                    valid_df[f"predicted_return_{timeframe}"]
                    - valid_df[f"actual_return_{timeframe}"]
                ).abs()
                mae = return_errors.mean()
                rmse = np.sqrt((return_errors**2).mean())
            else:
                mae = rmse = None

            metrics[timeframe] = {
                "accuracy": accuracy,
                "total_predictions": total,
                "correct_predictions": correct,
                "mae": mae,
                "rmse": rmse,
                "avg_confidence": valid_df[f"direction_{timeframe}_confidence"].mean(),
            }

        return metrics

    async def analyze_by_confidence(self) -> pd.DataFrame:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ —É—Ä–æ–≤–Ω—è–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        if self.predictions_df is None or self.predictions_df.empty:
            return pd.DataFrame()

        df = self.predictions_df
        results = []

        # –ì—Ä—É–ø–ø—ã —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        confidence_bins = [(0, 0.5), (0.5, 0.6), (0.6, 0.7), (0.7, 0.8), (0.8, 1.0)]

        for low, high in confidence_bins:
            mask = (df["signal_confidence"] >= low) & (df["signal_confidence"] < high)
            group_df = df[mask]

            if group_df.empty:
                continue

            # –°—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –≥—Ä—É–ø–ø—ã
            results.append(
                {
                    "confidence_range": f"{low:.0%}-{high:.0%}",
                    "count": len(group_df),
                    "avg_confidence": group_df["signal_confidence"].mean(),
                    "long_ratio": (group_df["signal_type"] == "LONG").mean(),
                    "short_ratio": (group_df["signal_type"] == "SHORT").mean(),
                    "neutral_ratio": (group_df["signal_type"] == "NEUTRAL").mean(),
                }
            )

        return pd.DataFrame(results)

    async def analyze_by_symbol(self) -> pd.DataFrame:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å–∏–º–≤–æ–ª–∞–º"""
        if self.predictions_df is None or self.predictions_df.empty:
            return pd.DataFrame()

        df = self.predictions_df

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        symbol_stats = df.groupby("symbol").agg(
            {
                "signal_confidence": ["mean", "std"],
                "signal_type": lambda x: (x == "LONG").sum(),
                "predicted_return_15m": "mean",
                "predicted_return_1h": "mean",
                "risk_score": "mean",
            }
        )

        symbol_stats.columns = ["_".join(col).strip() for col in symbol_stats.columns]
        symbol_stats["total_predictions"] = df.groupby("symbol").size()

        return symbol_stats.sort_values("total_predictions", ascending=False)

    async def analyze_feature_statistics(self) -> dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        if self.predictions_df is None or self.predictions_df.empty:
            return {}

        df = self.predictions_df

        return {
            "avg_features_count": df["features_count"].mean(),
            "avg_nan_count": df["nan_count"].mean(),
            "avg_zero_variance_count": df["zero_variance_count"].mean(),
            "features_mean": {
                "avg": df["features_mean"].mean(),
                "std": df["features_mean"].std(),
                "min": df["features_mean"].min(),
                "max": df["features_mean"].max(),
            },
            "features_std": {
                "avg": df["features_std"].mean(),
                "std": df["features_std"].std(),
                "min": df["features_std"].min(),
                "max": df["features_std"].max(),
            },
        }

    async def print_analysis_report(self):
        """–í—ã–≤–æ–¥–∏—Ç –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç –∞–Ω–∞–ª–∏–∑–∞"""
        print("\n" + "=" * 80)
        print(" ML PREDICTIONS ANALYSIS REPORT ".center(80, "="))
        print("=" * 80 + "\n")

        if self.predictions_df is None or self.predictions_df.empty:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return

        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print("-" * 40)
        print(f"–í—Å–µ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {len(self.predictions_df)}")
        print(
            f"–ü–µ—Ä–∏–æ–¥: {self.predictions_df['datetime'].min()} - {self.predictions_df['datetime'].max()}"
        )
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤: {self.predictions_df['symbol'].nunique()}")
        print(f"–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {self.predictions_df['signal_confidence'].mean():.2%}")
        print()

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–æ–≤
        signal_dist = self.predictions_df["signal_type"].value_counts()
        print("üìà –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –°–ò–ì–ù–ê–õ–û–í")
        print("-" * 40)
        for signal_type, count in signal_dist.items():
            print(f"{signal_type}: {count} ({count / len(self.predictions_df):.1%})")
        print()

        # –ú–µ—Ç—Ä–∏–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏
        accuracy_metrics = await self.calculate_accuracy_metrics()
        if accuracy_metrics:
            print("üéØ –ú–ï–¢–†–ò–ö–ò –¢–û–ß–ù–û–°–¢–ò")
            print("-" * 40)
            for timeframe, metrics in accuracy_metrics.items():
                if metrics["total_predictions"] > 0:
                    print(f"\n{timeframe.upper()}:")
                    print(f"  –¢–æ—á–Ω–æ—Å—Ç—å: {metrics['accuracy']:.2%}")
                    print(f"  –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {metrics['total_predictions']}")
                    print(f"  –ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö: {metrics['correct_predictions']}")
                    print(f"  –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {metrics['avg_confidence']:.2%}")
                    if metrics["mae"] is not None:
                        print(f"  MAE –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏: {metrics['mae']:.4f}")
                        print(f"  RMSE –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏: {metrics['rmse']:.4f}")

        # –ê–Ω–∞–ª–∏–∑ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        confidence_analysis = await self.analyze_by_confidence()
        if not confidence_analysis.empty:
            print("\nüíé –ê–ù–ê–õ–ò–ó –ü–û –£–í–ï–†–ï–ù–ù–û–°–¢–ò")
            print("-" * 40)
            print(confidence_analysis.to_string(index=False))

        # –ê–Ω–∞–ª–∏–∑ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        symbol_analysis = await self.analyze_by_symbol()
        if not symbol_analysis.empty:
            print("\nüìä –¢–û–ü-10 –°–ò–ú–í–û–õ–û–í")
            print("-" * 40)
            print(symbol_analysis.head(10).to_string())

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_stats = await self.analyze_feature_statistics()
        if feature_stats:
            print("\nüî¨ –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–ò–ó–ù–ê–ö–û–í")
            print("-" * 40)
            print(f"–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {feature_stats['avg_features_count']:.0f}")
            print(f"–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ NaN: {feature_stats['avg_nan_count']:.1f}")
            print(
                f"–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ zero variance: {feature_stats['avg_zero_variance_count']:.1f}"
            )
            print(f"Features mean - avg: {feature_stats['features_mean']['avg']:.4f}")
            print(f"Features std - avg: {feature_stats['features_std']['avg']:.4f}")

        # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        if "inference_time_ms" in self.predictions_df.columns:
            print("\n‚ö° –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨")
            print("-" * 40)
            print(
                f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è inference: {self.predictions_df['inference_time_ms'].mean():.1f} ms"
            )
            print(f"–ú–µ–¥–∏–∞–Ω–∞: {self.predictions_df['inference_time_ms'].median():.1f} ms")
            print(
                f"95 percentile: {self.predictions_df['inference_time_ms'].quantile(0.95):.1f} ms"
            )

        print("\n" + "=" * 80)


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    analyzer = MLPredictionsAnalyzer()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π
    await analyzer.load_predictions(days_back=7)

    # –í—ã–≤–æ–¥–∏–º –æ—Ç—á–µ—Ç
    await analyzer.print_analysis_report()

    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
    try:
        if analyzer.predictions_df is not None and not analyzer.predictions_df.empty:
            analyzer.predictions_df.to_csv(
                f"ml_predictions_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                index=False,
            )
            print("\n‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ CSV —Ñ–∞–π–ª")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ CSV: {e}")


if __name__ == "__main__":
    asyncio.run(main())
