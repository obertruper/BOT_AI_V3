"""
–ò–Ω–∂–µ–Ω–µ—Ä–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö - BOT_AI_V3 –≤–µ—Ä—Å–∏—è
–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –∏–∑ LLM TRANSFORM –ø—Ä–æ–µ–∫—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:
- RobustScaler –≤–º–µ—Å—Ç–æ StandardScaler
- Walk-forward validation
- Crypto-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
- –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –¥–µ–ª–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ NaN/Inf
"""

import warnings

import numpy as np
import pandas as pd
import ta
from sklearn.preprocessing import RobustScaler
from tqdm import tqdm

# –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º

warnings.filterwarnings("ignore")

# BOT_AI_V3 imports
from core.logger import setup_logger


class FeatureEngineer:
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è - BOT_AI_V3 –≤–µ—Ä—Å–∏—è"""

    def __init__(self, config: dict, inference_mode: bool = False):
        self.config = config
        self.logger = setup_logger(__name__)
        self.feature_config = config.get("features", {})
        self.scalers = {}
        self.process_position = None  # –ü–æ–∑–∏—Ü–∏—è –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–≤ –ø—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ
        self.disable_progress = False  # –§–ª–∞–≥ –¥–ª—è –æ—Ç–∫–ª—é—á–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–≤

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ inference —Ä–µ–∂–∏–º–∞
        self._is_inference_mode = inference_mode

        # –°—Ä–∞–∑—É –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –µ—Å–ª–∏ inference_mode –≤–∫–ª—é—á–µ–Ω
        if inference_mode:
            try:
                from ml.config.features_240 import REQUIRED_FEATURES_240

                self._required_features = REQUIRED_FEATURES_240
                self.logger.info(
                    f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω —Å–ø–∏—Å–æ–∫ –∏–∑ {len(REQUIRED_FEATURES_240)} —Ç—Ä–µ–±—É–µ–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è inference mode"
                )
            except ImportError:
                self.logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å REQUIRED_FEATURES_240")
                self._required_features = None

        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–æ–¥—ã-–∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –∫–æ–¥–æ–º
        if not hasattr(self.logger, "start_stage"):
            self.logger.start_stage = lambda *args, **kwargs: self.logger.info(
                f"Starting stage: {args[0] if args else 'unknown'}"
            )
        if not hasattr(self.logger, "end_stage"):
            self.logger.end_stage = lambda *args, **kwargs: self.logger.info(
                f"Completed stage: {args[0] if args else 'unknown'}"
            )

    @staticmethod
    def safe_divide(
        numerator: pd.Series,
        denominator: pd.Series,
        fill_value=0.0,
        max_value=1000.0,
        min_denominator=1e-8,
    ) -> pd.Series:
        """–ò–°–ü–†–ê–í–õ–ï–ù–û: –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –¥–µ–ª–µ–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –º–∞–ª—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∑–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å
        safe_denominator = denominator.copy()

        # –ó–∞–º–µ–Ω—è–µ–º –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        mask_small = safe_denominator.abs() < min_denominator
        safe_denominator[mask_small] = min_denominator

        # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–ª–µ–Ω–∏–µ
        result = numerator / safe_denominator

        # –ö–ª–∏–ø–ø–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        result = result.clip(lower=-max_value, upper=max_value)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ inf –∏ nan
        result = result.replace([np.inf, -np.inf], [fill_value, fill_value])
        result = result.fillna(fill_value)

        return result

    def calculate_vwap(self, df: pd.DataFrame) -> pd.Series:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç VWAP —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏"""
        # –ë–∞–∑–æ–≤—ã–π —Ä–∞—Å—á–µ—Ç VWAP
        vwap = self.safe_divide(df["turnover"], df["volume"], fill_value=df["close"])

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: VWAP –Ω–µ –¥–æ–ª–∂–µ–Ω —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –æ—Ç close
        # –ï—Å–ª–∏ VWAP —Å–ª–∏—à–∫–æ–º –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç close (–±–æ–ª–µ–µ —á–µ–º –≤ 2 —Ä–∞–∑–∞), –∏—Å–ø–æ–ª—å–∑—É–µ–º close
        mask_invalid = (vwap < df["close"] * 0.5) | (vwap > df["close"] * 2.0)
        vwap[mask_invalid] = df["close"][mask_invalid]

        return vwap

    def create_features(
        self,
        df: pd.DataFrame,
        train_end_date: str | None = None,
        use_enhanced_features: bool = False,
        inference_mode: bool = False,
    ) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π

        Args:
            df: DataFrame —Å raw –¥–∞–Ω–Ω—ã–º–∏
            train_end_date: –¥–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –¥–ª—è walk-forward –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            use_enhanced_features: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è direction prediction
            inference_mode: –µ—Å–ª–∏ True, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ 240 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è inference
        """
        if not self.disable_progress:
            mode_str = " (inference mode - 240 features)" if inference_mode else ""
            self.logger.info(
                f"üîß –ù–∞—á–∏–Ω–∞–µ–º feature engineering –¥–ª—è {df['symbol'].nunique()} —Å–∏–º–≤–æ–ª–æ–≤{mode_str}"
            )

        # –í inference mode –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã
        if inference_mode and not hasattr(self, "_required_features"):
            try:
                from ml.config.features_240 import REQUIRED_FEATURES_240

                self._required_features = REQUIRED_FEATURES_240
            except ImportError:
                self.logger.warning(
                    "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å REQUIRED_FEATURES_240, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"
                )
                inference_mode = False
                self._required_features = None

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        self._validate_data(df)

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–æ–±–∞–≤–ª—è–µ–º turnover –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç (–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–ª—è —Ä–∞—Å—á–µ—Ç–æ–≤)
        if "turnover" not in df.columns:
            df["turnover"] = df["close"] * df["volume"]
            if not self.disable_progress:
                self.logger.info("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω —Å—Ç–æ–ª–±–µ—Ü turnover (close * volume)")

        featured_dfs = []
        all_symbols_data = {}  # –î–ª—è enhanced features

        # –ü–µ—Ä–≤—ã–π –ø—Ä–æ—Ö–æ–¥ - –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        for symbol in df["symbol"].unique():
            symbol_data = df[df["symbol"] == symbol].copy()

            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ datetime –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ –∏–ª–∏ –∫–æ–ª–æ–Ω–∫–∏
            if "datetime" in symbol_data.columns:
                symbol_data = symbol_data.sort_values("datetime")
            else:
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–Ω–¥–µ–∫—Å—É, –µ—Å–ª–∏ datetime –≤ –∏–Ω–¥–µ–∫—Å–µ
                symbol_data = symbol_data.sort_index()

            symbol_data = self._create_basic_features(symbol_data)
            symbol_data = self._create_technical_indicators(symbol_data)
            symbol_data = self._create_microstructure_features(symbol_data)
            symbol_data = self._create_rally_detection_features(symbol_data)
            symbol_data = self._create_signal_quality_features(symbol_data)
            symbol_data = self._create_futures_specific_features(symbol_data)
            symbol_data = self._create_ml_optimized_features(symbol_data)
            symbol_data = self._create_temporal_features(symbol_data)
            symbol_data = self._create_target_variables(symbol_data)

            featured_dfs.append(symbol_data)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è enhanced features
            if use_enhanced_features:
                all_symbols_data[symbol] = symbol_data.copy()

        result_df = pd.concat(featured_dfs, ignore_index=True)

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: cross-asset features –≤—Å–µ–≥–¥–∞ –Ω—É–∂–Ω—ã –≤ inference mode
        # –í inference mode —Å–æ–∑–¥–∞–µ–º cross-asset features –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤
        if df["symbol"].nunique() > 1 or (inference_mode and self._is_inference_mode):
            result_df = self._create_cross_asset_features(result_df)

        # –î–æ–±–∞–≤–ª—è–µ–º enhanced features –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–æ
        if use_enhanced_features:
            result_df = self._add_enhanced_features(result_df, all_symbols_data)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ NaN –∑–Ω–∞—á–µ–Ω–∏–π
        result_df = self._handle_missing_values(result_df)

        # Walk-forward –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ –¥–∞—Ç–∞ (–∏–Ω–∞—á–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç –≤ prepare_trading_data.py)
        if train_end_date:
            result_df = self._normalize_walk_forward(result_df, train_end_date)

        # –í inference mode –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if inference_mode and hasattr(self, "_required_features"):
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata_cols = ["symbol", "datetime", "open", "high", "low", "close", "volume"]
            keep_cols = metadata_cols.copy()

            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–æ–±–∞–≤–ª—è–µ–º extra_feature –ø—Ä–∏–∑–Ω–∞–∫–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
            extra_features_to_add = [
                "extra_feature_12",
                "extra_feature_14",
                "extra_feature_16",
                "extra_feature_24",
                "extra_feature_48",
                "extra_feature_96",
                "extra_feature_192",
                "extra_feature_384",
                "extra_feature_768",
                "extra_feature_960",
            ]
            for extra_feat in extra_features_to_add:
                if extra_feat not in result_df.columns:
                    # –°–æ–∑–¥–∞–µ–º placeholder –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –Ω—É–ª–µ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                    result_df[extra_feat] = 0.0

            if not self.disable_progress:
                self.logger.info(
                    f"üîß Inference mode –∞–∫—Ç–∏–≤–µ–Ω: —Ç—Ä–µ–±—É–µ—Ç—Å—è {len(self._required_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
                )
                self.logger.info(f"üîß –î–æ—Å—Ç—É–ø–Ω–æ –∫–æ–ª–æ–Ω–æ–∫: {len(result_df.columns)}")

            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ REQUIRED_FEATURES_240, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ DataFrame
            missing_features = []

            for feature in self._required_features:
                if feature in result_df.columns:
                    keep_cols.append(feature)
                else:
                    # –ï—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞ –Ω–µ—Ç, —Å–æ–∑–¥–∞–µ–º –µ–≥–æ —Å –Ω—É–ª–µ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                    result_df[feature] = 0.0
                    keep_cols.append(feature)
                    missing_features.append(feature)

            if missing_features and not self.disable_progress:
                self.logger.warning(
                    f"üîß –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(missing_features)}): {missing_features[:10]}..."
                )

            # –§–∏–ª—å—Ç—Ä—É–µ–º DataFrame: –≤ inference mode –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –¢–û–õ–¨–ö–û –ø—Ä–∏–∑–Ω–∞–∫–∏ –±–µ–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            feature_cols = [col for col in keep_cols if col not in metadata_cols]
            # –ù–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (–¥–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–Ω—Ü–µ)
            final_cols = feature_cols + metadata_cols
            result_df = result_df[final_cols]

            if not self.disable_progress:
                self.logger.info(
                    f"üìä Inference mode: –æ—Å—Ç–∞–≤–ª–µ–Ω–æ {len(feature_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ {len(self._required_features)} —Ç—Ä–µ–±—É–µ–º—ã—Ö"
                )
        elif inference_mode and not hasattr(self, "_required_features"):
            self.logger.error("‚ùå Inference mode –≤–∫–ª—é—á–µ–Ω, –Ω–æ _required_features –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ!")

        self._log_feature_statistics(result_df)

        if not self.disable_progress:
            feature_count = (
                len(result_df.columns) - 7 if inference_mode else len(result_df.columns)
            )  # –í—ã—á–∏—Ç–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            self.logger.info(
                f"‚úÖ Feature engineering –∑–∞–≤–µ—Ä—à–µ–Ω. –°–æ–∑–¥–∞–Ω–æ {feature_count} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {len(result_df)} –∑–∞–ø–∏—Å–µ–π"
            )

        return result_df

    def _validate_data(self, df: pd.DataFrame):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö"""
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–∏–ø—ã
        numeric_columns = ["open", "high", "low", "close", "volume", "turnover"]
        for col in numeric_columns:
            if col in df.columns:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø, –∑–∞–º–µ–Ω—è—è –æ—à–∏–±–∫–∏ –Ω–∞ NaN
                df[col] = pd.to_numeric(df[col], errors="coerce")
                # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                df[col] = df[col].ffill().bfill()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        if df.isnull().any().any():
            if not self.disable_progress:
                self.logger.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∞–Ω–Ω—ã—Ö")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ —Ü–µ–Ω—ã
        price_changes = df.groupby("symbol")["close"].pct_change()
        extreme_moves = abs(price_changes) > 0.15  # >15% –∑–∞ 15 –º–∏–Ω—É—Ç

        if extreme_moves.sum() > 0:
            if not self.disable_progress:
                self.logger.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {extreme_moves.sum()} —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π —Ü–µ–Ω—ã")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≥—ç–ø–æ–≤ (—Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞–∑—Ä—ã–≤—ã > 2 —á–∞—Å–æ–≤)
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –†–∞–±–æ—Ç–∞–µ–º —Å datetime –≤ –∏–Ω–¥–µ–∫—Å–µ, –∞ –Ω–µ –≤ –∫–æ–ª–æ–Ω–∫–µ
        for symbol in df["symbol"].unique():
            symbol_data = df[df["symbol"] == symbol]

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º datetime –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ –∏–ª–∏ –∏–∑ –∫–æ–ª–æ–Ω–∫–∏, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
            if "datetime" in symbol_data.columns:
                time_series = symbol_data["datetime"]
            elif hasattr(symbol_data.index, "to_series"):
                time_series = symbol_data.index.to_series()
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç –Ω–∏ –∫–æ–ª–æ–Ω–∫–∏, –Ω–∏ datetime –∏–Ω–¥–µ–∫—Å–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É
                continue

            time_diff = time_series.diff()
            expected_diff = pd.Timedelta("15 minutes")
            # –°—á–∏—Ç–∞–µ–º –±–æ–ª—å—à–∏–º–∏ —Ç–æ–ª—å–∫–æ —Ä–∞–∑—Ä—ã–≤—ã –±–æ–ª—å—à–µ 2 —á–∞—Å–æ–≤ (8 –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤)
            large_gaps = time_diff > expected_diff * 8

            if large_gaps.sum() > 0:
                if not self.disable_progress:
                    self.logger.warning(
                        f"–°–∏–º–≤–æ–ª {symbol}: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ {large_gaps.sum()} –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞–∑—Ä—ã–≤–æ–≤ (> 2 —á–∞—Å–æ–≤)"
                    )

    def _create_basic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ OHLCV –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ look-ahead bias"""
        df["returns"] = np.log(df["close"] / df["close"].shift(1))

        # –î–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –∑–∞ —Ä–∞–∑–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã (–†–ê–°–®–ò–†–ï–ù–û –¥–ª—è REQUIRED_FEATURES_240)
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–æ–±–∞–≤–ª–µ–Ω—ã –≤—Å–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø–µ—Ä–∏–æ–¥—ã –∏–∑ features_240.py
        returns_periods = [
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            13,
            15,
            20,
            25,
            30,
            35,
            40,
            45,
            50,
            60,
            70,
            80,
            90,
            100,
            120,
            150,
            200,
        ]
        for period in returns_periods:
            df[f"returns_{period}"] = np.log(df["close"] / df["close"].shift(period))

        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –∑–∞ —Ä–∞–∑–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã (–ò–°–ü–†–ê–í–õ–ï–ù–û –¥–ª—è REQUIRED_FEATURES_240)
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–æ–±–∞–≤–ª–µ–Ω—ã –≤—Å–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø–µ—Ä–∏–æ–¥—ã –≤–∫–ª—é—á–∞—è volatility_2, volatility_3
        volatility_periods = [
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            13,
            15,
            20,
            25,
            30,
            35,
            40,
            45,
            50,
            60,
            70,
            80,
            90,
            100,
            150,
            200,
        ]
        for period in volatility_periods:
            df[f"volatility_{period}"] = (
                df["returns"].rolling(period, min_periods=max(1, period // 2)).std()
            )

        # –ú–∏–∫—Ä–æ—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–î–û–ë–ê–í–õ–ï–ù–û –¥–ª—è REQUIRED_FEATURES_240)
        # –°–ø—Ä–µ–¥ high-low (bid-ask spread –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è)
        df["spread"] = self.safe_divide(df["high"] - df["low"], df["close"], fill_value=0.0)
        df["spread_ma_10"] = df["spread"].rolling(10, min_periods=5).mean()
        df["spread_std_10"] = df["spread"].rolling(10, min_periods=5).std()

        # Order imbalance (–¥–∏—Å–±–∞–ª–∞–Ω—Å –æ—Ä–¥–µ—Ä–æ–≤) - –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è —á–µ—Ä–µ–∑ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏ –æ–±—ä–µ–º
        df["price_direction"] = np.sign(df["close"] - df["open"])
        df["order_imbalance"] = df["volume"] * df["price_direction"]
        df["order_imbalance_ma_10"] = df["order_imbalance"].rolling(10, min_periods=5).mean()
        df["order_imbalance_std_10"] = df["order_imbalance"].rolling(10, min_periods=5).std()

        # Buy/Sell pressure (–¥–∞–≤–ª–µ–Ω–∏–µ –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π/–ø—Ä–æ–¥–∞–≤—Ü–æ–≤)
        # –ü–æ–∑–∏—Ç–∏–≤–Ω–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ = –¥–∞–≤–ª–µ–Ω–∏–µ –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π
        df["buy_pressure"] = np.where(df["price_direction"] > 0, df["volume"], 0)
        df["sell_pressure"] = np.where(df["price_direction"] < 0, df["volume"], 0)
        df["net_pressure"] = df["buy_pressure"] - df["sell_pressure"]

        df["buy_pressure_ma_10"] = df["buy_pressure"].rolling(10, min_periods=5).mean()
        df["sell_pressure_ma_10"] = df["sell_pressure"].rolling(10, min_periods=5).mean()
        df["net_pressure_ma_10"] = df["net_pressure"].rolling(10, min_periods=5).mean()

        # Order flow –ø—Ä–∏–∑–Ω–∞–∫–∏
        for period in [5, 10, 20]:
            df[f"order_flow_{period}"] = (
                df["order_imbalance"].rolling(period, min_periods=max(1, period // 2)).sum()
            )
            # Order flow ratio
            total_volume = df["volume"].rolling(period, min_periods=max(1, period // 2)).sum()
            df[f"order_flow_ratio_{period}"] = self.safe_divide(
                df[f"order_flow_{period}"], total_volume, fill_value=0.0
            )

        # –¶–µ–Ω–æ–≤—ã–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è
        df["high_low_ratio"] = df["high"] / df["low"]
        df["close_open_ratio"] = df["close"] / df["open"]

        # –ü–æ–∑–∏—Ü–∏—è –∑–∞–∫—Ä—ã—Ç–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ
        df["close_position"] = (df["close"] - df["low"]) / (df["high"] - df["low"] + 1e-10)

        # –û–±—ä–µ–º–Ω—ã–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–æ–ª—å–∫–æ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        df["volume_ratio"] = self.safe_divide(
            df["volume"], df["volume"].rolling(20, min_periods=20).mean(), fill_value=1.0
        )
        df["turnover_ratio"] = self.safe_divide(
            df["turnover"], df["turnover"].rolling(20, min_periods=20).mean(), fill_value=1.0
        )

        # VWAP —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ä–∞—Å—á–µ—Ç–æ–º
        df["vwap"] = self.calculate_vwap(df)

        # –ë–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π —Ä–∞—Å—á–µ—Ç close_vwap_ratio
        # –ù–æ—Ä–º–∞–ª—å–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ close/vwap –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–∫–æ–ª–æ 1.0
        # VWAP —É–∂–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω –≤ calculate_vwap()

        # –ü—Ä–æ—Å—Ç–æ–π –∏ –Ω–∞–¥–µ–∂–Ω—ã–π —Ä–∞—Å—á–µ—Ç
        df["close_vwap_ratio"] = df["close"] / df["vwap"]

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç (¬±30%)
        # –ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã –º–æ–≥—É—Ç –æ—Ç–∫–ª–æ–Ω—è—Ç—å—Å—è –æ—Ç VWAP –Ω–∞ 20-50% –≤ –ø–µ—Ä–∏–æ–¥—ã –≤—ã—Å–æ–∫–æ–π –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        df["close_vwap_ratio"] = df["close_vwap_ratio"].clip(lower=0.7, upper=1.3)

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –æ—Ç VWAP
        df["vwap_extreme_deviation"] = (
            (df["close_vwap_ratio"] < 0.85) | (df["close_vwap_ratio"] > 1.15)
        ).astype(int)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–Ω–æ–º–∞–ª–∏–∏
        # –ï—Å–ª–∏ ratio –≤—Å–µ –µ—â–µ –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ —Ä–∞–∑—É–º–Ω—ã–µ –ø—Ä–µ–¥–µ–ª—ã, –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ 1.0
        mask_invalid = (df["close_vwap_ratio"] < 0.95) | (df["close_vwap_ratio"] > 1.05)
        if mask_invalid.sum() > 0:
            self.logger.debug(f"–ó–∞–º–µ–Ω–µ–Ω–æ {mask_invalid.sum()} –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö close_vwap_ratio –Ω–∞ 1.0")
            df.loc[mask_invalid, "close_vwap_ratio"] = 1.0

        return df

    def _create_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ò–°–ü–†–ê–í–õ–ï–ù–û: –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –í–°–ï —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∏–∑ REQUIRED_FEATURES_240"""

        # ========== RSI - –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–µ—Ä–∏–æ–¥—ã ==========
        rsi_periods = [5, 14, 21]  # –ò–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ features_240.py
        for period in rsi_periods:
            rsi_indicator = ta.momentum.RSIIndicator(df["close"], window=period)
            df[f"rsi_{period}"] = rsi_indicator.rsi()

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ RSI –ø–µ—Ä–∏–æ–¥—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        for period in [7, 30, 50, 70, 100]:
            rsi_indicator = ta.momentum.RSIIndicator(df["close"], window=period)
            df[f"rsi_{period}"] = rsi_indicator.rsi()

        # ========== SMA - –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–µ—Ä–∏–æ–¥—ã ==========
        sma_periods = [5, 10, 20, 50, 100, 200]  # –ò–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ features_240.py
        for period in sma_periods:
            df[f"sma_{period}"] = ta.trend.sma_indicator(df["close"], period)

        # ========== EMA - –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–µ—Ä–∏–æ–¥—ã ==========
        ema_periods = [5, 10, 20, 50, 100, 200]  # –ò–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ features_240.py
        for period in ema_periods:
            df[f"ema_{period}"] = ta.trend.ema_indicator(df["close"], period)
            # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –æ—Ç —Ü–µ–Ω—ã –¥–æ EMA (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ)
            df[f"ema_distance_{period}"] = self.safe_divide(
                df["close"] - df[f"ema_{period}"], df["close"], fill_value=0.0
            )

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ EMA –ø–µ—Ä–∏–æ–¥—ã
        for period in [15, 30, 150, 250]:
            df[f"ema_{period}"] = ta.trend.ema_indicator(df["close"], period)
            df[f"ema_distance_{period}"] = self.safe_divide(
                df["close"] - df[f"ema_{period}"], df["close"], fill_value=0.0
            )

        # ========== MACD - –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ==========
        macd_configs = [
            (5, 13, 5),  # –ë—ã—Å—Ç—Ä—ã–π MACD
            (5, 35, 5),  # –¢–†–ï–ë–£–ï–¢–°–Ø: –¥–ª—è REQUIRED_FEATURES_240
            (8, 21, 5),  # –°—Ä–µ–¥–Ω–∏–π MACD
            (12, 26, 9),  # –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π MACD
            (19, 39, 9),  # –ú–µ–¥–ª–µ–Ω–Ω—ã–π MACD
            (50, 100, 20),  # –û—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω—ã–π MACD
        ]

        for fast, slow, signal in macd_configs:
            macd = ta.trend.MACD(
                df["close"], window_fast=fast, window_slow=slow, window_sign=signal
            )
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º MACD –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ü–µ–Ω—ã
            df[f"macd_{fast}_{slow}"] = self.safe_divide(macd.macd(), df["close"], fill_value=0.0)
            df[f"macd_signal_{fast}_{slow}"] = self.safe_divide(
                macd.macd_signal(), df["close"], fill_value=0.0
            )
            df[f"macd_hist_{fast}_{slow}"] = self.safe_divide(
                macd.macd_diff(), df["close"], fill_value=0.0
            )

        # ========== Bollinger Bands - –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–µ—Ä–∏–æ–¥—ã ==========
        bb_periods = [10, 20, 30, 50, 100]
        for period in bb_periods:
            bb = ta.volatility.BollingerBands(df["close"], window=period, window_dev=2)
            df[f"bb_upper_{period}"] = bb.bollinger_hband()
            df[f"bb_middle_{period}"] = bb.bollinger_mavg()
            df[f"bb_lower_{period}"] = bb.bollinger_lband()

            # BB width (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è)
            df[f"bb_width_{period}"] = self.safe_divide(
                bb.bollinger_hband() - bb.bollinger_lband(), df["close"], fill_value=0.02
            )

            # BB position (–≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —Ü–µ–Ω–∞ –≤–Ω—É—Ç—Ä–∏ –ø–æ–ª–æ—Å)
            bb_range = bb.bollinger_hband() - bb.bollinger_lband()
            df[f"bb_position_{period}"] = self.safe_divide(
                df["close"] - bb.bollinger_lband(), bb_range, fill_value=0.5
            ).clip(0, 1)

        # ========== ATR - –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–µ—Ä–∏–æ–¥—ã ==========
        atr_periods = [7, 14, 21, 30, 50, 100]
        for period in atr_periods:
            atr = ta.volatility.AverageTrueRange(df["high"], df["low"], df["close"], window=period)
            df[f"atr_{period}"] = atr.average_true_range()

        # ========== ADX - –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–µ—Ä–∏–æ–¥—ã ==========
        adx_periods = [14, 21, 20, 30]  # –î–æ–±–∞–≤–ª–µ–Ω –ø–µ—Ä–∏–æ–¥ 21 –¥–ª—è REQUIRED_FEATURES_240
        for period in adx_periods:
            adx = ta.trend.ADXIndicator(df["high"], df["low"], df["close"], window=period)
            df[f"adx_{period}"] = adx.adx()
            df[f"plus_di_{period}"] = adx.adx_pos()
            df[f"minus_di_{period}"] = adx.adx_neg()
            df[f"dx_{period}"] = df[f"plus_di_{period}"] - df[f"minus_di_{period}"]

        # ========== CCI - –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–µ—Ä–∏–æ–¥—ã ==========
        cci_periods = [14, 20, 30]
        for period in cci_periods:
            cci = ta.trend.CCIIndicator(df["high"], df["low"], df["close"], window=period)
            df[f"cci_{period}"] = cci.cci()

        # ========== Stochastic - –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–µ—Ä–∏–æ–¥—ã ==========
        stoch_configs = [(14, 3), (21, 3)]  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–∏–æ–¥ 3 –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        for k_period, d_period in stoch_configs:
            stoch = ta.momentum.StochasticOscillator(
                df["high"], df["low"], df["close"], window=k_period, smooth_window=d_period
            )
            df[f"stoch_k_{k_period}"] = stoch.stoch()  # –ò–°–ü–†–ê–í–õ–ï–ù–û: —É–±–∏—Ä–∞–µ–º d_period –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è
            df[f"stoch_d_{k_period}"] = (
                stoch.stoch_signal()
            )  # –ò–°–ü–†–ê–í–õ–ï–ù–û: —É–±–∏—Ä–∞–µ–º d_period –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ stochastic —Å –ø–µ—Ä–∏–æ–¥–æ–º 21 –¥–ª—è D-–ª–∏–Ω–∏–∏
        stoch_21 = ta.momentum.StochasticOscillator(
            df["high"], df["low"], df["close"], window=21, smooth_window=3
        )
        df["stoch_k_21"] = stoch_21.stoch()
        df["stoch_d_21"] = stoch_21.stoch_signal()

        # ========== Williams %R - –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–µ—Ä–∏–æ–¥—ã ==========
        willr_periods = [14, 21, 28]
        for period in willr_periods:
            willr = ta.momentum.WilliamsRIndicator(df["high"], df["low"], df["close"], lbp=period)
            df[f"williams_r_{period}"] = willr.williams_r()

        # –î–û–ë–ê–í–õ–ï–ù–û: Williams %R –¥–ª—è –≤—Å–µ—Ö –ø–µ—Ä–∏–æ–¥–æ–≤ –∏–∑ REQUIRED_FEATURES_240

        # ========== MFI - –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–µ—Ä–∏–æ–¥—ã ==========
        mfi_periods = [14, 21, 20, 30]  # –î–æ–±–∞–≤–ª–µ–Ω –ø–µ—Ä–∏–æ–¥ 21 –¥–ª—è REQUIRED_FEATURES_240
        for period in mfi_periods:
            mfi = ta.volume.MFIIndicator(
                df["high"], df["low"], df["close"], df["volume"], window=period
            )
            df[f"mfi_{period}"] = mfi.money_flow_index()

        # ========== OBV –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ ==========
        obv = ta.volume.OnBalanceVolumeIndicator(df["close"], df["volume"])
        df["obv"] = obv.on_balance_volume()

        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π OBV
        df["obv_norm"] = self.safe_divide(
            df["obv"] - df["obv"].rolling(50).mean(), df["obv"].rolling(50).std(), fill_value=0.0
        )

        # OBV SMA –∏ EMA
        df["obv_sma_20"] = ta.trend.sma_indicator(df["obv"], 20)
        df["obv_ema_20"] = ta.trend.ema_indicator(df["obv"], 20)

        # ========== –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã ==========

        # Aroon
        for period in [14, 25]:
            aroon = ta.trend.AroonIndicator(df["high"], df["low"], window=period)
            df[f"aroon_up_{period}"] = aroon.aroon_up()
            df[f"aroon_down_{period}"] = aroon.aroon_down()

        # DPO (Detrended Price Oscillator)
        for period in [14, 20]:
            dpo = ta.trend.DPOIndicator(df["close"], window=period)
            df[f"dpo_{period}"] = dpo.dpo()

        # CMO (Chande Momentum Oscillator)
        for period in [14, 20]:
            cmo = ta.momentum.PercentageVolumeOscillator(
                df["volume"], window_slow=period * 2, window_fast=period
            )
            df[f"cmo_{period}"] = cmo.pvo()

        # ROC (Rate of Change)
        for period in [10, 20, 30, 50]:
            roc = ta.momentum.ROCIndicator(df["close"], window=period)
            df[f"roc_{period}"] = roc.roc()

        # RVI (Relative Vigor Index)
        for period in [10, 14]:
            # –ü—Ä–æ—Å—Ç–∞—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è RVI
            df[f"rvi_{period}"] = self.safe_divide(
                (df["close"] - df["open"]).rolling(period).mean(),
                (df["high"] - df["low"]).rolling(period).mean(),
                fill_value=0.0,
            )

        # TRIX
        for period in [14, 21, 30]:
            # –¢—Ä–æ–π–Ω–∞—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è —Å–∫–æ–ª—å–∑—è—â–∞—è —Å—Ä–µ–¥–Ω—è—è
            ema1 = ta.trend.ema_indicator(df["close"], period)
            ema2 = ta.trend.ema_indicator(ema1, period)
            ema3 = ta.trend.ema_indicator(ema2, period)
            df[f"trix_{period}"] = ema3.pct_change()

        # PPO (Percentage Price Oscillator)
        ppo = ta.momentum.PercentagePriceOscillator(df["close"])
        df["ppo"] = ppo.ppo()
        df["ppo_signal"] = ppo.ppo_signal()
        df["ppo_hist"] = ppo.ppo_hist()

        # Mass Index
        mass_index = ta.trend.MassIndex(df["high"], df["low"])
        df["mass_index"] = mass_index.mass_index()

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è crypto
        self._add_crypto_specific_indicators(df)

        return df

    def _add_crypto_specific_indicators(self, df: pd.DataFrame):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∫—Ä–∏–ø—Ç–æ—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã"""

        # Stochastic
        stoch = ta.momentum.StochasticOscillator(
            df["high"], df["low"], df["close"], window=14, smooth_window=3
        )
        df["stoch_k"] = stoch.stoch()
        df["stoch_d"] = stoch.stoch_signal()

        # ADX
        adx = ta.trend.ADXIndicator(df["high"], df["low"], df["close"])
        df["adx_14"] = adx.adx()
        df["adx"] = df["adx_14"]  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        df["adx_pos"] = adx.adx_pos()
        df["adx_neg"] = adx.adx_neg()

        # Parabolic SAR
        psar = ta.trend.PSARIndicator(df["high"], df["low"], df["close"])
        df["psar"] = psar.psar()
        # –í–º–µ—Å—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö psar_up –∏ psar_down, —Å–æ–∑–¥–∞–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        df["psar_trend"] = (df["close"] > df["psar"]).astype(float)

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ PSAR –ø–æ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        # –î–µ–ª–µ–Ω–∏–µ –Ω–∞ ATR –¥–µ–ª–∞–µ—Ç –º–µ—Ç—Ä–∏–∫—É —Å—Ä–∞–≤–Ω–∏–º–æ–π –º–µ–∂–¥—É –∞–∫—Ç–∏–≤–∞–º–∏
        df["psar_distance"] = (df["close"] - df["psar"]) / df["close"]
        if "atr" in df.columns:
            df["psar_distance_normalized"] = (df["close"] - df["psar"]) / (df["atr"] + 1e-10)
        else:
            df["psar_distance_normalized"] = df["psar_distance"]

        # ===== –ù–û–í–´–ï –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –ò–ù–î–ò–ö–ê–¢–û–†–´ (2024 best practices) =====

        # 1. Ichimoku Cloud - –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –≤ –∫—Ä–∏–ø—Ç–æ
        try:
            ichimoku = ta.trend.IchimokuIndicator(
                high=df["high"],
                low=df["low"],
                window1=9,  # Tenkan-sen
                window2=26,  # Kijun-sen
                window3=52,  # Senkou Span B
            )
            df["ichimoku_tenkan"] = ichimoku.ichimoku_conversion_line()  # Tenkan-sen
            df["ichimoku_kijun"] = ichimoku.ichimoku_base_line()  # Kijun-sen
            df["ichimoku_conversion"] = df["ichimoku_tenkan"]  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            df["ichimoku_base"] = df["ichimoku_kijun"]  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            df["ichimoku_span_a"] = ichimoku.ichimoku_a()
            df["ichimoku_span_b"] = ichimoku.ichimoku_b()
            # –û–±–ª–∞–∫–æ - —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É span A –∏ B
            df["ichimoku_cloud_thickness"] = (df["ichimoku_span_a"] - df["ichimoku_span_b"]) / df[
                "close"
            ]
            # –ü–æ–∑–∏—Ü–∏—è —Ü–µ–Ω—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –æ–±–ª–∞–∫–∞
            df["price_vs_cloud"] = (
                df["close"] - (df["ichimoku_span_a"] + df["ichimoku_span_b"]) / 2
            ) / df["close"]
        except:
            pass

        # 2. Keltner Channels - –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ Bollinger Bands
        try:
            keltner = ta.volatility.KeltnerChannel(
                high=df["high"], low=df["low"], close=df["close"], window=20, window_atr=10
            )
            df["keltner_upper_20"] = keltner.keltner_channel_hband()
            df["keltner_middle_20"] = keltner.keltner_channel_mband()
            df["keltner_lower_20"] = keltner.keltner_channel_lband()
            df["keltner_position_20"] = (df["close"] - df["keltner_lower_20"]) / (
                df["keltner_upper_20"] - df["keltner_lower_20"]
            )
            # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            df["keltner_upper"] = df["keltner_upper_20"]
            df["keltner_middle"] = df["keltner_middle_20"]
            df["keltner_lower"] = df["keltner_lower_20"]
            df["keltner_position"] = df["keltner_position_20"]
        except:
            pass

        # 3. Donchian Channels - –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–æ—Ä—ã–≤–æ–≤
        try:
            donchian = ta.volatility.DonchianChannel(
                high=df["high"], low=df["low"], close=df["close"], window=20
            )
            df["donchian_upper_20"] = donchian.donchian_channel_hband()
            df["donchian_middle_20"] = donchian.donchian_channel_mband()
            df["donchian_lower_20"] = donchian.donchian_channel_lband()
            df["donchian_position_20"] = (df["close"] - df["donchian_lower_20"]) / (
                df["donchian_upper_20"] - df["donchian_lower_20"]
            )
            # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ—Ä—ã–≤–∞
            df["donchian_breakout"] = (
                (df["close"] > df["donchian_upper_20"].shift(1))
                | (df["close"] < df["donchian_lower_20"].shift(1))
            ).astype(int)
            # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            df["donchian_upper"] = df["donchian_upper_20"]
            df["donchian_middle"] = df["donchian_middle_20"]
            df["donchian_lower"] = df["donchian_lower_20"]
        except:
            pass

        # 4. Volume Weighted Moving Average (VWMA)
        df["vwma_20"] = (df["close"] * df["volume"]).rolling(20).sum() / df["volume"].rolling(
            20
        ).sum()
        df["close_vwma_ratio"] = df["close"] / df["vwma_20"]

        # 5. Money Flow Index (MFI) - –æ–±—ä–µ–º–Ω—ã–π –æ—Å—Ü–∏–ª–ª—è—Ç–æ—Ä
        try:
            mfi = ta.volume.MFIIndicator(
                high=df["high"], low=df["low"], close=df["close"], volume=df["volume"], window=14
            )
            df["mfi_14"] = mfi.money_flow_index()
            df["mfi"] = df["mfi_14"]  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            df["mfi_overbought"] = (df["mfi"] > 80).astype(int)
            df["mfi_oversold"] = (df["mfi"] < 20).astype(int)
        except:
            pass

        # 6. Commodity Channel Index (CCI)
        try:
            cci = ta.trend.CCIIndicator(
                high=df["high"], low=df["low"], close=df["close"], window=20
            )
            df["cci_14"] = cci.cci()
            df["cci"] = df["cci_14"]  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            df["cci_overbought"] = (df["cci"] > 100).astype(int)
            df["cci_oversold"] = (df["cci"] < -100).astype(int)
        except:
            pass

        # 7. Williams %R
        try:
            williams = ta.momentum.WilliamsRIndicator(
                high=df["high"], low=df["low"], close=df["close"], lbp=14
            )
            df["williams_r_14"] = williams.williams_r()
            df["williams_r"] = df["williams_r_14"]  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        except:
            pass

        # 8. On Balance Volume (OBV)
        try:
            obv_indicator = ta.volume.OnBalanceVolumeIndicator(
                close=df["close"], volume=df["volume"]
            )
            df["obv"] = obv_indicator.on_balance_volume()
            # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π OBV
            df["obv_normalized"] = df["obv"] / (df["volume"].rolling(20).mean() + 1e-10)
        except:
            pass

        # 9. Ultimate Oscillator - –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–µ—Ä–∏–æ–¥–æ–≤
        try:
            ultimate = ta.momentum.UltimateOscillator(
                high=df["high"], low=df["low"], close=df["close"], window1=7, window2=14, window3=28
            )
            df["ultimate_oscillator"] = ultimate.ultimate_oscillator()
        except:
            pass

        # 9. Accumulation/Distribution Index
        try:
            adl = ta.volume.AccDistIndexIndicator(
                high=df["high"], low=df["low"], close=df["close"], volume=df["volume"]
            )
            df["accumulation_distribution"] = adl.acc_dist_index()
        except:
            pass

        # 10. On Balance Volume (OBV)
        try:
            obv = ta.volume.OnBalanceVolumeIndicator(close=df["close"], volume=df["volume"])
            df["obv"] = obv.on_balance_volume()
            # OBV trend
            df["obv_ema"] = df["obv"].ewm(span=20).mean()
            df["obv_trend"] = (df["obv"] > df["obv_ema"]).astype(int)
        except:
            pass

        # 11. Chaikin Money Flow (CMF)
        try:
            cmf = ta.volume.ChaikinMoneyFlowIndicator(
                high=df["high"], low=df["low"], close=df["close"], volume=df["volume"], window=20
            )
            df["cmf"] = cmf.chaikin_money_flow()
        except:
            pass

        # 12. Average Directional Movement Index Rating (ADXR)
        try:
            adxr = ta.trend.ADXIndicator(
                high=df["high"], low=df["low"], close=df["close"], window=14
            )
            df["adxr"] = adxr.adx().rolling(14).mean()  # ADXR = —Å—Ä–µ–¥–Ω–µ–µ ADX
        except:
            pass

        # 13. Aroon Indicator
        try:
            aroon = ta.trend.AroonIndicator(close=df["close"], window=25)
            df["aroon_up"] = aroon.aroon_up()
            df["aroon_down"] = aroon.aroon_down()
            df["aroon_oscillator"] = df["aroon_up"] - df["aroon_down"]
        except:
            pass

        # 14. Pivot Points (–ø–æ–¥–¥–µ—Ä–∂–∫–∞/—Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ)
        df["pivot"] = (df["high"] + df["low"] + df["close"]) / 3
        df["resistance1"] = 2 * df["pivot"] - df["low"]
        df["support1"] = 2 * df["pivot"] - df["high"]
        df["resistance2"] = df["pivot"] + (df["high"] - df["low"])
        df["support2"] = df["pivot"] - (df["high"] - df["low"])

        # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —É—Ä–æ–≤–Ω–µ–π
        df["dist_to_resistance1"] = (df["resistance1"] - df["close"]) / df["close"]
        df["dist_to_support1"] = (df["close"] - df["support1"]) / df["close"]

        # 15. Rate of Change (ROC)
        try:
            roc = ta.momentum.ROCIndicator(close=df["close"], window=10)
            df["roc"] = roc.roc()
        except:
            pass

        # 16. Trix - —Ç—Ä–æ–π–Ω–æ–µ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ
        try:
            trix = ta.trend.TRIXIndicator(close=df["close"], window=15)
            df["trix"] = trix.trix()
        except:
            pass

        return df

    def _create_microstructure_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–∏–∑–Ω–∞–∫–∏ –º–∏–∫—Ä–æ—Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ä—ã–Ω–∫–∞"""
        # –°–ø—Ä–µ–¥ high-low
        df["hl_spread"] = self.safe_divide(df["high"] - df["low"], df["close"], fill_value=0.0)
        df["hl_spread_ma"] = df["hl_spread"].rolling(20).mean()

        # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏ –æ–±—ä–µ–º
        df["price_direction"] = np.sign(df["close"] - df["open"])
        df["directed_volume"] = df["volume"] * df["price_direction"]
        df["volume_imbalance"] = (
            df["directed_volume"].rolling(10).sum() / df["volume"].rolling(10).sum()
        )

        # –¶–µ–Ω–æ–≤–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ - —É–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º dollar volume –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏
        df["dollar_volume"] = df["volume"] * df["close"]
        # –ò–°–ü–†–ê–í–õ–ï–ù–û v3: –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º price_impact –¥–ª—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç
        # –≥–¥–µ dollar_volume –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç $10K –¥–æ $100M+
        # log10($10K) ‚âà 4, log10($1M) ‚âà 6, log10($100M) ‚âà 8
        # –£–º–Ω–æ–∂–∞–µ–º –Ω–∞ 100 –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∑–Ω–∞—á–∏–º—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π price_impact
        df["price_impact"] = self.safe_divide(
            df["returns"].abs() * 100,  # –£–º–Ω–æ–∂–∞–µ–º –Ω–∞ 100 –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞
            np.log10(df["dollar_volume"] + 100),  # log10 –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞
            fill_value=0.0,
            max_value=0.1,  # –õ–∏–º–∏—Ç –¥–ª—è –Ω–æ–≤–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞
        )

        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ —Å –ª–æ–≥–∞—Ä–∏—Ñ–º–æ–º –æ–±—ä–µ–º–∞
        df["price_impact_log"] = self.safe_divide(
            df["returns"].abs(),
            np.log(df["volume"] + 10),  # –£–≤–µ–ª–∏—á–µ–Ω —Å–¥–≤–∏–≥ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            fill_value=0.0,
            max_value=10.0,
        )

        # –ò–°–ü–†–ê–í–õ–ï–ù–û v3: –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É–ª—É –¥–ª—è toxicity
        # toxicity = exp(-price_impact * 20)
        # –° –Ω–æ–≤—ã–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º price_impact:
        # –ü—Ä–∏ price_impact=0.04: toxicity‚âà0.45
        # –ü—Ä–∏ price_impact=0.02: toxicity‚âà0.67
        # –ü—Ä–∏ price_impact=0.01: toxicity‚âà0.82
        df["toxicity"] = np.exp(-df["price_impact"] * 20)
        df["toxicity"] = df["toxicity"].clip(0.3, 1.0)

        # –ê–º–∏—Ö—É–¥ –Ω–µ–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å - —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞
        # –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞: |returns| / dollar_volume
        # –ù–æ –º—ã –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –Ω–∞ –º–∏–ª–ª–∏–æ–Ω –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∑–Ω–∞—á–∏–º—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        df["amihud_illiquidity"] = self.safe_divide(
            df["returns"].abs() * 1e6,  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –Ω–∞ –º–∏–ª–ª–∏–æ–Ω
            df["turnover"],
            fill_value=0.0,
            max_value=100.0,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑—É–º–Ω—ã–º –º–∞–∫—Å–∏–º—É–º–æ–º
        )
        df["amihud_ma"] = df["amihud_illiquidity"].rolling(20).mean()

        # –ö–∞–π–ª –ª—è–º–±–¥–∞ - –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: |price_change| / volume, –∞ –Ω–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ std
        df["kyle_lambda"] = self.safe_divide(
            df["returns"].abs(), np.log(df["volume"] + 1), fill_value=0.0, max_value=10.0
        )

        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –≤–µ—Ä—Å–∏—è - –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–µ–π
        df["volatility_volume_ratio"] = self.safe_divide(
            df["returns"].rolling(10).std(),
            df["volume"].rolling(10).std(),
            fill_value=0.0,
            max_value=10.0,
        )

        # –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å - –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –∞–Ω–Ω—É–∞–ª–∏–∑–∞—Ü–∏—è
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –†–∞–∑–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã –∞–Ω–Ω—É–∞–ª–∏–∑–∞—Ü–∏–∏
        # –î–ª—è 15-–º–∏–Ω—É—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: 96 –ø–µ—Ä–∏–æ–¥–æ–≤ –≤ –¥–µ–Ω—å, 365 –¥–Ω–µ–π –≤ –≥–æ–¥—É
        df["realized_vol_1h"] = df["returns"].rolling(4).std() * np.sqrt(
            96
        )  # –ß–∞—Å–æ–≤–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å -> –¥–Ω–µ–≤–Ω–∞—è
        df["realized_vol_daily"] = df["returns"].rolling(96).std() * np.sqrt(
            96
        )  # –î–Ω–µ–≤–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        df["realized_vol_annual"] = df["returns"].rolling(96).std() * np.sqrt(
            96 * 365
        )  # –ì–æ–¥–æ–≤–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å

        # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –æ—Å—Ç–∞–≤–ª—è–µ–º —Å—Ç–∞—Ä–æ–µ –∏–º—è
        df["realized_vol"] = df["realized_vol_daily"]

        # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –æ–±—ä–µ–º–∞ –∫ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º log –æ–±—ä–µ–º–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞ —Å—Ä–µ–¥–Ω–∏–π –æ–±—ä–µ–º
        avg_volume = df["volume"].rolling(96).mean()
        normalized_volume = df["volume"] / (avg_volume + 1)  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –æ–±—ä–µ–º

        df["volume_volatility_ratio"] = self.safe_divide(
            normalized_volume,
            df["realized_vol"] * 100,  # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
            fill_value=1.0,
            max_value=100.0,  # –†–∞–∑—É–º–Ω—ã–π –ª–∏–º–∏—Ç
        )

        return df

    def _create_rally_detection_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–µ—Ç –¢–û–ß–ù–û 15 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ä–∞–ª–ª–∏ –∏–∑ REQUIRED_FEATURES_240"""
        if not self.disable_progress:
            self.logger.info("üéØ –°–æ–∑–¥–∞–Ω–∏–µ 15 RALLY_FEATURES...")

        # RALLY_FEATURES –∏–∑ features_240.py:
        # "current_rally_magnitude", "current_rally_duration", "current_rally_velocity",
        # "current_drawdown_magnitude", "current_drawdown_duration", "current_drawdown_velocity",
        # "recent_max_rally_1h", "recent_max_rally_4h", "recent_max_rally_12h",
        # "recent_max_drawdown_1h", "recent_max_drawdown_4h", "recent_max_drawdown_12h",
        # "prob_reach_1pct_4h", "prob_reach_2pct_4h", "prob_reach_3pct_12h"

        # 1. –¢–µ–∫—É—â–∏–µ —Ä–∞–ª–ª–∏/–ø–∞–¥–µ–Ω–∏—è (6 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–µ–∫—É—â–∏–π —Ç—Ä–µ–Ω–¥ –Ω–∞—á–∏–Ω–∞—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–æ–≤–æ—Ä–æ—Ç–∞

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–≤–æ—Ä–æ—Ç–Ω—ã–µ —Ç–æ—á–∫–∏ (–ª–æ–∫–∞–ª—å–Ω—ã–µ –º–∞–∫—Å–∏–º—É–º—ã/–º–∏–Ω–∏–º—É–º—ã)
        window = 5  # 75 –º–∏–Ω—É—Ç –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–≤–æ—Ä–æ—Ç–∞
        local_max = (
            df["high"]
            .rolling(window * 2 + 1, center=True)
            .apply(lambda x: x.iloc[window] == x.max(), raw=False)
            .fillna(False)
        )
        local_min = (
            df["low"]
            .rolling(window * 2 + 1, center=True)
            .apply(lambda x: x.iloc[window] == x.min(), raw=False)
            .fillna(False)
        )

        # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã –ø–æ–≤–æ—Ä–æ—Ç–Ω—ã—Ö —Ç–æ—á–µ–∫
        turning_points = local_max.astype(bool) | local_min.astype(bool)

        # –î–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ –Ω–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω—é—é –ø–æ–≤–æ—Ä–æ—Ç–Ω—É—é —Ç–æ—á–∫—É
        last_turn_idx = turning_points.cumsum().groupby(df.index).transform("last")
        last_turn_price = df["close"].where(turning_points).ffill()
        last_turn_high = df["high"].where(turning_points).ffill()
        last_turn_low = df["low"].where(turning_points).ffill()

        # –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–æ–≤–æ—Ä–æ—Ç–∞ (–≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –ø–µ—Ä–∏–æ–¥–æ–≤)
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ DatetimeIndex, —Ç–∞–∫ –∏ RangeIndex
        turn_indices = df.index[turning_points].tolist()
        turn_duration = pd.Series(0, index=df.index, dtype=int)
        for i, idx in enumerate(df.index):
            # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ–≤–æ—Ä–æ—Ç –¥–æ —Ç–µ–∫—É—â–µ–≥–æ –º–æ–º–µ–Ω—Ç–∞
            recent_turns = [t for t in turn_indices if t <= idx]
            if recent_turns:
                last_turn = recent_turns[-1]
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –∏–Ω–¥–µ–∫—Å–∞ –∏ –≤—ã—á–∏—Å–ª—è–µ–º –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
                if isinstance(df.index, pd.DatetimeIndex):
                    # –î–ª—è DatetimeIndex –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º timedelta –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ 15-–º–∏–Ω—É—Ç–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
                    duration_timedelta = idx - last_turn
                    duration_periods = int(duration_timedelta.total_seconds() / (15 * 60))
                else:
                    # –î–ª—è RangeIndex –ø—Ä–æ—Å—Ç–æ –≤—ã—á–∏—Ç–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã (—É–∂–µ –≤ –ø–µ—Ä–∏–æ–¥–∞—Ö)
                    duration_periods = i - df.index.get_loc(last_turn)
                turn_duration.iloc[i] = max(0, duration_periods)

        # –¢–µ–∫—É—â–µ–µ —Ä–∞–ª–ª–∏ (—Ä–æ—Å—Ç –æ—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –º–∏–Ω–∏–º—É–º–∞)
        current_is_rally = df["close"] > last_turn_price
        df["current_rally_magnitude"] = np.where(
            current_is_rally,
            (df["close"] / last_turn_price - 1) * 100,
            0.0,  # –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
        )
        df["current_rally_duration"] = np.where(current_is_rally, turn_duration, 0)
        df["current_rally_velocity"] = self.safe_divide(
            df["current_rally_magnitude"],
            df["current_rally_duration"] + 1,  # +1 —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
            fill_value=0.0,
        )

        # –¢–µ–∫—É—â–µ–µ –ø–∞–¥–µ–Ω–∏–µ (–ø–∞–¥–µ–Ω–∏–µ –æ—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –º–∞–∫—Å–∏–º—É–º–∞)
        current_is_drawdown = df["close"] < last_turn_price
        df["current_drawdown_magnitude"] = np.where(
            current_is_drawdown,
            (1 - df["close"] / last_turn_price) * 100,
            0.0,  # –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
        )
        df["current_drawdown_duration"] = np.where(current_is_drawdown, turn_duration, 0)
        df["current_drawdown_velocity"] = self.safe_divide(
            df["current_drawdown_magnitude"], df["current_drawdown_duration"] + 1, fill_value=0.0
        )

        if not self.disable_progress:
            self.logger.info("  ‚úì –¢–µ–∫—É—â–∏–µ —Ä–∞–ª–ª–∏/–ø–∞–¥–µ–Ω–∏—è: —Å–æ–∑–¥–∞–Ω–æ 6 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        # 2. –ù–µ–¥–∞–≤–Ω–∏–µ –º–∞–∫—Å–∏–º—É–º—ã —Ä–∞–ª–ª–∏ –∏ –ø–∞–¥–µ–Ω–∏–π (6 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
        periods = {"1h": 4, "4h": 16, "12h": 48}  # –≤ 15-–º–∏–Ω—É—Ç–Ω—ã—Ö —Å–≤–µ—á–∞—Ö

        for period_name, n_candles in periods.items():
            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞–ª–ª–∏ –∑–∞ –ø–µ—Ä–∏–æ–¥
            max_rally = 0
            max_drawdown = 0

            for i in range(1, n_candles + 1):
                # –†–∞–ª–ª–∏: –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç –æ—Ç –º–∏–Ω–∏–º—É–º–∞ –≤ –æ–∫–Ω–µ
                rolling_min = df["low"].rolling(i, min_periods=1).min()
                rally = (df["high"] / rolling_min - 1) * 100
                max_rally = np.maximum(max_rally, rally)

                # –ü–∞–¥–µ–Ω–∏–µ: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –ø–∞–¥–µ–Ω–∏–µ –æ—Ç –º–∞–∫—Å–∏–º—É–º–∞ –≤ –æ–∫–Ω–µ
                rolling_max = df["high"].rolling(i, min_periods=1).max()
                drawdown = (1 - df["low"] / rolling_max) * 100
                max_drawdown = np.maximum(max_drawdown, drawdown)

            df[f"recent_max_rally_{period_name}"] = max_rally
            df[f"recent_max_drawdown_{period_name}"] = max_drawdown

        if not self.disable_progress:
            self.logger.info("  ‚úì –ù–µ–¥–∞–≤–Ω–∏–µ –º–∞–∫—Å–∏–º—É–º—ã: —Å–æ–∑–¥–∞–Ω–æ 6 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        # 3. –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —É—Ä–æ–≤–Ω–µ–π (3 –ø—Ä–∏–∑–Ω–∞–∫–∞)
        # –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–π –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ —Ç–µ–∫—É—â–µ–º –º–æ–º–µ–Ω—Ç—É–º–µ

        # –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        returns = df["close"].pct_change()
        vol_4h = returns.rolling(16).std() * np.sqrt(16)  # 4-—á–∞—Å–æ–≤–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        vol_12h = returns.rolling(48).std() * np.sqrt(48)  # 12-—á–∞—Å–æ–≤–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å

        # –¢–µ–∫—É—â–∏–π –º–æ–º–µ–Ω—Ç—É–º
        momentum_1h = df["close"].pct_change(4)
        momentum_4h = df["close"].pct_change(16)

        # –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: –±–∞–∑–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ –º–æ–º–µ–Ω—Ç—É–º–µ
        # P(reach X% in T hours) = sigmoid(momentum/volatility - threshold)

        def sigmoid(x):
            return 1 / (1 + np.exp(-np.clip(x, -10, 10)))

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –º–æ–º–µ–Ω—Ç—É–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        momentum_vol_ratio_4h = self.safe_divide(momentum_4h, vol_4h, fill_value=0.0)
        momentum_vol_ratio_12h = self.safe_divide(momentum_4h, vol_12h, fill_value=0.0)

        # –ö–∞–ª–∏–±—Ä—É–µ–º –ø–æ—Ä–æ–≥–∏ —Ç–∞–∫, —á—Ç–æ–±—ã –±–∞–∑–æ–≤–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –±—ã–ª–∞ ~30%
        # –ü—Ä–∏ –Ω—É–ª–µ–≤–æ–º –º–æ–º–µ–Ω—Ç—É–º–µ: P = 0.3
        # threshold = -log(1/0.3 - 1) ‚âà -0.85
        base_threshold = -0.85

        # 1% –∑–∞ 4 —á–∞—Å–∞
        df["prob_reach_1pct_4h"] = sigmoid(momentum_vol_ratio_4h * 2 + base_threshold)

        # 2% –∑–∞ 4 —á–∞—Å–∞
        df["prob_reach_2pct_4h"] = sigmoid(momentum_vol_ratio_4h * 1.5 + base_threshold - 0.5)

        # 3% –∑–∞ 12 —á–∞—Å–æ–≤
        df["prob_reach_3pct_12h"] = sigmoid(momentum_vol_ratio_12h * 1.2 + base_threshold - 0.3)

        if not self.disable_progress:
            self.logger.info("  ‚úì –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —É—Ä–æ–≤–Ω–µ–π: —Å–æ–∑–¥–∞–Ω–æ 3 –ø—Ä–∏–∑–Ω–∞–∫–∞")

        # –ò—Ç–æ–≥–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        rally_features = [
            "current_rally_magnitude",
            "current_rally_duration",
            "current_rally_velocity",
            "current_drawdown_magnitude",
            "current_drawdown_duration",
            "current_drawdown_velocity",
            "recent_max_rally_1h",
            "recent_max_rally_4h",
            "recent_max_rally_12h",
            "recent_max_drawdown_1h",
            "recent_max_drawdown_4h",
            "recent_max_drawdown_12h",
            "prob_reach_1pct_4h",
            "prob_reach_2pct_4h",
            "prob_reach_3pct_12h",
        ]

        created_count = sum(1 for feat in rally_features if feat in df.columns)
        if not self.disable_progress:
            self.logger.info(f"‚úÖ Rally features: —Å–æ–∑–¥–∞–Ω–æ {created_count}/15 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
        for feat in rally_features:
            if feat in df.columns:
                df[feat] = df[feat].fillna(0.0)

        return df

    def _create_signal_quality_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–µ—Ç –¢–û–ß–ù–û 15 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–≥–Ω–∞–ª–æ–≤ –∏–∑ REQUIRED_FEATURES_240"""
        if not self.disable_progress:
            self.logger.info("üéØ –°–æ–∑–¥–∞–Ω–∏–µ 15 SIGNAL_QUALITY_FEATURES...")

        # SIGNAL_QUALITY_FEATURES –∏–∑ features_240.py:
        # "momentum_score", "trend_strength", "trend_consistency", "momentum_divergence",
        # "trend_acceleration", "trend_quality", "overbought_score", "oversold_score",
        # "divergence_bull", "divergence_bear", "pattern_strength", "breakout_strength",
        # "reversal_probability", "support_distance", "resistance_distance"

        # 1. –ú–æ–º–µ–Ω—Ç—É–º –∏ —Ç—Ä–µ–Ω–¥ (6 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)

        # momentum_score - –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–º–µ–Ω—Ç—É–º–∞
        rsi_momentum = (
            (df["rsi"] - 50) / 50 if "rsi" in df.columns else pd.Series(0, index=df.index)
        )
        macd_momentum = (
            df["macd"] / (df["close"] * 0.01)
            if "macd" in df.columns
            else pd.Series(0, index=df.index)
        )
        price_momentum = df["close"].pct_change(10) * 10  # 10-–ø–µ—Ä–∏–æ–¥–Ω—ã–π –º–æ–º–µ–Ω—Ç—É–º
        df["momentum_score"] = (
            rsi_momentum + macd_momentum.fillna(0) + price_momentum.fillna(0)
        ) / 3
        df["momentum_score"] = df["momentum_score"].clip(-2, 2)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [-2, 2]

        # trend_strength - —Å–∏–ª–∞ —Ç—Ä–µ–Ω–¥–∞ –∏–∑ ADX –∏–ª–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞
        if "adx" in df.columns:
            df["trend_strength"] = df["adx"] / 100  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º ADX –∫ [0, 1]
        else:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–∏–ª—ã —Ç—Ä–µ–Ω–¥–∞
            sma_short = df["close"].rolling(10).mean()
            sma_long = df["close"].rolling(50).mean()
            trend_diff = (sma_short - sma_long) / sma_long
            df["trend_strength"] = trend_diff.abs().rolling(20).mean().fillna(0.5)

        # trend_consistency - –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–∞
        price_direction = np.sign(df["close"] - df["close"].shift(1))
        df["trend_consistency"] = (
            price_direction.rolling(20).mean().abs()
        )  # –ß–µ–º –±–ª–∏–∂–µ –∫ 1, —Ç–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–µ–µ

        # momentum_divergence - –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è –º–µ–∂–¥—É —Ü–µ–Ω–æ–π –∏ –º–æ–º–µ–Ω—Ç—É–º–æ–º
        price_momentum_20 = df["close"].pct_change(20)
        rsi_momentum_20 = (
            df["rsi"].diff(20) if "rsi" in df.columns else pd.Series(0, index=df.index)
        )
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        price_dir = np.sign(price_momentum_20)
        rsi_dir = np.sign(rsi_momentum_20)
        df["momentum_divergence"] = (price_dir != rsi_dir).astype(float)  # 1 = –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è, 0 = –Ω–µ—Ç

        # trend_acceleration - —É—Å–∫–æ—Ä–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞
        momentum_short = df["close"].pct_change(5)
        momentum_long = df["close"].pct_change(20)
        df["trend_acceleration"] = momentum_short - momentum_long  # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ = —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤–≤–µ—Ä—Ö

        # trend_quality - –∫–∞—á–µ—Å—Ç–≤–æ —Ç—Ä–µ–Ω–¥–∞ (—Å–∏–ª–∞ + –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å)
        df["trend_quality"] = df["trend_strength"] * df["trend_consistency"]

        if not self.disable_progress:
            self.logger.info("  ‚úì –ú–æ–º–µ–Ω—Ç—É–º –∏ —Ç—Ä–µ–Ω–¥: —Å–æ–∑–¥–∞–Ω–æ 6 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        # 2. –£—Ä–æ–≤–Ω–∏ –ø–µ—Ä–µ–∫—É–ø–ª–µ–Ω–Ω–æ—Å—Ç–∏/–ø–µ—Ä–µ–ø—Ä–æ–¥–∞–Ω–Ω–æ—Å—Ç–∏ (4 –ø—Ä–∏–∑–Ω–∞–∫–∞)

        # overbought_score - —Å—Ç–µ–ø–µ–Ω—å –ø–µ—Ä–µ–∫—É–ø–ª–µ–Ω–Ω–æ—Å—Ç–∏
        if "rsi" in df.columns:
            rsi_overbought = np.maximum(0, df["rsi"] - 70) / 30  # –®–∫–∞–ª–∞ –æ—Ç 0 –¥–æ 1
        else:
            rsi_overbought = pd.Series(0, index=df.index)

        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ü–µ–Ω–∫—É –æ—Ç Bollinger Bands
        if "bb_position" in df.columns:
            bb_overbought = np.maximum(0, df["bb_position"] - 0.8) / 0.2
        else:
            bb_overbought = pd.Series(0, index=df.index)

        df["overbought_score"] = (rsi_overbought + bb_overbought) / 2

        # oversold_score - —Å—Ç–µ–ø–µ–Ω—å –ø–µ—Ä–µ–ø—Ä–æ–¥–∞–Ω–Ω–æ—Å—Ç–∏
        if "rsi" in df.columns:
            rsi_oversold = np.maximum(0, 30 - df["rsi"]) / 30
        else:
            rsi_oversold = pd.Series(0, index=df.index)

        if "bb_position" in df.columns:
            bb_oversold = np.maximum(0, 0.2 - df["bb_position"]) / 0.2
        else:
            bb_oversold = pd.Series(0, index=df.index)

        df["oversold_score"] = (rsi_oversold + bb_oversold) / 2

        # divergence_bull - –±—ã—á—å—è –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è (—Ü–µ–Ω–∞ –ø–∞–¥–∞–µ—Ç, –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Ä–∞—Å—Ç—É—Ç)
        price_falling = df["close"] < df["close"].shift(10)
        if "rsi" in df.columns:
            rsi_rising = df["rsi"] > df["rsi"].shift(10)
            df["divergence_bull"] = (price_falling & rsi_rising).astype(float)
        else:
            df["divergence_bull"] = pd.Series(0, index=df.index)

        # divergence_bear - –º–µ–¥–≤–µ–∂—å—è –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è (—Ü–µ–Ω–∞ —Ä–∞—Å—Ç–µ—Ç, –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –ø–∞–¥–∞—é—Ç)
        price_rising = df["close"] > df["close"].shift(10)
        if "rsi" in df.columns:
            rsi_falling = df["rsi"] < df["rsi"].shift(10)
            df["divergence_bear"] = (price_rising & rsi_falling).astype(float)
        else:
            df["divergence_bear"] = pd.Series(0, index=df.index)

        if not self.disable_progress:
            self.logger.info("  ‚úì –£—Ä–æ–≤–Ω–∏ –ø–µ—Ä–µ–∫—É–ø–ª–µ–Ω–Ω–æ—Å—Ç–∏/–ø–µ—Ä–µ–ø—Ä–æ–¥–∞–Ω–Ω–æ—Å—Ç–∏: —Å–æ–∑–¥–∞–Ω–æ 4 –ø—Ä–∏–∑–Ω–∞–∫–∞")

        # 3. –ü–∞—Ç—Ç–µ—Ä–Ω—ã –∏ —Å–∏–ª–∞ (5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)

        # pattern_strength - —Å–∏–ª–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ (–Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—ä–µ–º–∞ –∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏)
        volume_relative = df["volume"] / df["volume"].rolling(20).mean()
        if "atr" in df.columns:
            volatility_relative = df["atr"] / df["atr"].rolling(20).mean()
        else:
            volatility_relative = df["close"].rolling(5).std() / df["close"].rolling(20).std()

        df["pattern_strength"] = (volume_relative * volatility_relative).fillna(1.0)
        df["pattern_strength"] = df["pattern_strength"].clip(
            0, 5
        )  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

        # breakout_strength - —Å–∏–ª–∞ –ø—Ä–æ—Ä—ã–≤–∞
        if "bb_position" in df.columns:
            # –ü—Ä–æ—Ä—ã–≤ Bollinger Bands
            bb_breakout = (df["bb_position"] > 1) | (df["bb_position"] < 0)
            breakout_magnitude = np.abs(df["bb_position"] - 0.5) * 2  # –°–∏–ª–∞ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –æ—Ç —Ü–µ–Ω—Ç—Ä–∞
        else:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Ä–∞—Å—á–µ—Ç —á–µ—Ä–µ–∑ —Ü–µ–Ω–æ–≤—ã–µ —É—Ä–æ–≤–Ω–∏
            high_20 = df["high"].rolling(20).max()
            low_20 = df["low"].rolling(20).min()
            bb_breakout = (df["close"] > high_20) | (df["close"] < low_20)
            breakout_magnitude = np.maximum(
                (df["close"] - high_20) / high_20, (low_20 - df["close"]) / low_20
            ).fillna(0)

        df["breakout_strength"] = bb_breakout.astype(float) * breakout_magnitude

        # reversal_probability - –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ä–∞–∑–≤–æ—Ä–æ—Ç–∞
        # –û—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        extreme_rsi = (
            (df["rsi"] > 80) | (df["rsi"] < 20)
            if "rsi" in df.columns
            else pd.Series(False, index=df.index)
        )
        extreme_bb = (
            (df["bb_position"] > 0.9) | (df["bb_position"] < 0.1)
            if "bb_position" in df.columns
            else pd.Series(False, index=df.index)
        )

        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ä–∞–∑–≤–æ—Ä–æ—Ç–∞ –ø—Ä–∏ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö
        df["reversal_probability"] = (extreme_rsi.astype(float) + extreme_bb.astype(float)) / 2

        # support_distance - —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –±–ª–∏–∂–∞–π—à–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏
        low_50 = df["low"].rolling(50).min()  # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∑–∞ 50 –ø–µ—Ä–∏–æ–¥–æ–≤
        df["support_distance"] = (df["close"] - low_50) / df["close"]

        # resistance_distance - —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –±–ª–∏–∂–∞–π—à–µ–≥–æ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
        high_50 = df["high"].rolling(50).max()  # –°–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ –∑–∞ 50 –ø–µ—Ä–∏–æ–¥–æ–≤
        df["resistance_distance"] = (high_50 - df["close"]) / df["close"]

        if not self.disable_progress:
            self.logger.info("  ‚úì –ü–∞—Ç—Ç–µ—Ä–Ω—ã –∏ —Å–∏–ª–∞: —Å–æ–∑–¥–∞–Ω–æ 5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        # –ò—Ç–æ–≥–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        signal_quality_features = [
            "momentum_score",
            "trend_strength",
            "trend_consistency",
            "momentum_divergence",
            "trend_acceleration",
            "trend_quality",
            "overbought_score",
            "oversold_score",
            "divergence_bull",
            "divergence_bear",
            "pattern_strength",
            "breakout_strength",
            "reversal_probability",
            "support_distance",
            "resistance_distance",
        ]

        created_count = sum(1 for feat in signal_quality_features if feat in df.columns)
        if not self.disable_progress:
            self.logger.info(f"‚úÖ Signal quality features: —Å–æ–∑–¥–∞–Ω–æ {created_count}/15 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
        for feat in signal_quality_features:
            if feat in df.columns:
                df[feat] = df[feat].fillna(0.0)

        return df

    def _create_futures_specific_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–µ—Ç –¢–û–ß–ù–û 10 —Ñ—å—é—á–µ—Ä—Å-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ REQUIRED_FEATURES_240"""
        if not self.disable_progress:
            self.logger.info("üéØ –°–æ–∑–¥–∞–Ω–∏–µ 10 FUTURES_FEATURES...")

        # FUTURES_FEATURES –∏–∑ features_240.py:
        # "funding_rate", "open_interest", "oi_change_1h", "long_short_ratio", "taker_buy_sell_ratio",
        # "funding_momentum", "oi_weighted_momentum", "liquidation_pressure", "basis_spread", "term_structure"

        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–µ —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –ø–æ—Å—Ç—É–ø–∞—Ç—å —Å –±–∏—Ä–∂–∏
        # –î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ OHLCV

        # 1. –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Ñ—å—é—á–µ—Ä—Å–æ–≤ (5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)

        # funding_rate - —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π funding rate –Ω–∞ –æ—Å–Ω–æ–≤–µ momentum
        # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π momentum -> –¥–ª–∏–Ω–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –¥–æ–º–∏–Ω–∏—Ä—É—é—Ç -> –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π funding
        momentum_4h = df["close"].pct_change(16)  # 4-—á–∞—Å–æ–≤–æ–π momentum
        df["funding_rate"] = np.tanh(momentum_4h * 50) * 0.001  # –í –¥–∏–∞–ø–∞–∑–æ–Ω–µ ¬±0.1%

        # open_interest - —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—ä–µ–º–∞
        # –í—ã—Å–æ–∫–∏–π –æ–±—ä–µ–º —á–∞—Å—Ç–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å –≤—ã—Å–æ–∫–∏–º open interest
        volume_normalized = df["volume"] / df["volume"].rolling(96).mean()
        df["open_interest"] = volume_normalized.rolling(24).mean()  # –°–≥–ª–∞–∂–µ–Ω–Ω—ã–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å

        # oi_change_1h - –∏–∑–º–µ–Ω–µ–Ω–∏–µ open interest –∑–∞ —á–∞—Å
        df["oi_change_1h"] = df["open_interest"].pct_change(4)  # –ó–∞ 4 –ø–µ—Ä–∏–æ–¥–∞ = 1 —á–∞—Å

        # long_short_ratio - –Ω–∞ –æ—Å–Ω–æ–≤–µ price momentum –∏ volume
        # –ü—Ä–∏ —Ä–æ—Å—Ç–µ —Ü–µ–Ω—ã –Ω–∞ –æ–±—ä–µ–º–µ -> –±–æ–ª—å—à–µ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π
        price_change = df["close"].pct_change()
        volume_weight = volume_normalized.clip(0.1, 3.0)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        long_bias = pd.Series(
            np.where(price_change > 0, price_change * volume_weight, 0), index=df.index
        )
        short_bias = pd.Series(
            np.where(price_change < 0, abs(price_change) * volume_weight, 0), index=df.index
        )
        long_sum = long_bias.rolling(24).sum()
        short_sum = short_bias.rolling(24).sum()
        df["long_short_ratio"] = self.safe_divide(long_sum, short_sum, fill_value=1.0)

        # taker_buy_sell_ratio - –Ω–∞ –æ—Å–Ω–æ–≤–µ close vs vwap
        # –ï—Å–ª–∏ close > vwap -> –±–æ–ª—å—à–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –ø–æ–∫—É–ø–æ–∫
        if "vwap" in df.columns:
            buy_pressure = np.maximum(0, df["close"] - df["vwap"]) / df["close"]
            sell_pressure = np.maximum(0, df["vwap"] - df["close"]) / df["close"]
        else:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ —á–µ—Ä–µ–∑ high/low
            buy_pressure = (df["close"] - df["low"]) / (df["high"] - df["low"] + 1e-10)
            sell_pressure = 1 - buy_pressure

        buy_sum = pd.Series(buy_pressure, index=df.index).rolling(12).sum()
        sell_sum = pd.Series(sell_pressure, index=df.index).rolling(12).sum()
        df["taker_buy_sell_ratio"] = self.safe_divide(buy_sum, sell_sum, fill_value=1.0)

        if not self.disable_progress:
            self.logger.info("  ‚úì –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Ñ—å—é—á–µ—Ä—Å–æ–≤: —Å–æ–∑–¥–∞–Ω–æ 5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        # 2. –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)

        # funding_momentum - –∏–∑–º–µ–Ω–µ–Ω–∏–µ funding rate
        df["funding_momentum"] = df["funding_rate"].diff(4)  # –ò–∑–º–µ–Ω–µ–Ω–∏–µ –∑–∞ —á–∞—Å

        # oi_weighted_momentum - momentum –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π –Ω–∞ open interest
        price_momentum = df["close"].pct_change(12)  # 3-—á–∞—Å–æ–≤–æ–π momentum
        oi_weight = df["open_interest"] / (df["open_interest"].rolling(96).mean() + 1e-6)
        df["oi_weighted_momentum"] = price_momentum * oi_weight.clip(0.1, 3.0)

        # liquidation_pressure - –¥–∞–≤–ª–µ–Ω–∏–µ –ª–∏–∫–≤–∏–¥–∞—Ü–∏–π
        # –í—ã—Å–æ–∫–æ–µ –ø—Ä–∏ —Ä–µ–∑–∫–∏—Ö –¥–≤–∏–∂–µ–Ω–∏—è—Ö –ø—Ä–æ—Ç–∏–≤ trend + –≤—ã—Å–æ–∫–æ–µ –ø–ª–µ—á–æ (—á–µ—Ä–µ–∑ volatility)
        atr_normalized = (
            df["atr_pct"] / df["atr_pct"].rolling(96).mean()
            if "atr_pct" in df.columns
            else pd.Series(1, index=df.index)
        )
        price_shock = abs(df["close"].pct_change(4))  # –†–µ–∑–∫–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ –∑–∞ —á–∞—Å
        df["liquidation_pressure"] = price_shock * atr_normalized * volume_normalized
        df["liquidation_pressure"] = df["liquidation_pressure"].clip(
            0, 5
        )  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

        # basis_spread - —Å–ø—Ä–µ–¥ –º–µ–∂–¥—É —Ñ—å—é—á–µ—Ä—Å–æ–º –∏ —Å–ø–æ—Ç–æ–º
        # –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ funding rate (–æ–Ω–∏ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—Ç)
        df["basis_spread"] = df["funding_rate"] * 8  # –ü—Ä–∏–º–µ—Ä–Ω–æ 8x –æ—Ç funding rate

        # term_structure - —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å—Ä–æ–∫–æ–≤
        # –ù–∞ –æ—Å–Ω–æ–≤–µ volatility term structure (–±–ª–∏–∂–Ω–∏–µ vs –¥–∞–ª—å–Ω–∏–µ —Ñ—å—é—á–µ—Ä—Å—ã)
        vol_short = df["close"].rolling(24).std()  # –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        vol_long = df["close"].rolling(96).std()  # –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        df["term_structure"] = self.safe_divide(vol_short, vol_long, fill_value=1.0)

        if not self.disable_progress:
            self.logger.info("  ‚úì –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏: —Å–æ–∑–¥–∞–Ω–æ 5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        # –ò—Ç–æ–≥–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        futures_features = [
            "funding_rate",
            "open_interest",
            "oi_change_1h",
            "long_short_ratio",
            "taker_buy_sell_ratio",
            "funding_momentum",
            "oi_weighted_momentum",
            "liquidation_pressure",
            "basis_spread",
            "term_structure",
        ]

        created_count = sum(1 for feat in futures_features if feat in df.columns)
        if not self.disable_progress:
            self.logger.info(f"‚úÖ Futures features: —Å–æ–∑–¥–∞–Ω–æ {created_count}/10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
        for feat in futures_features:
            if feat in df.columns:
                df[feat] = df[feat].fillna(0.0)

        return df

    def _create_temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–µ—Ç –¢–û–ß–ù–û 12 –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ REQUIRED_FEATURES_240"""
        if not self.disable_progress:
            self.logger.info("üéØ –°–æ–∑–¥–∞–Ω–∏–µ 12 TEMPORAL_FEATURES...")

        # TEMPORAL_FEATURES –∏–∑ features_240.py:
        # "hour_sin", "hour_cos", "day_sin", "day_cos", "week_sin", "week_cos",
        # "month_sin", "month_cos", "is_weekend", "is_month_start", "is_month_end", "is_quarter_end"

        # 1. –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤—Ä–µ–º–µ–Ω–∏ (8 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü–æ–ª—É—á–∞–µ–º datetime –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ –∏–ª–∏ –∫–æ–ª–æ–Ω–∫–∏
        if "datetime" in df.columns:
            datetime_series = df["datetime"]
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å –∫–∞–∫ datetime
            datetime_series = pd.Series(df.index, index=df.index)

        # –ß–∞—Å –¥–Ω—è (0-23)
        hour = datetime_series.dt.hour
        df["hour_sin"] = np.sin(2 * np.pi * hour / 24)
        df["hour_cos"] = np.cos(2 * np.pi * hour / 24)

        # –î–µ–Ω—å –º–µ—Å—è—Ü–∞ (1-31)
        day = datetime_series.dt.day
        df["day_sin"] = np.sin(2 * np.pi * day / 31)
        df["day_cos"] = np.cos(2 * np.pi * day / 31)

        # –ù–µ–¥–µ–ª—è –≥–æ–¥–∞ (1-52)
        week = datetime_series.dt.isocalendar().week
        df["week_sin"] = np.sin(2 * np.pi * week / 52)
        df["week_cos"] = np.cos(2 * np.pi * week / 52)

        # –ú–µ—Å—è—Ü –≥–æ–¥–∞ (1-12)
        month = datetime_series.dt.month
        df["month_sin"] = np.sin(2 * np.pi * month / 12)
        df["month_cos"] = np.cos(2 * np.pi * month / 12)

        if not self.disable_progress:
            self.logger.info("  ‚úì –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤—Ä–µ–º–µ–Ω–∏: —Å–æ–∑–¥–∞–Ω–æ 8 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        # 2. –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (4 –ø—Ä–∏–∑–Ω–∞–∫–∞)

        # –í—ã—Ö–æ–¥–Ω—ã–µ –¥–Ω–∏
        dayofweek = datetime_series.dt.dayofweek
        df["is_weekend"] = (dayofweek >= 5).astype(int)  # –°—É–±–±–æ—Ç–∞=5, –í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ=6

        # –ù–∞—á–∞–ª–æ –º–µ—Å—è—Ü–∞ (–ø–µ—Ä–≤—ã–µ 3 –¥–Ω—è)
        df["is_month_start"] = (day <= 3).astype(int)

        # –ö–æ–Ω–µ—Ü –º–µ—Å—è—Ü–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 4 –¥–Ω—è)
        df["is_month_end"] = (day >= 28).astype(int)

        # –ö–æ–Ω–µ—Ü –∫–≤–∞—Ä—Ç–∞–ª–∞ (–º–∞—Ä—Ç, –∏—é–Ω—å, —Å–µ–Ω—Ç—è–±—Ä—å, –¥–µ–∫–∞–±—Ä—å)
        df["is_quarter_end"] = month.isin([3, 6, 9, 12]).astype(int)

        if not self.disable_progress:
            self.logger.info("  ‚úì –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: —Å–æ–∑–¥–∞–Ω–æ 4 –ø—Ä–∏–∑–Ω–∞–∫–∞")

        # –ò—Ç–æ–≥–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        temporal_features = [
            "hour_sin",
            "hour_cos",
            "day_sin",
            "day_cos",
            "week_sin",
            "week_cos",
            "month_sin",
            "month_cos",
            "is_weekend",
            "is_month_start",
            "is_month_end",
            "is_quarter_end",
        ]

        created_count = sum(1 for feat in temporal_features if feat in df.columns)
        if not self.disable_progress:
            self.logger.info(f"‚úÖ Temporal features: —Å–æ–∑–¥–∞–Ω–æ {created_count}/12 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
        for feat in temporal_features:
            if feat in df.columns:
                df[feat] = df[feat].fillna(0.0)

        return df

    def _create_ml_optimized_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ ML-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è 2024-2025"""
        if not self.disable_progress:
            self.logger.info("–°–æ–∑–¥–∞–Ω–∏–µ ML-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        # 1. Hurst Exponent - –º–µ—Ä–∞ –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Ä—ã–Ω–∫–∞
        # >0.5 = —Ç—Ä–µ–Ω–¥, <0.5 = –≤–æ–∑–≤—Ä–∞—Ç –∫ —Å—Ä–µ–¥–Ω–µ–º—É, ~0.5 = —Å–ª—É—á–∞–π–Ω–æ–µ –±–ª—É–∂–¥–∞–Ω–∏–µ
        def hurst_exponent(ts, max_lag=20):
            """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç—ã –•–µ—Ä—Å—Ç–∞"""
            lags = range(2, min(max_lag, len(ts) // 2))
            tau = []

            for lag in lags:
                pp = np.array(ts[:-lag])
                pn = np.array(ts[lag:])
                diff = pn - pp
                tau.append(np.sqrt(np.nanmean(diff**2)))

            if len(tau) > 0 and all(t > 0 for t in tau):
                poly = np.polyfit(np.log(lags), np.log(tau), 1)
                return poly[0] * 2.0
            return 0.5

        # –ü—Ä–∏–º–µ–Ω—è–µ–º Hurst –¥–ª—è close —Å –æ–∫–Ω–æ–º 50
        df["hurst_exponent"] = (
            df["close"].rolling(50).apply(lambda x: hurst_exponent(x) if len(x) == 50 else 0.5)
        )

        # 2. Fractal Dimension - —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Ü–µ–Ω–æ–≤–æ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è
        # 1 = –ø—Ä—è–º–∞—è –ª–∏–Ω–∏—è, 2 = –∑–∞–ø–æ–ª–Ω—è–µ—Ç –ø–ª–æ—Å–∫–æ—Å—Ç—å
        def fractal_dimension(ts):
            """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–º –•–∏–≥—É—á–∏"""
            N = len(ts)
            if N < 10:
                return 1.5

            kmax = min(5, N // 2)
            L = []

            for k in range(1, kmax + 1):
                Lk = 0
                for m in range(k):
                    Lmk = 0
                    for i in range(1, int((N - m) / k)):
                        Lmk += abs(ts[m + i * k] - ts[m + (i - 1) * k])
                    if int((N - m) / k) > 0:
                        Lmk = Lmk * (N - 1) / (k * int((N - m) / k))
                    Lk += Lmk
                L.append(Lk / k)

            if len(L) > 0 and all(l > 0 for l in L):
                x = np.log(range(1, kmax + 1))
                y = np.log(L)
                poly = np.polyfit(x, y, 1)
                return poly[0]
            return 1.5

        df["fractal_dimension"] = (
            df["close"]
            .rolling(30)
            .apply(lambda x: fractal_dimension(x.values) if len(x) == 30 else 1.5)
        )

        # 3. Market Efficiency Ratio - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã
        # –í—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è = —Å–∏–ª—å–Ω—ã–π —Ç—Ä–µ–Ω–¥, –Ω–∏–∑–∫–∏–µ = –±–æ–∫–æ–≤–∏–∫
        df["efficiency_ratio"] = self.safe_divide(
            (df["close"] - df["close"].shift(20)).abs(), df["close"].diff().abs().rolling(20).sum()
        )

        # 4. Trend Quality Index - –∫–∞—á–µ—Å—Ç–≤–æ —Ç—Ä–µ–Ω–¥–∞
        # –ö–æ–º–±–∏–Ω–∞—Ü–∏—è ADX, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        if "adx" in df.columns and "sma_50" in df.columns and "bb_width" in df.columns:
            df["trend_quality"] = (
                df["adx"]
                / 100  # –°–∏–ª–∞ —Ç—Ä–µ–Ω–¥–∞
                * ((df["close"] > df["sma_50"]).astype(float) * 2 - 1)  # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
                * (
                    1 - df["bb_width"] / df["bb_width"].rolling(50).max()
                )  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            )
        else:
            df["trend_quality"] = 0

        # 5. Regime Detection Features
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä—ã–Ω–æ—á–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ (—Ç—Ä–µ–Ω–¥/—Ñ–ª—ç—Ç/–≤—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å)
        returns = df["close"].pct_change()

        # Realized volatility
        df["realized_vol_5m"] = returns.rolling(20).std() * np.sqrt(20)
        df["realized_vol_15m"] = returns.rolling(60).std() * np.sqrt(60)
        df["realized_vol_1h"] = returns.rolling(240).std() * np.sqrt(240)

        # GARCH-–ø–æ–¥–æ–±–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–ª—É—á—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ NaN –∑–Ω–∞—á–µ–Ω–∏–π
        def garch_volatility(x):
            """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è GARCH –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ç–∏–ø–æ–≤"""
            try:
                if len(x) == 0 or x.isna().all():
                    return 0.0

                var_val = float(x.var())
                last_val = float(x.iloc[-1])

                if np.isnan(var_val) or np.isnan(last_val):
                    return 0.0

                result = 0.94 * var_val + 0.06 * (last_val**2)

                if result < 0:
                    return 0.0

                return float(np.sqrt(result))
            except (IndexError, ValueError, TypeError):
                return 0.0

        df["garch_vol"] = returns.rolling(20).apply(garch_volatility)

        # –†–µ–∂–∏–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        if "atr" in df.columns:
            atr_q25 = df["atr"].rolling(1000).quantile(0.25)
            atr_q75 = df["atr"].rolling(1000).quantile(0.75)
            df["vol_regime"] = 0  # –ù–æ—Ä–º–∞–ª—å–Ω–∞—è
            df.loc[df["atr"] < atr_q25, "vol_regime"] = -1  # –ù–∏–∑–∫–∞—è
            df.loc[df["atr"] > atr_q75, "vol_regime"] = 1  # –í—ã—Å–æ–∫–∞—è

        # 6. Information-theoretic features
        # –≠–Ω—Ç—Ä–æ–ø–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π
        def shannon_entropy(series, bins=10):
            """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –®–µ–Ω–Ω–æ–Ω–∞"""
            if len(series) < bins:
                return 0
            counts, _ = np.histogram(series, bins=bins)
            probs = counts / counts.sum()
            probs = probs[probs > 0]
            return -np.sum(probs * np.log(probs))

        df["return_entropy"] = returns.rolling(100).apply(lambda x: shannon_entropy(x))

        # 7. Microstructure features
        # Amihud illiquidity
        if "amihud_illiquidity" not in df.columns:
            df["amihud_illiquidity"] = (
                self.safe_divide(returns.abs(), df["turnover"]).rolling(20).mean()
            )

        # Kyle's lambda (price impact)
        if "kyle_lambda" not in df.columns:
            df["kyle_lambda"] = self.safe_divide(
                returns.abs().rolling(20).mean(), df["volume"].rolling(20).mean()
            )

        # 8. Cross-sectional features (–µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ BTC)
        if "btc_returns" in df.columns:
            # Beta –∫ BTC
            df["btc_beta"] = (
                returns.rolling(100).cov(df["btc_returns"]) / df["btc_returns"].rolling(100).var()
            )

            # –ò–¥–∏–æ—Å–∏–Ω–∫—Ä–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            df["idio_vol"] = (returns - df["btc_beta"] * df["btc_returns"]).rolling(50).std()

        # 9. Autocorrelation features
        # –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ª–∞–≥–∞—Ö
        df["returns_ac_1"] = returns.rolling(50).apply(
            lambda x: x.autocorr(lag=1) if len(x) > 1 else 0
        )
        df["returns_ac_5"] = returns.rolling(50).apply(
            lambda x: x.autocorr(lag=5) if len(x) > 5 else 0
        )
        df["returns_ac_10"] = returns.rolling(50).apply(
            lambda x: x.autocorr(lag=10) if len(x) > 10 else 0
        )
        df["returns_ac_20"] = returns.rolling(50).apply(
            lambda x: x.autocorr(lag=20) if len(x) > 20 else 0
        )

        # 10. Jump detection
        # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø—Ä—ã–∂–∫–æ–≤ –≤ —Ü–µ–Ω–µ
        df["price_jump"] = (returns.abs() > returns.rolling(100).std() * 3).astype(int)

        df["jump_intensity"] = df["price_jump"].rolling(50).mean()

        # 10a. Volatility clustering - –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        # –í—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—ã—á–Ω–æ —Å–ª–µ–¥—É–µ—Ç –∑–∞ –≤—ã—Å–æ–∫–æ–π –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å—é
        df["vol_clustering"] = returns.pow(2).rolling(20).mean().rolling(20).std()

        # 10b. Efficiency-adjusted volatility
        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å —Å —É—á–µ—Ç–æ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–≤–∏–∂–µ–Ω–∏—è
        if "efficiency_ratio" in df.columns:
            df["efficiency_volatility"] = df["realized_vol_1h"] * (1 - df["efficiency_ratio"])
        else:
            df["efficiency_volatility"] = df["realized_vol_1h"]

        # 10c. Microstructure noise estimation
        # –û—Ü–µ–Ω–∫–∞ —Ä—ã–Ω–æ—á–Ω–æ–≥–æ —à—É–º–∞ —á–µ—Ä–µ–∑ –ø–µ—Ä–≤—ã–µ —Ä–∞–∑–Ω–æ—Å—Ç–∏
        df["microstructure_noise"] = (df["close"].diff() / df["close"]).rolling(20).std()

        # 10d. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ ML –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
        # –ü–∞—Ä–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –æ–±—ä–µ–º–∞ –∏ —Ü–µ–Ω—ã
        df["vol_price_corr"] = df["volume"].rolling(50).corr(df["close"])
        df["vol_returns_corr"] = df["volume"].rolling(50).corr(returns.abs())

        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ (vol of vol)
        df["vol_of_vol"] = returns.rolling(20).std().rolling(20).std()

        # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è —Å–∏–ª–∞ –∏–Ω–¥–µ–∫—Å–∞ (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è)
        gains = returns.where(returns > 0, 0).rolling(14).mean()
        losses = -returns.where(returns < 0, 0).rolling(14).mean()
        df["rsi_alternative"] = 100 - (100 / (1 + gains / (losses + 1e-10)))

        # –¢—Ä–µ–Ω–¥-—Ñ–∏–ª—å—Ç—Ä –•–æ–¥—Ä–∏–∫–∞-–ü—Ä–µ—Å–∫–æ—Ç—Ç–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
        df["hp_trend"] = df["close"].rolling(100).mean()
        df["hp_cycle"] = (df["close"] - df["hp_trend"]) / df["close"]

        # –ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å —Ç–æ—Ä–≥–æ–≤ (tick rule)
        df["trade_intensity"] = (df["volume"] / df["volume"].rolling(20).mean()) * np.sign(returns)

        # 11. Order flow imbalance persistence
        if "order_flow_imbalance" in df.columns:
            df["ofi_persistence"] = (
                df["order_flow_imbalance"]
                .rolling(20)
                .apply(lambda x: x.autocorr(lag=1) if len(x) > 1 else 0)
            )

        # 12. Volume-synchronized probability of informed trading (VPIN)
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
        df["vpin"] = self.safe_divide(
            (df["volume"] * ((df["close"] > df["open"]).astype(float) - 0.5))
            .rolling(50)
            .sum()
            .abs(),
            df["volume"].rolling(50).sum(),
        )

        # 13. Liquidity-adjusted returns
        if "amihud_illiquidity" in df.columns:
            df["liquidity_adj_returns"] = returns * (
                1 - df["amihud_illiquidity"] / df["amihud_illiquidity"].rolling(100).max()
            )

        # 14. Tail risk measures
        # Conditional Value at Risk (CVaR)
        df["cvar_5pct"] = returns.rolling(100).apply(
            lambda x: (
                x[x <= x.quantile(0.05)].mean()
                if len(x[x <= x.quantile(0.05)]) > 0
                else x.quantile(0.05)
            )
        )

        # –î–û–ë–ê–í–õ–Ø–ï–ú –í–°–ï –û–¢–°–£–¢–°–¢–í–£–Æ–©–ò–ï ML_OPTIMIZED_FEATURES –∏–∑ features_240.py

        # 15. –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (price z-scores)
        df["price_z_score_20"] = (df["close"] - df["close"].rolling(20).mean()) / df[
            "close"
        ].rolling(20).std()
        df["price_z_score_50"] = (df["close"] - df["close"].rolling(50).mean()) / df[
            "close"
        ].rolling(50).std()
        df["price_z_score_100"] = (df["close"] - df["close"].rolling(100).mean()) / df[
            "close"
        ].rolling(100).std()

        # Volume z-scores
        df["volume_z_score_20"] = (df["volume"] - df["volume"].rolling(20).mean()) / df[
            "volume"
        ].rolling(20).std()
        df["volume_z_score_50"] = (df["volume"] - df["volume"].rolling(50).mean()) / df[
            "volume"
        ].rolling(50).std()

        # Return moments
        df["return_skewness_20"] = returns.rolling(20).skew()
        df["return_skewness_50"] = returns.rolling(50).skew()
        df["return_kurtosis_20"] = returns.rolling(20).kurt()
        df["return_kurtosis_50"] = returns.rolling(50).kurt()

        # Efficiency ratios
        df["price_efficiency_ratio"] = self.safe_divide(
            (df["close"] - df["close"].shift(20)).abs(), df["close"].diff().abs().rolling(20).sum()
        )
        df["volume_efficiency_ratio"] = self.safe_divide(
            (df["volume"] - df["volume"].shift(20)).abs(),
            df["volume"].diff().abs().rolling(20).sum(),
        )

        # Entropy features
        def rolling_entropy(series, window=20, bins=10):
            def entropy_calc(x):
                if len(x) < bins:
                    return 0
                counts, _ = np.histogram(x, bins=bins)
                probs = counts / counts.sum()
                probs = probs[probs > 0]
                return -np.sum(probs * np.log(probs))

            return series.rolling(window).apply(entropy_calc)

        df["entropy_20"] = rolling_entropy(returns, 20)
        df["entropy_50"] = rolling_entropy(returns, 50)

        # Autocorrelation features
        df["autocorrelation_lag_1"] = returns.rolling(50).apply(
            lambda x: x.autocorr(lag=1) if len(x) > 1 else 0
        )
        df["autocorrelation_lag_5"] = returns.rolling(50).apply(
            lambda x: x.autocorr(lag=5) if len(x) > 5 else 0
        )
        df["autocorrelation_lag_10"] = returns.rolling(50).apply(
            lambda x: x.autocorr(lag=10) if len(x) > 10 else 0
        )

        # Partial autocorrelation (simplified)
        df["partial_autocorr_1"] = df[
            "autocorrelation_lag_1"
        ]  # –î–ª—è –ª–∞–≥–∞ 1 —á–∞—Å—Ç–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è = –æ–±—ã—á–Ω–∞—è
        df["partial_autocorr_5"] = df["autocorrelation_lag_5"] - df["autocorrelation_lag_1"] * df[
            "autocorrelation_lag_1"
        ].shift(4)

        # 16. Pattern recognition features
        # Higher highs, lower lows patterns
        df["higher_high"] = (
            (df["high"] > df["high"].shift(1)) & (df["high"].shift(1) > df["high"].shift(2))
        ).astype(float)
        df["lower_low"] = (
            (df["low"] < df["low"].shift(1)) & (df["low"].shift(1) < df["low"].shift(2))
        ).astype(float)
        df["higher_low"] = (
            (df["low"] > df["low"].shift(1)) & (df["low"].shift(1) > df["low"].shift(2))
        ).astype(float)
        df["lower_high"] = (
            (df["high"] < df["high"].shift(1)) & (df["high"].shift(1) < df["high"].shift(2))
        ).astype(float)

        # Candlestick patterns
        body = (df["close"] - df["open"]).abs()
        upper_shadow = df["high"] - np.maximum(df["open"], df["close"])
        lower_shadow = np.minimum(df["open"], df["close"]) - df["low"]

        df["bullish_engulfing"] = (
            (df["close"] > df["open"])
            & (df["close"].shift(1) < df["open"].shift(1))
            & (df["open"] < df["close"].shift(1))
            & (df["close"] > df["open"].shift(1))
        ).astype(float)

        df["bearish_engulfing"] = (
            (df["close"] < df["open"])
            & (df["close"].shift(1) > df["open"].shift(1))
            & (df["open"] > df["close"].shift(1))
            & (df["close"] < df["open"].shift(1))
        ).astype(float)

        df["hammer"] = (
            (lower_shadow > body * 2) & (upper_shadow < body * 0.1) & (body > 0)
        ).astype(float)

        df["shooting_star"] = (
            (upper_shadow > body * 2) & (lower_shadow < body * 0.1) & (body > 0)
        ).astype(float)

        df["doji"] = (body < (df["high"] - df["low"]) * 0.1).astype(float)

        # Three soldiers/crows patterns
        df["three_white_soldiers"] = (
            (df["close"] > df["open"])
            & (df["close"].shift(1) > df["open"].shift(1))
            & (df["close"].shift(2) > df["open"].shift(2))
            & (df["close"] > df["close"].shift(1))
            & (df["close"].shift(1) > df["close"].shift(2))
        ).astype(float)

        df["three_black_crows"] = (
            (df["close"] < df["open"])
            & (df["close"].shift(1) < df["open"].shift(1))
            & (df["close"].shift(2) < df["open"].shift(2))
            & (df["close"] < df["close"].shift(1))
            & (df["close"].shift(1) < df["close"].shift(2))
        ).astype(float)

        # Star patterns (simplified)
        gap_up = df["low"] > df["high"].shift(1)
        gap_down = df["high"] < df["low"].shift(1)

        df["morning_star"] = (
            (df["close"].shift(2) < df["open"].shift(2))  # Bearish candle
            & gap_down.shift(1)  # Gap down
            & (df["close"] > df["open"])  # Bullish candle
            & gap_up  # Gap up
        ).astype(float)

        df["evening_star"] = (
            (df["close"].shift(2) > df["open"].shift(2))  # Bullish candle
            & gap_up.shift(1)  # Gap up
            & (df["close"] < df["open"])  # Bearish candle
            & gap_down  # Gap down
        ).astype(float)

        # Harami patterns
        prev_body = (df["close"].shift(1) - df["open"].shift(1)).abs()
        df["harami_bull"] = (
            (df["close"].shift(1) < df["open"].shift(1))  # Bearish mother
            & (df["close"] > df["open"])  # Bullish baby
            & (body < prev_body * 0.5)  # Baby smaller than mother
        ).astype(float)

        df["harami_bear"] = (
            (df["close"].shift(1) > df["open"].shift(1))  # Bullish mother
            & (df["close"] < df["open"])  # Bearish baby
            & (body < prev_body * 0.5)  # Baby smaller than mother
        ).astype(float)

        # 17. Adaptive features
        # Adaptive momentum based on volatility
        vol_norm = df["realized_vol_1h"] / df["realized_vol_1h"].rolling(100).mean()
        df["adaptive_momentum"] = returns.rolling(10).mean() * vol_norm.clip(0.5, 2.0)

        # Adaptive volatility (GARCH-like)
        df["adaptive_volatility"] = returns.rolling(20).std() * np.sqrt(vol_norm.clip(0.5, 2.0))

        # Adaptive trend (trend strength adjusted for volatility)
        trend_raw = (df["close"].rolling(20).mean() - df["close"].rolling(50).mean()) / df["close"]
        df["adaptive_trend"] = trend_raw / (vol_norm + 0.01)

        # 18. Regime features
        # Market regimes based on volatility and trend
        vol_percentile = df["realized_vol_1h"].rolling(200).rank(pct=True)
        trend_strength = trend_raw.abs().rolling(50).rank(pct=True)

        df["regime_state"] = 0  # Normal
        df.loc[(vol_percentile > 0.8) & (trend_strength > 0.7), "regime_state"] = (
            1  # Trending high vol
        )
        df.loc[(vol_percentile > 0.8) & (trend_strength < 0.3), "regime_state"] = (
            2  # Sideways high vol
        )
        df.loc[(vol_percentile < 0.2) & (trend_strength > 0.7), "regime_state"] = (
            3  # Trending low vol
        )
        df.loc[(vol_percentile < 0.2) & (trend_strength < 0.3), "regime_state"] = (
            4  # Sideways low vol
        )

        # Market phase (bull/bear/sideways)
        trend_50 = (df["close"] / df["close"].rolling(50).mean() - 1) * 100
        df["market_phase"] = 0  # Sideways
        df.loc[trend_50 > 5, "market_phase"] = 1  # Bull
        df.loc[trend_50 < -5, "market_phase"] = -1  # Bear

        # Regime classifications (with NaN handling)
        try:
            df["volatility_regime"] = pd.cut(
                vol_percentile.fillna(0.5), bins=3, labels=[0, 1, 2], duplicates="drop"
            ).astype(float)
        except ValueError:
            df["volatility_regime"] = 1.0  # Default to normal regime

        try:
            df["trend_regime"] = pd.cut(
                trend_strength.fillna(0.5), bins=3, labels=[0, 1, 2], duplicates="drop"
            ).astype(float)
        except ValueError:
            df["trend_regime"] = 1.0  # Default to normal regime

        try:
            momentum_percentile = (
                df["adaptive_momentum"].abs().rolling(50).rank(pct=True).fillna(0.5)
            )
            df["momentum_regime"] = pd.cut(
                momentum_percentile, bins=3, labels=[0, 1, 2], duplicates="drop"
            ).astype(float)
        except ValueError:
            df["momentum_regime"] = 1.0

        try:
            volume_percentile = df["volume"].rolling(50).rank(pct=True).fillna(0.5)
            df["volume_regime"] = pd.cut(
                volume_percentile, bins=3, labels=[0, 1, 2], duplicates="drop"
            ).astype(float)
        except ValueError:
            df["volume_regime"] = 1.0

        # Correlation regime (if BTC data available)
        if "btc_returns" in df.columns:
            corr_rolling = returns.rolling(100).corr(df["btc_returns"])
            try:
                corr_percentile = corr_rolling.abs().rolling(50).rank(pct=True).fillna(0.5)
                df["correlation_regime"] = pd.cut(
                    corr_percentile, bins=3, labels=[0, 1, 2], duplicates="drop"
                ).astype(float)
            except ValueError:
                df["correlation_regime"] = 1.0
        else:
            df["correlation_regime"] = 1.0  # Default normal

        # 19. Basic price features
        df["median_price"] = (df["high"] + df["low"]) / 2
        df["typical_price"] = (df["high"] + df["low"] + df["close"]) / 3
        df["weighted_close"] = (df["high"] + df["low"] + 2 * df["close"]) / 4
        df["price_range"] = df["high"] - df["low"]
        df["true_range"] = np.maximum(
            df["high"] - df["low"],
            np.maximum(
                (df["high"] - df["close"].shift(1)).abs(), (df["low"] - df["close"].shift(1)).abs()
            ),
        )
        df["average_price"] = (df["open"] + df["high"] + df["low"] + df["close"]) / 4

        # Return features
        df["log_return"] = np.log(df["close"] / df["close"].shift(1))
        df["squared_return"] = returns**2
        df["abs_return"] = returns.abs()

        # Volume features
        df["volume_rate"] = df["volume"] / df["volume"].rolling(20).mean()
        df["volume_trend"] = (
            df["volume"].rolling(10).mean() / df["volume"].rolling(30).mean() - 1
        ) * 100
        df["volume_oscillator"] = (
            df["volume"].rolling(5).mean() / df["volume"].rolling(20).mean() - 1
        ) * 100

        # Price dynamics
        df["price_momentum"] = (df["close"] / df["close"].shift(10) - 1) * 100
        df["price_acceleration"] = df["price_momentum"] - df["price_momentum"].shift(5)
        df["price_jerk"] = df["price_acceleration"] - df["price_acceleration"].shift(5)

        # Rolling statistics
        df["rolling_min_5"] = df["close"].rolling(5).min()
        df["rolling_max_5"] = df["close"].rolling(5).max()
        df["rolling_median_5"] = df["close"].rolling(5).median()
        df["rolling_std_5"] = df["close"].rolling(5).std()
        df["rolling_var_5"] = df["close"].rolling(5).var()

        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        ml_features = [
            "hurst_exponent",
            "fractal_dimension",
            "efficiency_ratio",
            "trend_quality",
            "realized_vol_5m",
            "realized_vol_15m",
            "realized_vol_1h",
            "garch_vol",
            "vol_regime",
            "return_entropy",
            "amihud_illiquidity",
            "kyle_lambda",
            "returns_ac_1",
            "returns_ac_5",
            "returns_ac_10",
            "price_jump",
            "jump_intensity",
            "vpin",
            "liquidity_adj_returns",
            "cvar_5pct",
            # –ù–æ–≤—ã–µ ML –ø—Ä–∏–∑–Ω–∞–∫–∏
            "price_z_score_20",
            "price_z_score_50",
            "price_z_score_100",
            "volume_z_score_20",
            "volume_z_score_50",
            "return_skewness_20",
            "return_skewness_50",
            "return_kurtosis_20",
            "return_kurtosis_50",
            "price_efficiency_ratio",
            "volume_efficiency_ratio",
            "entropy_20",
            "entropy_50",
            "autocorrelation_lag_1",
            "autocorrelation_lag_5",
            "autocorrelation_lag_10",
            "partial_autocorr_1",
            "partial_autocorr_5",
            "higher_high",
            "lower_low",
            "higher_low",
            "lower_high",
            "bullish_engulfing",
            "bearish_engulfing",
            "hammer",
            "shooting_star",
            "doji",
            "three_white_soldiers",
            "three_black_crows",
            "morning_star",
            "evening_star",
            "harami_bull",
            "harami_bear",
            "adaptive_momentum",
            "adaptive_volatility",
            "adaptive_trend",
            "regime_state",
            "market_phase",
            "volatility_regime",
            "trend_regime",
            "momentum_regime",
            "volume_regime",
            "correlation_regime",
            "median_price",
            "typical_price",
            "weighted_close",
            "price_range",
            "true_range",
            "average_price",
            "log_return",
            "squared_return",
            "abs_return",
            "volume_rate",
            "volume_trend",
            "volume_oscillator",
            "price_momentum",
            "price_acceleration",
            "price_jerk",
            "rolling_min_5",
            "rolling_max_5",
            "rolling_median_5",
            "rolling_std_5",
            "rolling_var_5",
        ]

        # –î–æ–±–∞–≤–ª—è–µ–º —É—Å–ª–æ–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏ —Å–æ–∑–¥–∞–Ω—ã
        if "btc_beta" in df.columns:
            ml_features.extend(["btc_beta", "idio_vol"])
        if "ofi_persistence" in df.columns:
            ml_features.append("ofi_persistence")

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
        for feature in ml_features:
            if feature in df.columns:
                df[feature] = df[feature].fillna(method="ffill").fillna(0)

        if not self.disable_progress:
            created_count = sum(1 for feat in ml_features if feat in df.columns)
            self.logger.info(f"‚úÖ ML optimized features: —Å–æ–∑–¥–∞–Ω–æ {created_count} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        return df

    def _handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        if not self.disable_progress:
            self.logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        info_cols = [
            "id",
            "symbol",
            "timestamp",
            "datetime",
            "open",
            "high",
            "low",
            "close",
            "volume",
            "turnover",
        ]

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Å–∏–º–≤–æ–ª–∞–º –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        processed_dfs = []

        for symbol in df["symbol"].unique():
            symbol_data = df[df["symbol"] == symbol].copy()

            # –î–ª—è –∫–∞–∂–¥–æ–π –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–∏–º–µ–Ω—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –º–µ—Ç–æ–¥ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è
            for col in symbol_data.columns:
                if col in info_cols:
                    continue

                if symbol_data[col].isna().any():
                    # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö (Categorical dtype)
                    if hasattr(symbol_data[col], "cat"):
                        # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–ª–∏ 'FLAT'/'HOLD'
                        if "direction" in col:
                            symbol_data[col] = symbol_data[col].fillna("FLAT")
                        else:
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥—É (–Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)
                            mode = symbol_data[col].mode()
                            if len(mode) > 0:
                                symbol_data[col] = symbol_data[col].fillna(mode.iloc[0])
                    # –î–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º forward fill
                    elif any(
                        indicator in col
                        for indicator in ["sma", "ema", "rsi", "macd", "bb_", "adx"]
                    ):
                        symbol_data[col] = symbol_data[col].ffill()
                    # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º 0
                    else:
                        symbol_data[col] = symbol_data[col].fillna(0)

            # –£–¥–∞–ª—è–µ–º –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –º–æ–≥—É—Ç –±—ã—Ç—å NaN –∏–∑-–∑–∞ —Ä–∞—Å—á–µ—Ç–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º max_period –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Ä–∞—Å—á–µ—Ç–∞ –≤—Å–µ—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            max_period = 240  # –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–ª—è SMA/EMA_200 –∏ –¥—Ä—É–≥–∏—Ö –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            if len(symbol_data) > max_period:
                symbol_data = symbol_data.iloc[max_period:].copy()
            else:
                # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ, –æ—Å—Ç–∞–≤–ª—è–µ–º –≤—Å–µ —á—Ç–æ –µ—Å—Ç—å
                pass

            processed_dfs.append(symbol_data)

        result_df = pd.concat(processed_dfs, ignore_index=True)

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ NaN
        nan_count = result_df.isna().sum().sum()
        if nan_count > 0:
            if not self.disable_progress:
                self.logger.warning(f"–û—Å—Ç–∞–ª–∏—Å—å {nan_count} NaN –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            # –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ NaN
            for col in result_df.columns:
                if result_df[col].isna().any():
                    # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
                    if hasattr(result_df[col], "cat") or result_df[col].dtype == "object":
                        if "direction" in col:
                            result_df[col] = result_df[col].fillna("FLAT")
                        else:
                            mode = result_df[col].mode()
                            if len(mode) > 0:
                                result_df[col] = result_df[col].fillna(mode.iloc[0])
                            else:
                                result_df[col] = result_df[col].fillna("UNKNOWN")
                    # –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
                    else:
                        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º forward fill, –ø–æ—Ç–æ–º backward fill, –ø–æ—Ç–æ–º 0
                        result_df[col] = result_df[col].ffill().bfill().fillna(0)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        numeric_cols = result_df.select_dtypes(include=[np.number]).columns
        inf_count = np.isinf(result_df[numeric_cols]).sum().sum()
        if inf_count > 0:
            if not self.disable_progress:
                self.logger.warning(
                    f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã {inf_count} –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π, –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ –∫–æ–Ω–µ—á–Ω—ã–µ"
                )
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ó–∞–º–µ–Ω—è–µ–º –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç–∏ –Ω–∞ 99-–π –ø–µ—Ä—Å–µ–Ω—Ç–∏–ª—å –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–ª–æ–Ω–∫–∏
            for col in numeric_cols:
                if np.isinf(result_df[col]).any():
                    # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä—Å–µ–Ω—Ç–∏–ª–∏ –Ω–∞ –∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö
                    finite_vals = result_df[col][np.isfinite(result_df[col])]
                    if len(finite_vals) > 0:
                        p99 = finite_vals.quantile(0.99)
                        p1 = finite_vals.quantile(0.01)
                        result_df[col] = result_df[col].replace([np.inf, -np.inf], [p99, p1])
                    else:
                        result_df[col] = result_df[col].replace([np.inf, -np.inf], [0, 0])

        if not self.disable_progress:
            self.logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {len(result_df)} –∑–∞–ø–∏—Å–µ–π")
        return result_df

    def _create_cross_asset_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–µ—Ç –¢–û–ß–ù–û 8 –∫—Ä–æ—Å—Å-–∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ REQUIRED_FEATURES_240"""
        if not self.disable_progress:
            self.logger.info("üéØ –°–æ–∑–¥–∞–Ω–∏–µ 8 CROSS_ASSET_FEATURES...")

        # CROSS_ASSET_FEATURES –∏–∑ features_240.py:
        # "btc_correlation_15m", "btc_correlation_1h", "btc_correlation_4h",
        # "eth_correlation_15m", "eth_correlation_1h", "eth_correlation_4h",
        # "market_beta_1h", "market_beta_4h"

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –±–∞–∑–æ–≤—ã—Ö –∞–∫—Ç–∏–≤–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π inference —Ä–µ–∂–∏–º–∞
        btc_data = (
            df[df["symbol"] == "BTCUSDT"][["datetime", "close"]].copy()
            if "BTCUSDT" in df["symbol"].values
            else pd.DataFrame()
        )
        eth_data = (
            df[df["symbol"] == "ETHUSDT"][["datetime", "close"]].copy()
            if "ETHUSDT" in df["symbol"].values
            else pd.DataFrame()
        )

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ï—Å–ª–∏ —ç—Ç–æ inference —Ä–µ–∂–∏–º –∏ –Ω–µ—Ç BTC/ETH –¥–∞–Ω–Ω—ã—Ö –≤ DataFrame,
        # –∑–∞–≥—Ä—É–∂–∞–µ–º –∏—Ö –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        if (
            hasattr(self, "_is_inference_mode")
            and self._is_inference_mode
            and (len(btc_data) == 0 or len(eth_data) == 0)
        ):
            if not self.disable_progress:
                self.logger.info("üîÑ Inference —Ä–µ–∂–∏–º: –∑–∞–≥—Ä—É–∂–∞–µ–º BTC/ETH –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î...")
            btc_data, eth_data = self._load_btc_eth_data_for_inference(df)
            if not self.disable_progress:
                self.logger.info(
                    f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ BTC –¥–∞–Ω–Ω—ã—Ö: {len(btc_data)}, ETH –¥–∞–Ω–Ω—ã—Ö: {len(eth_data)}"
                )

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –û–±—Ä–∞–±–æ—Ç–∫–∞ BTC –¥–∞–Ω–Ω—ã—Ö —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç NaN –∏ —Ç–∏–ø–æ–≤
        if len(btc_data) > 0:
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º Decimal –≤ float –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
            btc_data["close"] = btc_data["close"].astype(float)
            btc_data["btc_returns"] = btc_data["close"].pct_change()
            btc_data = btc_data[["datetime", "btc_returns"]].copy()
            # –£–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É —Å NaN
            btc_data = btc_data.dropna()
            df = df.merge(btc_data, on="datetime", how="left")
            # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –Ω—É–ª—è–º–∏ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float
            df["btc_returns"] = df["btc_returns"].astype(float).fillna(0.0)
        else:
            if not self.disable_progress:
                self.logger.warning("‚ö†Ô∏è BTC –¥–∞–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω—É–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
            df["btc_returns"] = 0.0

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –û–±—Ä–∞–±–æ—Ç–∫–∞ ETH –¥–∞–Ω–Ω—ã—Ö —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç NaN –∏ —Ç–∏–ø–æ–≤
        if len(eth_data) > 0:
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º Decimal –≤ float –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
            eth_data["close"] = eth_data["close"].astype(float)
            eth_data["eth_returns"] = eth_data["close"].pct_change()
            eth_data = eth_data[["datetime", "eth_returns"]].copy()
            # –£–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É —Å NaN
            eth_data = eth_data.dropna()
            df = df.merge(eth_data, on="datetime", how="left")
            # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –Ω—É–ª—è–º–∏ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float
            df["eth_returns"] = df["eth_returns"].astype(float).fillna(0.0)
        else:
            if not self.disable_progress:
                self.logger.warning("‚ö†Ô∏è ETH –¥–∞–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω—É–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
            df["eth_returns"] = 0.0

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¢–û–ß–ù–û –ö–ê–ö –ü–†–ò –û–ë–£–ß–ï–ù–ò–ò –ú–û–î–ï–õ–ò (–∏–∑ –∞–∞–∞.py)
        # 1. –û—Å–Ω–æ–≤–Ω–∞—è BTC –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è (–∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ - window=96)
        for symbol in df["symbol"].unique():
            if symbol == "BTCUSDT":
                df.loc[df["symbol"] == symbol, "btc_correlation"] = 1.0
            else:
                mask = df["symbol"] == symbol
                symbol_returns = df.loc[mask, "returns"].astype(float)
                btc_returns = df.loc[mask, "btc_returns"].astype(float)

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¢–û–ß–ù–û —Ç–∞–∫–∏–µ –∂–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
                rolling_corr = symbol_returns.rolling(
                    window=96, min_periods=50  # –ö–ê–ö –í –û–†–ò–ì–ò–ù–ê–õ–ï  # –ö–ê–ö –í –û–†–ò–ì–ò–ù–ê–õ–ï
                ).corr(btc_returns)

                df.loc[mask, "btc_correlation"] = rolling_corr

        # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å REQUIRED_FEATURES_240 –¥—É–±–ª–∏—Ä—É–µ–º –≤ –Ω–æ–≤—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
        df["btc_correlation_15m"] = df["btc_correlation"]  # –û—Å–Ω–æ–≤–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
        df["btc_correlation_1h"] = df["btc_correlation"]  # –î—É–±–ª–∏—Ä—É–µ–º
        df["btc_correlation_4h"] = df["btc_correlation"]  # –î—É–±–ª–∏—Ä—É–µ–º

        if not self.disable_progress:
            self.logger.info("  ‚úì BTC –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è: —Å–æ–∑–¥–∞–Ω–∞ –∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ (window=96)")

        # 2. ETH –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ - –í –û–†–ò–ì–ò–ù–ê–õ–ï –ù–ï –ë–´–õ–û, —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫–∏
        # –¢–∞–∫ –∫–∞–∫ –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–∞–ª–∞—Å—å —Å ETH –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è–º–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º 0.5 (–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)
        df["eth_correlation_15m"] = 0.5
        df["eth_correlation_1h"] = 0.5
        df["eth_correlation_4h"] = 0.5

        if not self.disable_progress:
            self.logger.info("ETH correlations: stubs created (not in original training)")

        # 3. BTC Beta - –ö–ê–ö –í –û–†–ò–ì–ò–ù–ê–õ–ï (–∏–∑ –∞–∞–∞.py —Å—Ç—Ä–æ–∫–∞ 1366)
        # Beta –∫ BTC –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ: rolling(100)
        for symbol in df["symbol"].unique():
            mask = df["symbol"] == symbol
            symbol_returns = df.loc[mask, "returns"].astype(float)
            btc_returns = df.loc[mask, "btc_returns"].astype(float)

            # –ö–ê–ö –í –û–†–ò–ì–ò–ù–ê–õ–ï: Beta = Cov / Var —Å window=100
            covariance = symbol_returns.rolling(100, min_periods=50).cov(btc_returns)
            btc_variance = btc_returns.rolling(100, min_periods=50).var()

            # Beta = Cov / Var
            beta = self.safe_divide(covariance, btc_variance, fill_value=1.0)
            beta = beta.clip(-3, 3)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è beta

            df.loc[mask, "btc_beta"] = beta

        # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å REQUIRED_FEATURES_240 –¥—É–±–ª–∏—Ä—É–µ–º –≤ –Ω–æ–≤—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
        df["market_beta_1h"] = df["btc_beta"]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é beta
        df["market_beta_4h"] = df["btc_beta"]  # –î—É–±–ª–∏—Ä—É–µ–º

        if not self.disable_progress:
            self.logger.info("  ‚úì Market beta: —Å–æ–∑–¥–∞–Ω–æ 2 –ø—Ä–∏–∑–Ω–∞–∫–∞")

        # –ò—Ç–æ–≥–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        cross_asset_features = [
            "btc_correlation_15m",
            "btc_correlation_1h",
            "btc_correlation_4h",
            "eth_correlation_15m",
            "eth_correlation_1h",
            "eth_correlation_4h",
            "market_beta_1h",
            "market_beta_4h",
        ]

        created_count = sum(1 for feat in cross_asset_features if feat in df.columns)
        missing_features = [feat for feat in cross_asset_features if feat not in df.columns]

        if not self.disable_progress:
            self.logger.info(f"‚úÖ Cross-asset features: —Å–æ–∑–¥–∞–Ω–æ {created_count}/8 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        if missing_features:
            self.logger.warning(f"üö´ –û–¢–°–£–¢–°–¢–í–£–Æ–¢: {missing_features}")
        # Cross-asset features —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω—ã
        if not self.disable_progress:
            self.logger.info(f"‚úÖ Cross-asset features: —Å–æ–∑–¥–∞–Ω–æ {created_count}/8 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
        for feat in cross_asset_features:
            if feat in df.columns:
                # –î–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ 0.5
                if "correlation" in feat:
                    fill_value = 0.5
                # –î–ª—è beta –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä—ã–Ω–æ—á–Ω—É—é –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç—å 1.0
                elif "beta" in feat:
                    fill_value = 1.0
                else:
                    fill_value = 0.0

                df[feat] = df[feat].fillna(fill_value)

        return df

    def _load_btc_eth_data_for_inference(
        self, df: pd.DataFrame
    ) -> tuple[pd.DataFrame, pd.DataFrame]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç BTC –∏ ETH –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î –¥–ª—è inference —Ä–µ–∂–∏–º–∞ (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ)"""
        try:
            import os
            from datetime import UTC, datetime, timedelta

            from sqlalchemy import create_engine, text

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ DataFrame
            if "datetime" in df.columns:
                start_time = df["datetime"].min() - timedelta(hours=1)
                end_time = df["datetime"].max() + timedelta(hours=1)
            else:
                # Fallback - –±–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 200 —Å–≤–µ—á–µ–π
                end_time = datetime.now(UTC)
                start_time = end_time - timedelta(days=2)

            if not self.disable_progress:
                self.logger.info(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º BTC/ETH –¥–∞–Ω–Ω—ã–µ –∑–∞ –ø–µ—Ä–∏–æ–¥ {start_time} - {end_time}")

            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
            db_url = f"postgresql://{os.getenv('PGUSER', 'obertruper')}:{os.getenv('PGPASSWORD', '')}@{os.getenv('PGHOST', 'localhost')}:{os.getenv('PGPORT', '5555')}/{os.getenv('PGDATABASE', 'bot_trading_v3')}"
            engine = create_engine(db_url)

            # –ó–∞–≥—Ä—É–∂–∞–µ–º BTC –¥–∞–Ω–Ω—ã–µ
            btc_query = text(
                """
                SELECT datetime, close 
                FROM raw_market_data 
                WHERE symbol = 'BTCUSDT' 
                AND datetime BETWEEN :start_time AND :end_time 
                ORDER BY datetime
            """
            )

            # –ó–∞–≥—Ä—É–∂–∞–µ–º ETH –¥–∞–Ω–Ω—ã–µ
            eth_query = text(
                """
                SELECT datetime, close 
                FROM raw_market_data 
                WHERE symbol = 'ETHUSDT' 
                AND datetime BETWEEN :start_time AND :end_time 
                ORDER BY datetime
            """
            )

            with engine.connect() as connection:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º BTC –¥–∞–Ω–Ω—ã–µ
                btc_result = connection.execute(
                    btc_query, {"start_time": start_time, "end_time": end_time}
                ).fetchall()

                # –ó–∞–≥—Ä—É–∂–∞–µ–º ETH –¥–∞–Ω–Ω—ã–µ
                eth_result = connection.execute(
                    eth_query, {"start_time": start_time, "end_time": end_time}
                ).fetchall()

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DataFrame
            btc_data = (
                pd.DataFrame(btc_result, columns=["datetime", "close"])
                if btc_result
                else pd.DataFrame()
            )
            eth_data = (
                pd.DataFrame(eth_result, columns=["datetime", "close"])
                if eth_result
                else pd.DataFrame()
            )

            if not self.disable_progress:
                self.logger.info(
                    f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ BTC –¥–∞–Ω–Ω—ã—Ö: {len(btc_data)}, ETH –¥–∞–Ω–Ω—ã—Ö: {len(eth_data)}"
                )

            return btc_data, eth_data

        except Exception as e:
            if not self.disable_progress:
                self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ BTC/ETH –¥–∞–Ω–Ω—ã—Ö: {e}")
                import traceback

                traceback.print_exc()
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—ã–µ DataFrame
            return pd.DataFrame(), pd.DataFrame()

    def _create_target_variables(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –ë–ï–ó –£–¢–ï–ß–ï–ö –î–ê–ù–ù–´–• - –≤–µ—Ä—Å–∏—è 4.0"""
        if not self.disable_progress:
            self.logger.info("üéØ –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö v4.0 (–±–µ–∑ —É—Ç–µ—á–µ–∫)...")

        # –ü–µ—Ä–∏–æ–¥—ã –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –±—É–¥—É—â–∏—Ö –≤–æ–∑–≤—Ä–∞—Ç–æ–≤ (–≤ —Å–≤–µ—á–∞—Ö –ø–æ 15 –º–∏–Ω—É—Ç)
        return_periods = {
            "15m": 1,  # 15 –º–∏–Ω—É—Ç
            "1h": 4,  # 1 —á–∞—Å
            "4h": 16,  # 4 —á–∞—Å–∞
            "12h": 48,  # 12 —á–∞—Å–æ–≤
        }

        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        # –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–´ –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–∏–≥–Ω–∞–ª–æ–≤
        direction_thresholds = {
            "15m": 0.0015,  # 0.15% - —É–º–µ–Ω—å—à–∞–µ—Ç —à—É–º –æ—Ç –º–µ–ª–∫–∏—Ö –¥–≤–∏–∂–µ–Ω–∏–π
            "1h": 0.003,  # 0.3% - —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ –∫–æ–ª–µ–±–∞–Ω–∏—è
            "4h": 0.007,  # 0.7% - —Ñ–æ–∫—É—Å –Ω–∞ –∑–Ω–∞—á–∏–º—ã—Ö –¥–≤–∏–∂–µ–Ω–∏—è—Ö
            "12h": 0.01,  # 1% - –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã
        }

        # –£—Ä–æ–≤–Ω–∏ –ø—Ä–∏–±—ã–ª–∏ –¥–ª—è –±–∏–Ω–∞—Ä–Ω—ã—Ö —Ü–µ–ª–µ–≤—ã—Ö
        profit_levels = {
            "1pct_4h": (0.01, 16),  # 1% –∑–∞ 4 —á–∞—Å–∞
            "2pct_4h": (0.02, 16),  # 2% –∑–∞ 4 —á–∞—Å–∞
            "3pct_12h": (0.03, 48),  # 3% –∑–∞ 12 —á–∞—Å–æ–≤
            "5pct_12h": (0.05, 48),  # 5% –∑–∞ 12 —á–∞—Å–æ–≤
        }

        # Commission and costs
        commission_rate = 0.0006  # 0.06%
        slippage = 0.0005  # 0.05%

        # A. –ë–∞–∑–æ–≤—ã–µ –≤–æ–∑–≤—Ä–∞—Ç—ã (4)
        for period_name, n_candles in return_periods.items():
            df[f"future_return_{period_name}"] = df.groupby("symbol")["close"].transform(
                lambda x: x.shift(-n_candles) / x - 1
            )

        # B. –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è (4)
        for period_name in return_periods:
            future_return = df[f"future_return_{period_name}"]
            threshold = direction_thresholds[period_name]

            df[f"direction_{period_name}"] = pd.cut(
                future_return,
                bins=[-np.inf, -threshold, threshold, np.inf],
                labels=["DOWN", "FLAT", "UP"],
            )

        # C. –î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π –ø—Ä–∏–±—ã–ª–∏ LONG (4) - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ shift –¥–ª—è –±—É–¥—É—â–∏—Ö —Ü–µ–Ω
        for level_name, (profit_threshold, n_candles) in profit_levels.items():
            # –î–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∏–≥–Ω–µ—Ç –ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ –Ω—É–∂–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è
            max_future_returns = pd.DataFrame()
            for i in range(1, n_candles + 1):
                future_high = df.groupby("symbol")["high"].transform(lambda x: x.shift(-i))
                future_return = future_high / df["close"] - 1
                max_future_returns[f"return_{i}"] = future_return

            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π return –∑–∞ –ø–µ—Ä–∏–æ–¥
            max_return = max_future_returns.max(axis=1)
            df[f"long_will_reach_{level_name}"] = (max_return >= profit_threshold).astype(int)

        # D. –î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π –ø—Ä–∏–±—ã–ª–∏ SHORT (4)
        for level_name, (profit_threshold, n_candles) in profit_levels.items():
            # –î–ª—è SHORT: –ø—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é —Ü–µ–Ω—É
            min_future_returns = pd.DataFrame()
            for i in range(1, n_candles + 1):
                future_low = df.groupby("symbol")["low"].transform(lambda x: x.shift(-i))
                future_return = df["close"] / future_low - 1  # –î–ª—è SHORT –∏–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º
                min_future_returns[f"return_{i}"] = future_return

            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π return –¥–ª—è SHORT –∑–∞ –ø–µ—Ä–∏–æ–¥
            max_return = min_future_returns.max(axis=1)
            df[f"short_will_reach_{level_name}"] = (max_return >= profit_threshold).astype(int)

        # E. –†–∏—Å–∫-–º–µ—Ç—Ä–∏–∫–∏ (4)
        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞ –∑–∞ –ø–µ—Ä–∏–æ–¥ (–¥–ª—è LONG)
        for period_name, n_candles in [("1h", 4), ("4h", 16)]:
            min_prices = pd.DataFrame()
            for i in range(1, n_candles + 1):
                future_low = df.groupby("symbol")["low"].transform(lambda x: x.shift(-i))
                min_prices[f"low_{i}"] = future_low

            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ –∑–∞ –ø–µ—Ä–∏–æ–¥
            min_price = min_prices.min(axis=1)
            df[f"max_drawdown_{period_name}"] = (df["close"] / min_price - 1).fillna(0)

        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç –∑–∞ –ø–µ—Ä–∏–æ–¥ (–¥–ª—è SHORT)
        for period_name, n_candles in [("1h", 4), ("4h", 16)]:
            max_prices = pd.DataFrame()
            for i in range(1, n_candles + 1):
                future_high = df.groupby("symbol")["high"].transform(lambda x: x.shift(-i))
                max_prices[f"high_{i}"] = future_high

            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ –∑–∞ –ø–µ—Ä–∏–æ–¥
            max_price = max_prices.max(axis=1)
            df[f"max_rally_{period_name}"] = (max_price / df["close"] - 1).fillna(0)

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–±–∏—Ä–∞–µ–º —Ç–æ—Ä–≥–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã —Å —É—Ç–µ—á–∫–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö
        # best_action, risk_reward_ratio –∏ optimal_hold_time –±—É–¥—É—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è
        # –≤ trading/signal_generator.py –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏

        # –ü–ï–†–ï–ù–ï–°–ï–ù–û –í –ü–†–ò–ó–ù–ê–ö–ò: signal_strength —Ç–µ–ø–µ—Ä—å feature, –Ω–µ target
        # –≠—Ç–æ –æ—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –±–µ–∑ —É—Ç–µ—á–µ–∫

        # –£–î–ê–õ–ï–ù–û: risk_reward_ratio –∏ optimal_hold_time —Å–æ–¥–µ—Ä–∂–∞–ª–∏ —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        # –≠—Ç–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –±—É–¥—É—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –≤ trading/signal_generator.py
        # –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏, –∞ –Ω–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –±—É–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö

        # –£–î–ê–õ–ï–ù–û: best_action –∏ –≤—Å–µ legacy –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (best_direction, reached, hit)
        # –í –≤–µ—Ä—Å–∏–∏ 4.0 –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ 20 —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –±–µ–∑ —É—Ç–µ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö
        # –í—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —É–∂–µ —Å–æ–∑–¥–∞–Ω—ã –≤—ã—à–µ

        # –§–∏–∫—Ç–∏–≤–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        df["long_tp1_time"] = 16  # 4 —á–∞—Å–∞
        df["long_tp2_time"] = 16
        df["long_tp3_time"] = 48  # 12 —á–∞—Å–æ–≤
        df["long_sl_time"] = 100
        df["short_tp1_time"] = 16
        df["short_tp2_time"] = 16
        df["short_tp3_time"] = 48
        df["short_sl_time"] = 100

        # Expected value –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        df["long_expected_value"] = df["future_return_4h"] * df["long_will_reach_2pct_4h"] * 2.0
        df["short_expected_value"] = -df["future_return_4h"] * df["short_will_reach_2pct_4h"] * 2.0

        # Optimal entry —Ñ–∏–∫—Ç–∏–≤–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        df["long_optimal_entry_time"] = 1
        df["long_optimal_entry_price"] = df["close"]
        df["long_optimal_entry_improvement"] = 0
        df["short_optimal_entry_time"] = 1
        df["short_optimal_entry_price"] = df["close"]
        df["short_optimal_entry_improvement"] = 0

        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if not self.disable_progress:
            self.logger.info("  ‚úÖ –°–æ–∑–¥–∞–Ω–æ 20 —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –±–µ–∑ —É—Ç–µ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö")
            self.logger.info("  üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π:")
            for period in ["15m", "1h", "4h", "12h"]:
                if f"direction_{period}" in df.columns:
                    dist = df[f"direction_{period}"].value_counts(normalize=True) * 100
                    self.logger.info(
                        f"     {period}: UP={dist.get('UP', 0):.1f}%, DOWN={dist.get('DOWN', 0):.1f}%, FLAT={dist.get('FLAT', 0):.1f}%"
                    )

        return df

    def _normalize_walk_forward(self, df: pd.DataFrame, train_end_date: str) -> pd.DataFrame:
        """Walk-forward –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RobustScaler"""
        if not self.disable_progress:
            self.logger.info(f"üìä Walk-forward –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å –≥—Ä–∞–Ω–∏—Ü–µ–π: {train_end_date}")

        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        train_mask = df["datetime"] <= pd.to_datetime(train_end_date)
        test_mask = ~train_mask

        # –°—Ç–æ–ª–±—Ü—ã –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        exclude_cols = [
            "id",
            "symbol",
            "timestamp",
            "datetime",
            "sector",
            "open",
            "high",
            "low",
            "close",
            "volume",
            "turnover",
        ]

        # –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        target_cols = [
            col
            for col in df.columns
            if col.startswith(("future_", "direction_", "long_will_reach_", "short_will_reach_"))
        ]
        exclude_cols.extend(target_cols)

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (—É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã)
        time_cols = [
            "hour",
            "minute",
            "dayofweek",
            "day",
            "month",
            "is_weekend",
            "asian_session",
            "european_session",
            "american_session",
            "session_overlap",
        ]
        exclude_cols.extend(time_cols)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        feature_cols = [col for col in numeric_cols if col not in exclude_cols]

        if not self.disable_progress:
            self.logger.info(
                f"–ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º {len(feature_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {df['symbol'].nunique()} —Å–∏–º–≤–æ–ª–æ–≤"
            )

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        for symbol in df["symbol"].unique():
            symbol_mask = df["symbol"] == symbol
            train_symbol_mask = train_mask & symbol_mask
            test_symbol_mask = test_mask & symbol_mask

            if train_symbol_mask.sum() == 0:
                continue

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RobustScaler –¥–ª—è —Å–∏–º–≤–æ–ª–∞
            if symbol not in self.scalers:
                self.scalers[symbol] = RobustScaler()

            # –ü–æ–ª—É—á–∞–µ–º train –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è scaler
            train_data = df.loc[train_symbol_mask, feature_cols].dropna()

            if len(train_data) > 0:
                # –û–±—É—á–∞–µ–º scaler —Ç–æ–ª—å–∫–æ –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö
                self.scalers[symbol].fit(train_data)

                # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ train –¥–∞–Ω–Ω—ã–º
                if train_symbol_mask.sum() > 0:
                    train_to_scale = df.loc[train_symbol_mask, feature_cols].fillna(0)
                    df.loc[train_symbol_mask, feature_cols] = self.scalers[symbol].transform(
                        train_to_scale
                    )

                # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ test –¥–∞–Ω–Ω—ã–º
                if test_symbol_mask.sum() > 0:
                    test_to_scale = df.loc[test_symbol_mask, feature_cols].fillna(0)
                    df.loc[test_symbol_mask, feature_cols] = self.scalers[symbol].transform(
                        test_to_scale
                    )

        if not self.disable_progress:
            self.logger.info("‚úÖ Walk-forward –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

        return df

    def _log_feature_statistics(self, df: pd.DataFrame):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º"""
        if not self.disable_progress:
            feature_counts = {
                "basic": len(
                    [
                        col
                        for col in df.columns
                        if col in ["returns", "high_low_ratio", "close_open_ratio", "volume_ratio"]
                    ]
                ),
                "technical": len(
                    [
                        col
                        for col in df.columns
                        if any(ind in col for ind in ["sma", "ema", "rsi", "macd", "bb", "atr"])
                    ]
                ),
                "microstructure": len(
                    [
                        col
                        for col in df.columns
                        if any(
                            ms in col for ms in ["spread", "imbalance", "toxicity", "illiquidity"]
                        )
                    ]
                ),
                "temporal": len(
                    [
                        col
                        for col in df.columns
                        if any(t in col for t in ["hour", "day", "month", "session"])
                    ]
                ),
                "cross_asset": len(
                    [
                        col
                        for col in df.columns
                        if any(ca in col for ca in ["btc_", "sector", "rank", "momentum"])
                    ]
                ),
            }

            self.logger.info(f"üìä –°–æ–∑–¥–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º: {feature_counts}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            missing_counts = df.isnull().sum()
            if missing_counts.sum() > 0:
                self.logger.warning(
                    f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ {missing_counts[missing_counts > 0].shape[0]} –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö"
                )

    def get_feature_names(self, include_targets: bool = False) -> list[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        return []

    def save_scalers(self, path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∫–µ–π–ª–µ—Ä–æ–≤ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ"""
        import pickle

        with open(path, "wb") as f:
            pickle.dump(self.scalers, f)

        if not self.disable_progress:
            self.logger.info(f"–°–∫–µ–π–ª–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {path}")

    def load_scalers(self, path: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Å–∫–µ–π–ª–µ—Ä–æ–≤"""
        import pickle

        with open(path, "rb") as f:
            self.scalers = pickle.load(f)

        if not self.disable_progress:
            self.logger.info(f"–°–∫–µ–π–ª–µ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ {path}")

    def _add_enhanced_features(
        self, df: pd.DataFrame, all_symbols_data: dict[str, pd.DataFrame]
    ) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è direction prediction

        Args:
            df: DataFrame —Å –±–∞–∑–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            all_symbols_data: —Å–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è cross-asset features

        Returns:
            DataFrame —Å enhanced features
        """
        try:
            from data.enhanced_features import EnhancedFeatureEngineer
        except ImportError:
            self.logger.warning(
                "‚ö†Ô∏è –ú–æ–¥—É–ª—å enhanced_features –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º enhanced features"
            )
            return df

        self.logger.info("üöÄ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ enhanced features –¥–ª—è direction prediction...")

        enhanced_engineer = EnhancedFeatureEngineer()
        enhanced_dfs = []

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Å–∏–º–≤–æ–ª
        for symbol in tqdm(
            df["symbol"].unique(), desc="Enhanced features", disable=self.disable_progress
        ):
            symbol_data = df[df["symbol"] == symbol].copy()

            # –ü—Ä–∏–º–µ–Ω—è–µ–º enhanced features
            enhanced_data = enhanced_engineer.create_enhanced_features(
                symbol_data, all_symbols_data if len(all_symbols_data) > 1 else None
            )

            enhanced_dfs.append(enhanced_data)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        result_df = pd.concat(enhanced_dfs, ignore_index=True)

        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        original_cols = set(df.columns)
        new_cols = set(result_df.columns) - original_cols

        if new_cols:
            self.logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(new_cols)} enhanced features")

            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            categories = {
                "market_regime": [col for col in new_cols if "regime" in col or "wyckoff" in col],
                "microstructure": [
                    col for col in new_cols if any(x in col for x in ["ofi", "tick", "imbalance"])
                ],
                "cross_asset": [
                    col for col in new_cols if any(x in col for x in ["btc_", "sector_", "beta_"])
                ],
                "sentiment": [
                    col
                    for col in new_cols
                    if any(x in col for x in ["fear_greed", "panic", "euphoria"])
                ],
            }

            for category, cols in categories.items():
                if cols:
                    self.logger.info(f"  - {category}: {len(cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        return result_df
