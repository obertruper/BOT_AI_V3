#!/usr/bin/env python3
"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö ML - –≤—Ö–æ–¥–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –≤ –ë–î
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞ NaN, –Ω—É–ª–∏ –∏ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
"""

import asyncio

import numpy as np
import pandas as pd

from core.logger import setup_logger
from database.connections.postgres import AsyncPGPool

logger = setup_logger("ml_data_quality")


async def check_processed_market_data():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–∞–±–ª–∏—Ü–µ processed_market_data"""

    logger.info("\n" + "=" * 60)
    logger.info("üìä –ü–†–û–í–ï–†–ö–ê –¢–ê–ë–õ–ò–¶–´ processed_market_data")
    logger.info("=" * 60)

    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ
    query = """
        SELECT *
        FROM processed_market_data
        WHERE timestamp > EXTRACT(EPOCH FROM NOW() - INTERVAL '1 hour') * 1000
        ORDER BY timestamp DESC
        LIMIT 100
    """

    rows = await AsyncPGPool.fetch(query)

    if not rows:
        logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å")
        return False

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DataFrame –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    df = pd.DataFrame([dict(row) for row in rows])

    logger.info(f"üìà –ù–∞–π–¥–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é –∫–æ–ª–æ–Ω–∫—É —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    numeric_columns = df.select_dtypes(include=[np.number]).columns

    issues = []

    for col in numeric_columns:
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        if col in ["id", "timestamp", "created_at", "updated_at"]:
            continue

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN
        nan_count = df[col].isna().sum()
        if nan_count > 0:
            issues.append(f"‚ùå {col}: {nan_count} NaN –∑–Ω–∞—á–µ–Ω–∏–π")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω—É–ª–∏ (–¥–ª—è –∫–æ–ª–æ–Ω–æ–∫ –≥–¥–µ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –Ω—É–ª–µ–π)
        if col not in ["returns_5m", "returns_15m", "returns_30m", "rsi_diff", "macd_diff"]:
            zero_count = (df[col] == 0).sum()
            if zero_count > len(df) * 0.5:  # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 50% –Ω—É–ª–µ–π
                issues.append(f"‚ö†Ô∏è {col}: {zero_count}/{len(df)} –Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ inf
        inf_count = np.isinf(df[col].replace([None], 0)).sum()
        if inf_count > 0:
            issues.append(f"‚ùå {col}: {inf_count} inf –∑–Ω–∞—á–µ–Ω–∏–π")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        if "rsi" in col.lower():
            out_of_range = ((df[col] < 0) | (df[col] > 100)).sum()
            if out_of_range > 0:
                issues.append(f"‚ùå {col}: {out_of_range} –∑–Ω–∞—á–µ–Ω–∏–π –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ [0, 100]")

    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–ª—é—á–µ–≤—ã–º –ø–æ–ª—è–º
    logger.info("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–ª–µ–π:")

    key_fields = [
        "symbol",
        "close",
        "volume",
        "rsi_14",
        "macd",
        "bb_upper",
        "bb_lower",
        "returns_5m",
        "returns_15m",
        "returns_30m",
    ]

    for field in key_fields:
        if field in df.columns:
            if field == "symbol":
                unique = df[field].nunique()
                logger.info(f"  ‚Ä¢ {field}: {unique} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤")
            else:
                mean_val = df[field].mean()
                std_val = df[field].std()
                min_val = df[field].min()
                max_val = df[field].max()
                logger.info(
                    f"  ‚Ä¢ {field}: mean={mean_val:.4f}, std={std_val:.4f}, min={min_val:.4f}, max={max_val:.4f}"
                )

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
    logger.info("\n‚è∞ –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫:")
    timestamps = df["timestamp"].values
    time_diffs = np.diff(sorted(timestamps))

    if len(time_diffs) > 0:
        avg_diff = np.mean(time_diffs) / 1000 / 60  # –≤ –º–∏–Ω—É—Ç–∞—Ö
        logger.info(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –∑–∞–ø–∏—Å—è–º–∏: {avg_diff:.2f} –º–∏–Ω—É—Ç")

        if avg_diff > 5:
            issues.append(f"‚ö†Ô∏è –ë–æ–ª—å—à–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–∫–∏ –º–µ–∂–¥—É –¥–∞–Ω–Ω—ã–º–∏: {avg_diff:.2f} –º–∏–Ω—É—Ç")

    # –í—ã–≤–æ–¥–∏–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
    if issues:
        logger.warning("\n‚ö†Ô∏è –ù–ê–ô–î–ï–ù–ù–´–ï –ü–†–û–ë–õ–ï–ú–´:")
        for issue in issues:
            logger.warning(f"  {issue}")
        return False
    else:
        logger.info("\n‚úÖ –í—Å–µ –¥–∞–Ω–Ω—ã–µ –≤ –Ω–æ—Ä–º–µ!")
        return True


async def check_ml_predictions():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π ML –º–æ–¥–µ–ª–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ signals"""

    logger.info("\n" + "=" * 60)
    logger.info("ü§ñ –ü–†–û–í–ï–†–ö–ê ML –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô (signals)")
    logger.info("=" * 60)

    query = """
        SELECT *
        FROM signals
        WHERE created_at > NOW() - INTERVAL '1 hour'
        AND strategy_name LIKE '%ml%'
        ORDER BY created_at DESC
        LIMIT 50
    """

    rows = await AsyncPGPool.fetch(query)

    if not rows:
        logger.warning("‚ö†Ô∏è –ù–µ—Ç ML —Å–∏–≥–Ω–∞–ª–æ–≤ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å")
        return False

    logger.info(f"üìà –ù–∞–π–¥–µ–Ω–æ {len(rows)} ML —Å–∏–≥–Ω–∞–ª–æ–≤ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å")

    issues = []
    signal_stats = {"total": len(rows), "buy": 0, "sell": 0, "symbols": set(), "strengths": []}

    for row in rows:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è
        if not row["symbol"]:
            issues.append("‚ùå –ü—É—Å—Ç–æ–π —Å–∏–º–≤–æ–ª –≤ —Å–∏–≥–Ω–∞–ª–µ")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º signal_type –≤–º–µ—Å—Ç–æ direction
        signal_type = row.get("signal_type", "")
        if signal_type not in ["BUY", "SELL"]:
            issues.append(f"‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø —Å–∏–≥–Ω–∞–ª–∞: {signal_type}")
        else:
            signal_stats["buy" if signal_type == "BUY" else "sell"] += 1

        signal_stats["symbols"].add(row["symbol"])

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º strength (–Ω–µ signal_strength)
        strength = float(row["strength"]) if row.get("strength") else 0
        if strength <= 0 or strength > 1:
            issues.append(f"‚ùå –ù–µ–≤–µ—Ä–Ω–∞—è —Å–∏–ª–∞ —Å–∏–≥–Ω–∞–ª–∞: {strength} –¥–ª—è {row['symbol']}")
        else:
            signal_stats["strengths"].append(strength)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–Ω—ã suggested_*
        for price_field in ["suggested_price", "suggested_stop_loss", "suggested_take_profit"]:
            if row.get(price_field):
                price = float(row[price_field])
                if price <= 0:
                    issues.append(f"‚ùå –ù–µ–≤–µ—Ä–Ω–∞—è —Ü–µ–Ω–∞ {price_field}: {price} –¥–ª—è {row['symbol']}")
                elif price > 1000000:  # –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è —Ü–µ–Ω–∞
                    issues.append(
                        f"‚ö†Ô∏è –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ü–µ–Ω–∞ {price_field}: {price} –¥–ª—è {row['symbol']}"
                    )

    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    logger.info("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏–≥–Ω–∞–ª–æ–≤:")
    logger.info(f"  ‚Ä¢ –í—Å–µ–≥–æ —Å–∏–≥–Ω–∞–ª–æ–≤: {signal_stats['total']}")
    logger.info(
        f"  ‚Ä¢ BUY: {signal_stats['buy']} ({signal_stats['buy'] / signal_stats['total'] * 100:.1f}%)"
    )
    logger.info(
        f"  ‚Ä¢ SELL: {signal_stats['sell']} ({signal_stats['sell'] / signal_stats['total'] * 100:.1f}%)"
    )
    logger.info(f"  ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤: {len(signal_stats['symbols'])}")

    if signal_stats["strengths"]:
        avg_strength = np.mean(signal_stats["strengths"])
        logger.info(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è —Å–∏–ª–∞ —Å–∏–≥–Ω–∞–ª–∞: {avg_strength:.3f}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        if avg_strength < 0.4 or avg_strength > 0.6:
            issues.append(f"‚ö†Ô∏è –ù–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ä–µ–¥–Ω—è—è —Å–∏–ª–∞: {avg_strength:.3f}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–æ—Ç—É —Å–∏–≥–Ω–∞–ª–æ–≤
    if rows:
        first_time = rows[-1]["created_at"]
        last_time = rows[0]["created_at"]
        time_span = (last_time - first_time).total_seconds() / 60  # –≤ –º–∏–Ω—É—Ç–∞—Ö

        if time_span > 0:
            signals_per_minute = len(rows) / time_span
            logger.info(f"  ‚Ä¢ –ß–∞—Å—Ç–æ—Ç–∞: {signals_per_minute:.2f} —Å–∏–≥–Ω–∞–ª–æ–≤/–º–∏–Ω—É—Ç—É")

            if signals_per_minute > 10:
                issues.append(
                    f"‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∞—è —á–∞—Å—Ç–æ—Ç–∞: {signals_per_minute:.2f} —Å–∏–≥–Ω–∞–ª–æ–≤/–º–∏–Ω—É—Ç—É"
                )

    # –í—ã–≤–æ–¥–∏–º –ø—Ä–æ–±–ª–µ–º—ã
    if issues:
        logger.warning("\n‚ö†Ô∏è –ù–ê–ô–î–ï–ù–ù–´–ï –ü–†–û–ë–õ–ï–ú–´:")
        for issue in issues[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
            logger.warning(f"  {issue}")
        if len(issues) > 10:
            logger.warning(f"  ... –∏ –µ—â–µ {len(issues) - 10} –ø—Ä–æ–±–ª–µ–º")
        return False
    else:
        logger.info("\n‚úÖ ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –Ω–æ—Ä–º–µ!")
        return True


async def check_feature_engineering():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ feature engineering"""

    logger.info("\n" + "=" * 60)
    logger.info("üîß –ü–†–û–í–ï–†–ö–ê FEATURE ENGINEERING")
    logger.info("=" * 60)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    query = """
        SELECT
            symbol,
            COUNT(*) as count,
            MIN(timestamp) as min_ts,
            MAX(timestamp) as max_ts,
            AVG(close) as avg_close,
            AVG(volume) as avg_volume
        FROM processed_market_data
        WHERE timestamp > EXTRACT(EPOCH FROM NOW() - INTERVAL '1 hour') * 1000
        GROUP BY symbol
    """

    rows = await AsyncPGPool.fetch(query)

    if not rows:
        logger.error("‚ùå –ù–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")
        return False

    logger.info("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–∏–º–≤–æ–ª–∞–º:")

    for row in rows:
        time_range = (row["max_ts"] - row["min_ts"]) / 1000 / 60  # –≤ –º–∏–Ω—É—Ç–∞—Ö
        logger.info(f"  ‚Ä¢ {row['symbol']}: {row['count']} –∑–∞–ø–∏—Å–µ–π –∑–∞ {time_range:.1f} –º–∏–Ω—É—Ç")
        logger.info(
            f"    –°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞: ${row['avg_close']:.2f}, –°—Ä–µ–¥–Ω–∏–π –æ–±—ä–µ–º: {row['avg_volume']:.0f}"
        )

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
    query = """
        SELECT DISTINCT symbol
        FROM processed_market_data
        WHERE timestamp > EXTRACT(EPOCH FROM NOW() - INTERVAL '5 minutes') * 1000
    """

    recent_symbols = await AsyncPGPool.fetch(query)
    recent_symbols = {row["symbol"] for row in recent_symbols}

    if len(recent_symbols) < 5:
        logger.warning(f"‚ö†Ô∏è –ú–∞–ª–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤: {len(recent_symbols)}")
        return False
    else:
        logger.info(f"‚úÖ –ê–∫—Ç–∏–≤–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤: {len(recent_symbols)}")
        return True


async def fix_data_issues():
    """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º —Å –¥–∞–Ω–Ω—ã–º–∏"""

    logger.info("\n" + "=" * 60)
    logger.info("üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ü–†–û–ë–õ–ï–ú –° –î–ê–ù–ù–´–ú–ò")
    logger.info("=" * 60)

    fixes_applied = 0

    # 1. –£–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å–∏ —Å NaN –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
    query = """
        DELETE FROM processed_market_data
        WHERE close IS NULL
           OR volume IS NULL
           OR rsi_14 IS NULL
           OR close = 'NaN'::float
           OR volume = 'NaN'::float
    """

    result = await AsyncPGPool.execute(query)
    if "DELETE" in result:
        count = int(result.split()[1])
        if count > 0:
            logger.info(f"‚úÖ –£–¥–∞–ª–µ–Ω–æ {count} –∑–∞–ø–∏—Å–µ–π —Å NaN –∑–Ω–∞—á–µ–Ω–∏—è–º–∏")
            fixes_applied += count

    # 2. –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –Ω—É–ª–µ–≤—ã–µ –æ–±—ä–µ–º—ã
    query = """
        UPDATE processed_market_data
        SET volume = (
            SELECT AVG(volume)
            FROM processed_market_data p2
            WHERE p2.symbol = processed_market_data.symbol
              AND p2.volume > 0
              AND p2.timestamp > EXTRACT(EPOCH FROM NOW() - INTERVAL '1 hour') * 1000
        )
        WHERE volume = 0
          AND timestamp > EXTRACT(EPOCH FROM NOW() - INTERVAL '1 hour') * 1000
    """

    result = await AsyncPGPool.execute(query)
    if "UPDATE" in result:
        count = int(result.split()[1])
        if count > 0:
            logger.info(f"‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ {count} –∑–∞–ø–∏—Å–µ–π —Å –Ω—É–ª–µ–≤—ã–º –æ–±—ä–µ–º–æ–º")
            fixes_applied += count

    # 3. –ò—Å–ø—Ä–∞–≤–ª—è–µ–º RSI –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
    query = """
        UPDATE processed_market_data
        SET rsi_14 = CASE
            WHEN rsi_14 > 100 THEN 100
            WHEN rsi_14 < 0 THEN 0
            ELSE rsi_14
        END
        WHERE (rsi_14 > 100 OR rsi_14 < 0)
          AND timestamp > EXTRACT(EPOCH FROM NOW() - INTERVAL '1 hour') * 1000
    """

    result = await AsyncPGPool.execute(query)
    if "UPDATE" in result:
        count = int(result.split()[1])
        if count > 0:
            logger.info(f"‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ {count} –∑–∞–ø–∏—Å–µ–π —Å RSI –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞")
            fixes_applied += count

    if fixes_applied > 0:
        logger.info(f"\n‚úÖ –í—Å–µ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: {fixes_applied} –ø—Ä–æ–±–ª–µ–º")
    else:
        logger.info("\n‚úÖ –ü—Ä–æ–±–ª–µ–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ω–µ —Ç—Ä–µ–±—É—é—Ç—Å—è")

    return fixes_applied


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏"""

    logger.info("\n" + "=" * 80)
    logger.info("üîç –ü–†–û–í–ï–†–ö–ê –ö–ê–ß–ï–°–¢–í–ê ML –î–ê–ù–ù–´–•")
    logger.info("=" * 80)

    results = {}

    # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º processed_market_data
    results["processed_data"] = await check_processed_market_data()

    # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    results["ml_predictions"] = await check_ml_predictions()

    # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º feature engineering
    results["feature_engineering"] = await check_feature_engineering()

    # 4. –ü—Ä–∏–º–µ–Ω—è–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if not all(results.values()):
        fixes = await fix_data_issues()
        if fixes > 0:
            logger.info("\nüîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π...")
            results["processed_data_after"] = await check_processed_market_data()
            results["ml_predictions_after"] = await check_ml_predictions()

    # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    logger.info("\n" + "=" * 80)
    logger.info("üìä –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
    logger.info("=" * 80)

    passed = sum(1 for v in results.values() if v)
    total = len(results)

    for check, status in results.items():
        status_str = "‚úÖ PASSED" if status else "‚ùå FAILED"
        logger.info(f"  ‚Ä¢ {check}: {status_str}")

    logger.info(f"\nüìà –†–µ–∑—É–ª—å—Ç–∞—Ç: {passed}/{total} –ø—Ä–æ–≤–µ—Ä–æ–∫ –ø—Ä–æ–π–¥–µ–Ω–æ")

    if passed == total:
        logger.info("üéâ –í—Å–µ –¥–∞–Ω–Ω—ã–µ –≤ –æ—Ç–ª–∏—á–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏!")
    elif passed >= total * 0.7:
        logger.info("‚úÖ –î–∞–Ω–Ω—ã–µ –≤ —Ö–æ—Ä–æ—à–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏")
    else:
        logger.warning("‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç—Å—è –≤–Ω–∏–º–∞–Ω–∏–µ –∫ –∫–∞—á–µ—Å—Ç–≤—É –¥–∞–Ω–Ω—ã—Ö")


if __name__ == "__main__":
    asyncio.run(main())
