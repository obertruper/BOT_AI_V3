#!/usr/bin/env python3
"""
–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GPU –¥–ª—è RTX 5090 —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
"""

import os

# –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ: —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –î–û –∏–º–ø–æ—Ä—Ç–∞ torch
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
os.environ["TORCH_CUDA_ARCH_LIST"] = "8.6;8.9;9.0;12.0"  # –í–∫–ª—é—á–∞–µ–º sm_120 –¥–ª—è RTX 5090
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"
os.environ["TORCH_COMPILE_DISABLE"] = "1"  # RTX 5090 –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç torch.compile

# –û—Ç–∫–ª—é—á–∞–µ–º JIT –∫–æ–º–ø–∏–ª—è—Ü–∏—é –¥–ª—è –Ω–æ–≤—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
os.environ["PYTORCH_JIT"] = "0"

# –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –Ω–æ–≤—ã–º–∏ GPU
os.environ["CUDA_FORCE_PTX_JIT"] = "1"

import torch


def check_gpu_compatibility():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ GPU —Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º PyTorch"""
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ GPU RTX 5090...")
    print(f"PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__}")
    print(f"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}")

    if torch.cuda.is_available():
        print(f"CUDA –≤–µ—Ä—Å–∏—è –≤ PyTorch: {torch.version.cuda}")
        print(f"cuDNN –≤–µ—Ä—Å–∏—è: {torch.backends.cudnn.version()}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU: {torch.cuda.device_count()}")

        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            print(f"\nGPU {i}: {props.name}")
            print(f"  Compute Capability: {props.major}.{props.minor}")
            print(f"  –ü–∞–º—è—Ç—å: {props.total_memory / 1024**3:.1f} GB")
            print(f"  Multiprocessors: {props.multi_processor_count}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ RTX 5090
            if props.major == 12 and props.minor == 0:
                print("  ‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω RTX 5090 (sm_120)")
                print("  ‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–µ–∂–∏–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏")
    else:
        print("‚ùå CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        try:
            import subprocess

            result = subprocess.run(
                [
                    "nvidia-smi",
                    "--query-gpu=name,driver_version,compute_cap",
                    "--format=csv",
                ],
                capture_output=True,
                text=True,
            )
            if result.returncode == 0:
                print("\nüìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç nvidia-smi:")
                print(result.stdout)
        except Exception as e:
            print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å nvidia-smi: {e}")


def test_gpu_operations():
    """–¢–µ—Å—Ç –±–∞–∑–æ–≤—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –Ω–∞ GPU"""
    print("\nüß™ –¢–µ—Å—Ç –æ–ø–µ—Ä–∞—Ü–∏–π –Ω–∞ GPU...")

    try:
        # –ü–æ–ø—ã—Ç–∫–∞ —Å–æ–∑–¥–∞—Ç—å —Ç–µ–Ω–∑–æ—Ä –Ω–∞ GPU
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

        if device.type == "cuda":
            # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç
            x = torch.zeros(100, 100, device=device)
            y = torch.ones(100, 100, device=device)
            z = x + y

            print("‚úÖ –ë–∞–∑–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç–∞—é—Ç")
            print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç —Å—É–º–º—ã: {z[0, 0].item()}")

            # –¢–µ—Å—Ç –º–∞—Ç—Ä–∏—á–Ω–æ–≥–æ —É–º–Ω–æ–∂–µ–Ω–∏—è
            a = torch.randn(1000, 1000, device=device)
            b = torch.randn(1000, 1000, device=device)
            c = torch.matmul(a, b)

            print("‚úÖ –ú–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")
            print(f"   –§–æ—Ä–º–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {c.shape}")

            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
            print("\nüíæ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ GPU:")
            print(f"   –í—ã–¥–µ–ª–µ–Ω–æ: {torch.cuda.memory_allocated(0) / 1024**2:.1f} MB")
            print(
                f"   –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ: {torch.cuda.memory_reserved(0) / 1024**2:.1f} MB"
            )

        else:
            print("‚ö†Ô∏è  –†–∞–±–æ—Ç–∞–µ–º –Ω–∞ CPU")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ GPU: {e}")
        import traceback

        traceback.print_exc()


def get_gpu_init_code():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–¥ –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ GPU –≤ –¥—Ä—É–≥–∏—Ö –º–æ–¥—É–ª—è—Ö"""
    return """
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GPU –¥–ª—è RTX 5090
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ['TORCH_CUDA_ARCH_LIST'] = '8.6;8.9;9.0;12.0'
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
os.environ['TORCH_COMPILE_DISABLE'] = '1'
os.environ['PYTORCH_JIT'] = '0'
os.environ['CUDA_FORCE_PTX_JIT'] = '1'
"""


if __name__ == "__main__":
    check_gpu_compatibility()
    test_gpu_operations()

    print("\nüìù –ö–æ–¥ –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ GPU (–¥–æ–±–∞–≤—å—Ç–µ –≤ –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–æ–≤):")
    print(get_gpu_init_code())
