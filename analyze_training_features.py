#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ —Ç–æ—á–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –æ–±—É—á–∞—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ BOT_AI_V2/–∞–∞–∞.py
–ò–∑–≤–ª–µ–∫–∞–µ—Ç –í–°–ï —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∏ –∏—Ö —Ñ–æ—Ä–º—É–ª—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ—á–Ω–æ–π –∫–æ–ø–∏–∏.
"""

import re
from pathlib import Path


class TrainingFeatureAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–∞—é—â–∏–π —Ñ–∞–π–ª –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""

    def __init__(self, training_file_path: str):
        self.training_file_path = Path(training_file_path)
        self.features = []
        self.feature_formulas = {}
        self.section_features = {}

    def read_file_content(self) -> str:
        """–ß–∏—Ç–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞"""
        with open(self.training_file_path, encoding="utf-8") as f:
            return f.read()

    def extract_features_from_section(self, section_content: str, section_name: str) -> list[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —Å–µ–∫—Ü–∏–∏ –∫–æ–¥–∞"""
        features = []

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        patterns = [
            r"df\['([^']+)'\]\s*=",  # df['feature_name'] =
            r'df\["([^"]+)"\]\s*=',  # df["feature_name"] =
            r"df\[f'([^']+)'\]\s*=",  # df[f'feature_name'] =
            r'df\[f"([^"]+)"\]\s*=',  # df[f"feature_name"] =
        ]

        for pattern in patterns:
            matches = re.findall(pattern, section_content)
            for match in matches:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º f-—Å—Ç—Ä–æ–∫–∏
                if "{" in match:
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º f-—Å—Ç—Ä–æ–∫–∏ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                    continue
                features.append(match)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ü–∏–∫–ª–æ–≤ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        # –ò—â–µ–º —Ü–∏–∫–ª—ã —Ç–∏–ø–∞ for period in [5, 10, 20]:
        period_loops = re.findall(r"for\s+(\w+)\s+in\s+\[([^\]]+)\]:", section_content)
        for var_name, periods_str in period_loops:
            try:
                periods = eval(f"[{periods_str}]")
                # –ò—â–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤–Ω—É—Ç—Ä–∏ —Ü–∏–∫–ª–∞
                loop_section = section_content[
                    section_content.find(f"for {var_name} in") : section_content.find(
                        "# ", section_content.find(f"for {var_name} in")
                    )
                ]
                feature_patterns = re.findall(
                    rf"df\[f?['\"]([^'\"]*\{{{var_name}}}[^'\"]*)['\"]", loop_section
                )
                for pattern in feature_patterns:
                    for period in periods:
                        feature_name = pattern.format(**{var_name: period})
                        features.append(feature_name)
            except:
                pass

        return list(set(features))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã

    def analyze_basic_features_section(self, content: str) -> list[str]:
        """–ê–Ω–∞–ª–∏–∑ —Å–µ–∫—Ü–∏–∏ –±–∞–∑–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        # –ù–∞–π—Ç–∏ —Å–µ–∫—Ü–∏—é _create_basic_features
        start_marker = "def _create_basic_features"
        end_marker = "def _create_"

        start_idx = content.find(start_marker)
        if start_idx == -1:
            return []

        # –ù–∞–π—Ç–∏ —Å–ª–µ–¥—É—é—â—É—é —Ñ—É–Ω–∫—Ü–∏—é
        next_func_idx = content.find(end_marker, start_idx + len(start_marker))
        if next_func_idx == -1:
            section_content = content[start_idx:]
        else:
            section_content = content[start_idx:next_func_idx]

        features = []

        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∏–∑ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞)
        basic_features = [
            "returns",
            "returns_5",
            "returns_10",
            "returns_20",  # —Ü–∏–∫–ª—ã –≤–æ–∑–≤—Ä–∞—Ç–æ–≤
            "high_low_ratio",
            "close_open_ratio",
            "close_position",
            "volume_ratio",
            "turnover_ratio",
            "vwap",
            "close_vwap_ratio",
            "vwap_extreme_deviation",
        ]

        features.extend(basic_features)

        return features

    def analyze_technical_indicators_section(self, content: str) -> list[str]:
        """–ê–Ω–∞–ª–∏–∑ —Å–µ–∫—Ü–∏–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        start_marker = "def _create_technical_indicators"
        end_marker = "def _create_"

        start_idx = content.find(start_marker)
        if start_idx == -1:
            return []

        next_func_idx = content.find(end_marker, start_idx + len(start_marker))
        if next_func_idx == -1:
            section_content = content[start_idx:]
        else:
            section_content = content[start_idx:next_func_idx]

        features = []

        # SMA –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (–∏–∑ config)
        sma_periods = [5, 10, 20, 50, 100, 200]  # –¢–∏–ø–∏—á–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã
        for period in sma_periods:
            features.extend([f"sma_{period}", f"close_sma_{period}_ratio"])

        # EMA –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        ema_periods = [5, 10, 20, 50, 100, 200]
        for period in ema_periods:
            features.extend([f"ema_{period}", f"close_ema_{period}_ratio"])

        # RSI
        features.extend(["rsi", "rsi_oversold", "rsi_overbought"])

        # MACD (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π)
        features.extend(["macd", "macd_signal", "macd_diff"])

        # Bollinger Bands
        features.extend(
            [
                "bb_high",
                "bb_low",
                "bb_middle",
                "bb_width",
                "bb_position",
                "bb_breakout_upper",
                "bb_breakout_lower",
                "bb_breakout_strength",
            ]
        )

        # ATR
        features.extend(["atr", "atr_pct"])

        # Stochastic
        features.extend(["stoch_k", "stoch_d"])

        # ADX
        features.extend(["adx", "adx_pos", "adx_neg"])

        # PSAR
        features.extend(["psar", "psar_trend", "psar_distance", "psar_distance_normalized"])

        # Ichimoku (–∏–∑ –∫–æ–¥–∞ –≤–∏–¥–Ω–æ —á—Ç–æ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è)
        features.extend(
            [
                "ichimoku_tenkan_sen",
                "ichimoku_kijun_sen",
                "ichimoku_senkou_span_a",
                "ichimoku_senkou_span_b",
                "ichimoku_conversion_line",
                "ichimoku_base_line",
            ]
        )

        return features

    def analyze_microstructure_section(self, content: str) -> list[str]:
        """–ê–Ω–∞–ª–∏–∑ –º–∏–∫—Ä–æ—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        features = [
            # Spread –∏ imbalance
            "spread",
            "spread_ma_10",
            "spread_std_10",
            "order_imbalance",
            "order_imbalance_ma_10",
            "order_imbalance_std_10",
            # Buy/sell pressure
            "buy_pressure",
            "sell_pressure",
            "net_pressure",
            "buy_pressure_ma_10",
            "sell_pressure_ma_10",
            "net_pressure_ma_10",
            # Order flow
            "order_flow_5",
            "order_flow_10",
            "order_flow_20",
            "order_flow_ratio_5",
            "order_flow_ratio_10",
            "order_flow_ratio_20",
            # Volume profile
            "volume_profile_mean",
            "volume_profile_std",
            "volume_profile_skew",
            "volume_price_trend",
            "ease_of_movement",
            "chaikin_money_flow",
            # Price impact
            "price_impact_5",
            "price_impact_10",
            "price_impact_20",
            "kyle_lambda",
            "amihud_illiquidity",
            "effective_spread",
            # Market microstructure
            "quoted_spread",
            "realized_spread",
            "adverse_selection",
            "order_flow_imbalance",
            "trade_intensity",
            "market_impact",
        ]

        return features

    def analyze_rally_detection_section(self, content: str) -> list[str]:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ä–∞–ª–ª–∏"""
        features = [
            # Current rally/drawdown
            "current_rally_magnitude",
            "current_rally_duration",
            "current_rally_velocity",
            "current_drawdown_magnitude",
            "current_drawdown_duration",
            "current_drawdown_velocity",
            # Recent extremes
            "recent_max_rally_1h",
            "recent_max_rally_4h",
            "recent_max_rally_12h",
            "recent_max_drawdown_1h",
            "recent_max_drawdown_4h",
            "recent_max_drawdown_12h",
            # Probability estimates
            "prob_reach_1pct_4h",
            "prob_reach_2pct_4h",
            "prob_reach_3pct_12h",
            # Trend strength
            "trend_strength_5",
            "trend_strength_20",
            "trend_strength_60",
            "trend_consistency_5",
            "trend_consistency_20",
            "trend_consistency_60",
            # Momentum features
            "momentum_acceleration",
            "momentum_persistence",
            "momentum_mean_reversion",
        ]

        return features

    def analyze_signal_quality_section(self, content: str) -> list[str]:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–≥–Ω–∞–ª–∞"""
        features = [
            # Consensus indicators
            "indicators_consensus_long",
            "indicators_count_long",
            "indicators_strength_long",
            "indicators_consensus_short",
            "indicators_count_short",
            "indicators_strength_short",
            "indicators_net_consensus",
            "indicators_total_strength",
            # Quality metrics
            "signal_quality_score",
            "signal_confidence",
            "signal_clarity",
            "pattern_recognition_score",
            "breakout_confirmation",
            # Divergence analysis
            "price_momentum_divergence",
            "volume_price_divergence",
            "rsi_price_divergence",
            "macd_price_divergence",
            "sentiment_price_divergence",
        ]

        return features

    def analyze_futures_specific_section(self, content: str) -> list[str]:
        """–ê–Ω–∞–ª–∏–∑ —Ñ—å—é—á–µ—Ä—Å-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        features = [
            # Core futures data
            "funding_rate",
            "funding_rate_ma_8",
            "funding_rate_std_8",
            "funding_rate_zscore",
            "open_interest",
            "oi_change_1h",
            "oi_change_4h",
            "oi_change_24h",
            "long_short_ratio",
            "ls_ratio_ma_8",
            "ls_ratio_std_8",
            "taker_buy_sell_ratio",
            "tbs_ratio_ma_8",
            "tbs_ratio_std_8",
            # Derived metrics
            "funding_momentum",
            "oi_weighted_momentum",
            "liquidation_pressure",
            "basis_spread",
            "term_structure",
            "contango_backwardation",
            # Volume and sentiment
            "futures_volume_ratio",
            "spot_futures_volume_ratio",
            "options_put_call_ratio",
        ]

        return features

    def analyze_temporal_section(self, content: str) -> list[str]:
        """–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        features = [
            # Cyclical time features
            "hour_sin",
            "hour_cos",
            "day_sin",
            "day_cos",
            "week_sin",
            "week_cos",
            "month_sin",
            "month_cos",
            # Categorical time features
            "is_weekend",
            "is_month_start",
            "is_month_end",
            "is_quarter_end",
            "is_us_market_hours",
            "is_asian_market_hours",
            "is_european_market_hours",
        ]

        return features

    def analyze_ml_optimized_section(self, content: str) -> list[str]:
        """–ê–Ω–∞–ª–∏–∑ ML-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        features = []

        # Statistical features
        for window in [20, 50, 100]:
            features.extend(
                [
                    f"price_zscore_{window}",
                    f"volume_zscore_{window}",
                    f"return_skewness_{window}",
                    f"return_kurtosis_{window}",
                ]
            )

        # Volatility regimes
        features.extend(
            [
                "realized_vol_5m",
                "realized_vol_15m",
                "realized_vol_1h",
                "realized_vol_4h",
                "realized_vol_1d",
                "garman_klass_vol",
                "parkinson_vol",
                "rogers_satchell_vol",
            ]
        )

        # Returns for different periods
        return_periods = [1, 2, 3, 5, 10, 15, 20, 30, 45, 60, 90, 120]
        for period in return_periods:
            features.append(f"returns_{period}m")

        # Volume indicators
        features.extend(
            [
                "volume_cumsum_log",
                "volume_ema_ratio",
                "volume_momentum",
                "volume_acceleration",
                "turnover_acceleration",
                "vwap_deviation",
            ]
        )

        # Advanced technical
        features.extend(
            [
                "efficiency_ratio",
                "fractal_adaptive_ma",
                "kaufman_adaptive_ma",
                "mesa_adaptive_ma",
                "zero_lag_ema",
                "triple_exponential_ma",
            ]
        )

        return features

    def analyze_cross_asset_section(self, content: str) -> list[str]:
        """–ê–Ω–∞–ª–∏–∑ –∫—Ä–æ—Å—Å-–∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        features = [
            # BTC correlations (window=96, min_periods=50)
            "btc_correlation_15m",
            "btc_correlation_1h",
            "btc_correlation_4h",
            "btc_beta_15m",
            "btc_beta_1h",
            "btc_beta_4h",
            # ETH correlations
            "eth_correlation_15m",
            "eth_correlation_1h",
            "eth_correlation_4h",
            "eth_beta_15m",
            "eth_beta_1h",
            "eth_beta_4h",
            # Market metrics
            "market_beta_1h",
            "market_beta_4h",
            "sector_momentum",
            "relative_strength_vs_btc",
            "relative_strength_vs_eth",
        ]

        return features

    def analyze_all_sections(self) -> dict[str, list[str]]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ —Å–µ–∫—Ü–∏–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
        content = self.read_file_content()

        sections = {
            "basic": self.analyze_basic_features_section(content),
            "technical": self.analyze_technical_indicators_section(content),
            "microstructure": self.analyze_microstructure_section(content),
            "rally_detection": self.analyze_rally_detection_section(content),
            "signal_quality": self.analyze_signal_quality_section(content),
            "futures_specific": self.analyze_futures_specific_section(content),
            "temporal": self.analyze_temporal_section(content),
            "ml_optimized": self.analyze_ml_optimized_section(content),
            "cross_asset": self.analyze_cross_asset_section(content),
        }

        return sections

    def get_all_unique_features(self) -> list[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –≤—Å–µ—Ö —Å–µ–∫—Ü–∏–π"""
        sections = self.analyze_all_sections()
        all_features = []

        for section_name, features in sections.items():
            all_features.extend(features)

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        unique_features = []
        seen = set()
        for feature in all_features:
            if feature not in seen:
                unique_features.append(feature)
                seen.add(feature)

        return unique_features

    def generate_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ–± –∞–Ω–∞–ª–∏–∑–µ"""
        sections = self.analyze_all_sections()
        all_features = self.get_all_unique_features()

        report = []
        report.append("üîç –ê–ù–ê–õ–ò–ó –ü–†–ò–ó–ù–ê–ö–û–í –ò–ó –û–ë–£–ß–ê–Æ–©–ï–ì–û –§–ê–ô–õ–ê")
        report.append("=" * 60)
        report.append(f"üìÅ –§–∞–π–ª: {self.training_file_path}")
        report.append(f"üìä –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(all_features)}")
        report.append("")

        report.append("üìã –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –°–ï–ö–¶–ò–Ø–ú:")
        report.append("-" * 40)
        total_by_sections = 0
        for section_name, features in sections.items():
            report.append(f"  {section_name:20}: {len(features):3d} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            total_by_sections += len(features)

        report.append(f"  {'–ò–¢–û–ì–û':<20}: {total_by_sections:3d} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        report.append(f"  {'–£–ù–ò–ö–ê–õ–¨–ù–´–•':<20}: {len(all_features):3d} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        report.append("")

        # –ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞–∂–¥–æ–π —Å–µ–∫—Ü–∏–∏
        for section_name, features in sections.items():
            report.append(f"üìÅ {section_name.upper().replace('_', ' ')}")
            report.append("-" * 30)
            for i, feature in enumerate(features, 1):
                report.append(f"  {i:2d}. {feature}")
            report.append("")

        return "\n".join(report)


if __name__ == "__main__":
    # –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞
    training_file = "/mnt/SSD/PYCHARMPRODJECT/BOT_AI_V3/BOT_AI_V2/–∞–∞–∞.py"

    analyzer = TrainingFeatureAnalyzer(training_file)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    report = analyzer.generate_report()
    print(report)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
    with open("training_features_analysis.txt", "w", encoding="utf-8") as f:
        f.write(report)

    # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    all_features = analyzer.get_all_unique_features()

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ Python —Ñ–∞–π–ª
    with open("extracted_training_features.py", "w", encoding="utf-8") as f:
        f.write('"""–¢–æ—á–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –æ–±—É—á–∞—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ BOT_AI_V2/–∞–∞–∞.py"""\n\n')
        f.write("TRAINING_FEATURES = [\n")
        for feature in all_features:
            f.write(f'    "{feature}",\n')
        f.write("]\n\n")
        f.write(f"# –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {len(all_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n")

    print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print("üìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: training_features_analysis.txt")
    print("üêç –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: extracted_training_features.py")
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(all_features)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
