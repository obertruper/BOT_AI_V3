#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è scaler –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
–†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
"""

import asyncio
import pickle
import sys
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
from sklearn.preprocessing import RobustScaler

sys.path.append("/mnt/SSD/PYCHARMPRODJECT/BOT_AI_V3")

from core.config.config_manager import ConfigManager
from core.logger import setup_logger
from data.data_loader import DataLoader
from ml.logic.feature_engineering import FeatureEngineer

logger = setup_logger("fix_ml_scaler")


async def retrain_scaler():
    """–ü–µ—Ä–µ–æ–±—É—á–∞–µ–º scaler –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""

    logger.info("üîß –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï SCALER –ù–ê –ê–ö–¢–£–ê–õ–¨–ù–´–• –î–ê–ù–ù–´–•")
    logger.info("=" * 50)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    data_loader = DataLoader()
    config_manager = ConfigManager()
    feature_engineer = FeatureEngineer(config_manager.get_ml_config())

    try:
        await data_loader.initialize()

        # –°–∏–º–≤–æ–ª—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è scaler
        training_symbols = [
            "BTCUSDT",
            "ETHUSDT",
            "ADAUSDT",
            "BNBUSDT",
            "XRPUSDT",
            "SOLUSDT",
            "DOTUSDT",
            "LINKUSDT",
        ]

        logger.info(f"–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ {len(training_symbols)} —Å–∏–º–≤–æ–ª–æ–≤...")

        all_features = []

        for symbol in training_symbols:
            logger.info(f"  –û–±—Ä–∞–±–æ—Ç–∫–∞ {symbol}...")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            data = await data_loader.get_data_for_ml(symbol, limit=2000)

            if data is None or len(data) < 300:
                logger.warning(
                    f"  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {len(data) if data is not None else 0}"
                )
                continue

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = feature_engineer.create_features(data)

            if isinstance(features, pd.DataFrame):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                numeric_cols = features.select_dtypes(include=[np.number]).columns
                # –ò—Å–∫–ª—é—á–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                feature_cols = [
                    col
                    for col in numeric_cols
                    if not col.startswith(("future_", "direction_", "profit_"))
                    and col not in ["id", "timestamp", "datetime", "symbol"]
                ]
                features_array = features[feature_cols].values
            elif isinstance(features, np.ndarray):
                features_array = features
            else:
                logger.error(
                    f"  –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {symbol}: {type(features)}"
                )
                continue

            # –£–±–∏—Ä–∞–µ–º NaN –∏ Inf
            mask = np.isfinite(features_array).all(axis=1)
            features_clean = features_array[mask]

            if len(features_clean) > 0:
                all_features.append(features_clean)
                logger.info(
                    f"  –î–æ–±–∞–≤–ª–µ–Ω–æ {len(features_clean)} –æ–±—Ä–∞–∑—Ü–æ–≤, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {features_clean.shape[1]}"
                )
            else:
                logger.warning(f"  –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}")

        if not all_features:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞!")
            return False

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        X = np.vstack(all_features)
        logger.info(f"–û–±—â–∏–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è scaler: {X.shape}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        expected_features = 240
        if X.shape[1] != expected_features:
            logger.warning(
                f"‚ö†Ô∏è –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: –æ–∂–∏–¥–∞–ª–æ—Å—å {expected_features}, –ø–æ–ª—É—á–µ–Ω–æ {X.shape[1]}"
            )

            if X.shape[1] < expected_features:
                # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
                padding = np.zeros((X.shape[0], expected_features - X.shape[1]))
                X = np.hstack([X, padding])
                logger.info(f"–î–æ–ø–æ–ª–Ω–µ–Ω–æ –Ω—É–ª—è–º–∏ –¥–æ {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            else:
                # –û–±—Ä–µ–∑–∞–µ–º
                X = X[:, :expected_features]
                logger.info(f"–û–±—Ä–µ–∑–∞–Ω–æ –¥–æ {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        # –û–±—É—á–∞–µ–º –Ω–æ–≤—ã–π scaler
        logger.info("–û–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ RobustScaler...")
        new_scaler = RobustScaler(
            quantile_range=(5.0, 95.0)
        )  # –ë–æ–ª–µ–µ —Ä–æ–±–∞—Å—Ç–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω
        new_scaler.fit(X)

        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–æ–≤—ã–π scaler
        logger.info("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ scaler...")

        # –ë–µ—Ä–µ–º –Ω–µ–±–æ–ª—å—à—É—é –≤—ã–±–æ—Ä–∫—É –¥–ª—è —Ç–µ—Å—Ç–∞
        test_sample = X[:1000] if len(X) > 1000 else X[:100]
        scaled_test = new_scaler.transform(test_sample)

        logger.info("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:")
        logger.info(
            f"  –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - —Å—Ä–µ–¥–Ω–µ–µ: {test_sample.mean():.2f}, std: {test_sample.std():.2f}"
        )
        logger.info(
            f"  –ü–æ—Å–ª–µ scaler - —Å—Ä–µ–¥–Ω–µ–µ: {scaled_test.mean():.6f}, std: {scaled_test.std():.6f}"
        )
        logger.info(
            f"  –î–∏–∞–ø–∞–∑–æ–Ω –ø–æ—Å–ª–µ scaler: [{scaled_test.min():.3f}, {scaled_test.max():.3f}]"
        )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑—É–º–Ω–æ—Å—Ç—å –∑–Ω–∞—á–µ–Ω–∏–π
        if abs(scaled_test.mean()) > 1.0 or scaled_test.std() > 10.0:
            logger.error("‚ùå –ù–æ–≤—ã–π scaler –¥–∞–µ—Ç –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è!")
            return False

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ä—ã–π scaler –∫–∞–∫ –±—ç–∫–∞–ø
        model_dir = Path("models/saved")
        old_scaler_path = model_dir / "data_scaler.pkl"
        backup_path = (
            model_dir
            / f"data_scaler_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
        )

        if old_scaler_path.exists():
            import shutil

            shutil.copy2(old_scaler_path, backup_path)
            logger.info(f"–°—Ç–∞—Ä—ã–π scaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: {backup_path}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–π scaler
        with open(old_scaler_path, "wb") as f:
            pickle.dump(new_scaler, f)

        logger.info(f"‚úÖ –ù–æ–≤—ã–π scaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {old_scaler_path}")

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        logger.info("\n–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É —Å–∏–º–≤–æ–ª–∞–º–∏ —Å –Ω–æ–≤—ã–º scaler:")

        for symbol in ["BTCUSDT", "ETHUSDT"][:2]:  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ 2 –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
            data = await data_loader.get_data_for_ml(symbol, limit=100)
            if data is not None:
                features = feature_engineer.create_features(data)
                if isinstance(features, pd.DataFrame):
                    numeric_cols = features.select_dtypes(include=[np.number]).columns
                    feature_cols = [
                        col
                        for col in numeric_cols
                        if not col.startswith(("future_", "direction_", "profit_"))
                        and col not in ["id", "timestamp", "datetime", "symbol"]
                    ]
                    features_array = features[feature_cols].values
                elif isinstance(features, np.ndarray):
                    features_array = features

                if features_array.shape[1] != expected_features:
                    if features_array.shape[1] < expected_features:
                        padding = np.zeros(
                            (
                                features_array.shape[0],
                                expected_features - features_array.shape[1],
                            )
                        )
                        features_array = np.hstack([features_array, padding])
                    else:
                        features_array = features_array[:, :expected_features]

                scaled = new_scaler.transform(features_array[-1:])  # –ü–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–æ–∫–∞
                logger.info(
                    f"  {symbol}: —Å—Ä–µ–¥–Ω–µ–µ={scaled.mean():.6f}, std={scaled.std():.6f}"
                )

        return True

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è scaler: {e}")
        import traceback

        traceback.print_exc()
        return False

    finally:
        await data_loader.cleanup()


if __name__ == "__main__":
    success = asyncio.run(retrain_scaler())
    if success:
        print("‚úÖ Scaler —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω!")
        print("üîÑ –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ ML —Å–∏—Å—Ç–µ–º—É –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π")
    else:
        print("‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è scaler!")
